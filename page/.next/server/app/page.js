(()=>{var a={};a.id=974,a.ids=[974],a.modules={261:a=>{"use strict";a.exports=require("next/dist/shared/lib/router/utils/app-paths")},440:(a,b,c)=>{"use strict";c.r(b),c.d(b,{default:()=>e});var d=c(1658);let e=async a=>[{type:"image/x-icon",sizes:"16x16",url:(0,d.fillMetadataSegment)(".",await a.params,"favicon.ico")+""}]},512:(a,b,c)=>{"use strict";Object.defineProperty(b,"__esModule",{value:!0}),!function(a,b){for(var c in b)Object.defineProperty(a,c,{enumerable:!0,get:b[c]})}(b,{default:function(){return p},defaultHead:function(){return l}});let d=c(4985),e=c(740),f=c(687),g=e._(c(3210)),h=d._(c(7755)),i=c(4959),j=c(9513),k=c(4604);function l(a){void 0===a&&(a=!1);let b=[(0,f.jsx)("meta",{charSet:"utf-8"},"charset")];return a||b.push((0,f.jsx)("meta",{name:"viewport",content:"width=device-width"},"viewport")),b}function m(a,b){return"string"==typeof b||"number"==typeof b?a:b.type===g.default.Fragment?a.concat(g.default.Children.toArray(b.props.children).reduce((a,b)=>"string"==typeof b||"number"==typeof b?a:a.concat(b),[])):a.concat(b)}c(148);let n=["name","httpEquiv","charSet","itemProp"];function o(a,b){let{inAmpMode:c}=b;return a.reduce(m,[]).reverse().concat(l(c).reverse()).filter(function(){let a=new Set,b=new Set,c=new Set,d={};return e=>{let f=!0,g=!1;if(e.key&&"number"!=typeof e.key&&e.key.indexOf("$")>0){g=!0;let b=e.key.slice(e.key.indexOf("$")+1);a.has(b)?f=!1:a.add(b)}switch(e.type){case"title":case"base":b.has(e.type)?f=!1:b.add(e.type);break;case"meta":for(let a=0,b=n.length;a<b;a++){let b=n[a];if(e.props.hasOwnProperty(b))if("charSet"===b)c.has(b)?f=!1:c.add(b);else{let a=e.props[b],c=d[b]||new Set;("name"!==b||!g)&&c.has(a)?f=!1:(c.add(a),d[b]=c)}}}return f}}()).reverse().map((a,b)=>{let c=a.key||b;return g.default.cloneElement(a,{key:c})})}let p=function(a){let{children:b}=a,c=(0,g.useContext)(i.AmpStateContext),d=(0,g.useContext)(j.HeadManagerContext);return(0,f.jsx)(h.default,{reduceComponentsToState:o,headManager:d,inAmpMode:(0,k.isInAmpMode)(c),children:b})};("function"==typeof b.default||"object"==typeof b.default&&null!==b.default)&&void 0===b.default.__esModule&&(Object.defineProperty(b.default,"__esModule",{value:!0}),Object.assign(b.default,b),a.exports=b.default)},846:a=>{"use strict";a.exports=require("next/dist/compiled/next-server/app-page.runtime.prod.js")},1025:a=>{"use strict";a.exports=require("next/dist/server/app-render/dynamic-access-async-storage.external.js")},1135:()=>{},1204:(a,b,c)=>{"use strict";c.r(b),c.d(b,{default:()=>d});let d=(0,c(1369).registerClientReference)(function(){throw Error("Attempted to call the default export of \"/home/user/Desktop/awesome-text-to-motion/page/src/app/page.tsx\" from the server, but it's on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.")},"/home/user/Desktop/awesome-text-to-motion/page/src/app/page.tsx","default")},1261:(a,b,c)=>{"use strict";Object.defineProperty(b,"__esModule",{value:!0}),!function(a,b){for(var c in b)Object.defineProperty(a,c,{enumerable:!0,get:b[c]})}(b,{default:function(){return i},getImageProps:function(){return h}});let d=c(4985),e=c(4953),f=c(6533),g=d._(c(1933));function h(a){let{props:b}=(0,e.getImgProps)(a,{defaultLoader:g.default,imgConf:{deviceSizes:[640,750,828,1080,1200,1920,2048,3840],imageSizes:[16,32,48,64,96,128,256,384],path:"/_next/image",loader:"default",dangerouslyAllowSVG:!1,unoptimized:!1}});for(let[a,c]of Object.entries(b))void 0===c&&delete b[a];return{props:b}}let i=f.Image},1480:(a,b)=>{"use strict";function c(a){let{widthInt:b,heightInt:c,blurWidth:d,blurHeight:e,blurDataURL:f,objectFit:g}=a,h=d?40*d:b,i=e?40*e:c,j=h&&i?"viewBox='0 0 "+h+" "+i+"'":"";return"%3Csvg xmlns='http://www.w3.org/2000/svg' "+j+"%3E%3Cfilter id='b' color-interpolation-filters='sRGB'%3E%3CfeGaussianBlur stdDeviation='20'/%3E%3CfeColorMatrix values='1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1' result='s'/%3E%3CfeFlood x='0' y='0' width='100%25' height='100%25'/%3E%3CfeComposite operator='out' in='s'/%3E%3CfeComposite in2='SourceGraphic'/%3E%3CfeGaussianBlur stdDeviation='20'/%3E%3C/filter%3E%3Cimage width='100%25' height='100%25' x='0' y='0' preserveAspectRatio='"+(j?"none":"contain"===g?"xMidYMid":"cover"===g?"xMidYMid slice":"none")+"' style='filter: url(%23b);' href='"+f+"'/%3E%3C/svg%3E"}Object.defineProperty(b,"__esModule",{value:!0}),Object.defineProperty(b,"getImageBlurSvg",{enumerable:!0,get:function(){return c}})},1933:(a,b)=>{"use strict";function c(a){var b;let{config:c,src:d,width:e,quality:f}=a,g=f||(null==(b=c.qualities)?void 0:b.reduce((a,b)=>Math.abs(b-75)<Math.abs(a-75)?b:a))||75;return c.path+"?url="+encodeURIComponent(d)+"&w="+e+"&q="+g+(d.startsWith("/_next/static/media/"),"")}Object.defineProperty(b,"__esModule",{value:!0}),Object.defineProperty(b,"default",{enumerable:!0,get:function(){return d}}),c.__next_img_default=!0;let d=c},1947:()=>{},2756:(a,b)=>{"use strict";Object.defineProperty(b,"__esModule",{value:!0}),!function(a,b){for(var c in b)Object.defineProperty(a,c,{enumerable:!0,get:b[c]})}(b,{VALID_LOADERS:function(){return c},imageConfigDefault:function(){return d}});let c=["default","imgix","cloudinary","akamai","custom"],d={deviceSizes:[640,750,828,1080,1200,1920,2048,3840],imageSizes:[16,32,48,64,96,128,256,384],path:"/_next/image",loader:"default",loaderFile:"",domains:[],disableStaticImages:!1,minimumCacheTTL:60,formats:["image/webp"],dangerouslyAllowSVG:!1,contentSecurityPolicy:"script-src 'none'; frame-src 'none'; sandbox;",contentDispositionType:"attachment",localPatterns:void 0,remotePatterns:[],qualities:void 0,unoptimized:!1}},3033:a=>{"use strict";a.exports=require("next/dist/server/app-render/work-unit-async-storage.external.js")},3038:(a,b,c)=>{"use strict";Object.defineProperty(b,"__esModule",{value:!0}),Object.defineProperty(b,"useMergedRef",{enumerable:!0,get:function(){return e}});let d=c(3210);function e(a,b){let c=(0,d.useRef)(null),e=(0,d.useRef)(null);return(0,d.useCallback)(d=>{if(null===d){let a=c.current;a&&(c.current=null,a());let b=e.current;b&&(e.current=null,b())}else a&&(c.current=f(a,d)),b&&(e.current=f(b,d))},[a,b])}function f(a,b){if("function"!=typeof a)return a.current=b,()=>{a.current=null};{let c=a(b);return"function"==typeof c?c:()=>a(null)}}("function"==typeof b.default||"object"==typeof b.default&&null!==b.default)&&void 0===b.default.__esModule&&(Object.defineProperty(b.default,"__esModule",{value:!0}),Object.assign(b.default,b),a.exports=b.default)},3295:a=>{"use strict";a.exports=require("next/dist/server/app-render/after-task-async-storage.external.js")},3449:(a,b,c)=>{Promise.resolve().then(c.t.bind(c,5227,23)),Promise.resolve().then(c.t.bind(c,6346,23)),Promise.resolve().then(c.t.bind(c,7924,23)),Promise.resolve().then(c.t.bind(c,99,23)),Promise.resolve().then(c.t.bind(c,8243,23)),Promise.resolve().then(c.t.bind(c,8827,23)),Promise.resolve().then(c.t.bind(c,2763,23)),Promise.resolve().then(c.t.bind(c,7173,23)),Promise.resolve().then(c.bind(c,5587))},3873:a=>{"use strict";a.exports=require("path")},4263:()=>{},4431:(a,b,c)=>{"use strict";c.r(b),c.d(b,{default:()=>j,metadata:()=>i});var d=c(7413),e=c(2202),f=c.n(e),g=c(4988),h=c.n(g);c(1135);let i={title:"Create Next App",description:"Generated by create next app"};function j({children:a}){return(0,d.jsx)("html",{lang:"en",children:(0,d.jsx)("body",{className:`${f().variable} ${h().variable} antialiased`,children:a})})}},4604:(a,b)=>{"use strict";function c(a){let{ampFirst:b=!1,hybrid:c=!1,hasQuery:d=!1}=void 0===a?{}:a;return b||c&&d}Object.defineProperty(b,"__esModule",{value:!0}),Object.defineProperty(b,"isInAmpMode",{enumerable:!0,get:function(){return c}})},4872:(a,b,c)=>{Promise.resolve().then(c.bind(c,1204))},4946:(a,b,c)=>{"use strict";c.r(b),c.d(b,{default:()=>fj});var d,e,f,g=c(687),h=c(3210),i=c.t(h,2);let j=JSON.parse('[{"arxiv_id":"2508.05162","title":"X-MoGen: Unified Motion Generation across Humans and Animals","abstract":"Text-driven motion generation has attracted increasing attention due to its\\nbroad applications in virtual reality, animation, and robotics. While existing\\nmethods typically model human and animal motion separately, a joint\\ncross-species approach offers key advantages, such as a unified representation\\nand improved generalization. However, morphological differences across species\\nremain a key challenge, often compromising motion plausibility. To address\\nthis, we propose \\\\textbf{X-MoGen}, the first unified framework for\\ncross-species text-driven motion generation covering both humans and animals.\\nX-MoGen adopts a two-stage architecture. First, a conditional graph variational\\nautoencoder learns canonical T-pose priors, while an autoencoder encodes motion\\ninto a shared latent space regularized by morphological loss. In the second\\nstage, we perform masked motion modeling to generate motion embeddings\\nconditioned on textual descriptions. During training, a morphological\\nconsistency module is employed to promote skeletal plausibility across species.\\nTo support unified modeling, we construct \\\\textbf{UniMo4D}, a large-scale\\ndataset of 115 species and 119k motion sequences, which integrates human and\\nanimal motions under a shared skeletal topology for joint training. Extensive\\nexperiments on UniMo4D demonstrate that X-MoGen outperforms state-of-the-art\\nmethods on both seen and unseen species.","authors":["Xuan Wang","Kai Ruan","Liyang Qian","Zhizhi Guo","Chang Su","Gaoang Wang"],"year":2025,"month":8,"url":"https://arxiv.org/abs/2508.05162","survey":false,"survey_abbr":"","model":true,"model_abbr":"X-MoGen","dataset":true,"dataset_abbr":"UniMo4D","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Transformer, Diffusion","approach_tags":"Graph"},{"arxiv_id":"2508.02605","title":"ReMoMask: Retrieval-Augmented Masked Motion Generation","abstract":"Text-to-Motion (T2M) generation aims to synthesize realistic and semantically\\naligned human motion sequences from natural language descriptions. However,\\ncurrent approaches face dual challenges: Generative models (e.g., diffusion\\nmodels) suffer from limited diversity, error accumulation, and physical\\nimplausibility, while Retrieval-Augmented Generation (RAG) methods exhibit\\ndiffusion inertia, partial-mode collapse, and asynchronous artifacts. To\\naddress these limitations, we propose ReMoMask, a unified framework integrating\\nthree key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples\\nnegative sample scale from batch size via momentum queues, substantially\\nimproving cross-modal retrieval precision; 2) A Semantic Spatio-temporal\\nAttention mechanism enforces biomechanical constraints during part-level fusion\\nto eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates\\nminor unconditional generation to enhance generalization. Built upon MoMask\'s\\nRVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal\\nsteps. Extensive experiments on standard benchmarks demonstrate the\\nstate-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97%\\nimprovement in FID scores on HumanML3D and KIT-ML, respectively, compared to\\nthe previous SOTA method RAG-T2M. Code:\\nhttps://github.com/AIGeeksGroup/ReMoMask. Website:\\nhttps://aigeeksgroup.github.io/ReMoMask.","authors":["Zhengdao Li","Siheng Wang","Zeyu Zhang","Hao Tang"],"year":2025,"month":8,"url":"https://arxiv.org/abs/2508.02605","survey":false,"survey_abbr":"","model":true,"model_abbr":"ReMoMask","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://aigeeksgroup.github.io/ReMoMask/","repo":"https://github.com/AIGeeksGroup/ReMoMask","backbone_tags":"VQ-VAE, Transformer","approach_tags":"Retrieval"},{"arxiv_id":"2507.19850","title":"FineMotion: A Dataset and Benchmark with both Spatial and Temporal Annotation for Fine-grained Motion Generation and Editing","abstract":"Generating realistic human motions from textual descriptions has undergone\\nsignificant advancements. However, existing methods often overlook specific\\nbody part movements and their timing. In this paper, we address this issue by\\nenriching the textual description with more details. Specifically, we propose\\nthe FineMotion dataset, which contains over 442,000 human motion snippets -\\nshort segments of human motion sequences - and their corresponding detailed\\ndescriptions of human body part movements. Additionally, the dataset includes\\nabout 95k detailed paragraphs describing the movements of human body parts of\\nentire motion sequences. Experimental results demonstrate the significance of\\nour dataset on the text-driven finegrained human motion generation task,\\nespecially with a remarkable +15.3% improvement in Top-3 accuracy for the MDM\\nmodel. Notably, we further support a zero-shot pipeline of fine-grained motion\\nediting, which focuses on detailed editing in both spatial and temporal\\ndimensions via text. Dataset and code available at: CVI-SZU/FineMotion","authors":["Bizhu Wu","Jinheng Xie","Meidan Ding","Zhe Kong","Jianfeng Ren","Ruibin Bai","Rong Qu","Linlin Shen"],"year":2025,"month":7,"url":"https://arxiv.org/abs/2507.19850","survey":false,"survey_abbr":"","model":false,"model_abbr":"","dataset":true,"dataset_abbr":"FineMotion","submission":"","submission_year":"","page":"","repo":"https://github.com/BizhuWu/FineMotion","backbone_tags":"","approach_tags":""},{"arxiv_id":"2507.09122","title":"SnapMoGen: Human Motion Generation from Expressive Texts","abstract":"Text-to-motion generation has experienced remarkable progress in recent\\nyears. However, current approaches remain limited to synthesizing motion from\\nshort or general text prompts, primarily due to dataset constraints. This\\nlimitation undermines fine-grained controllability and generalization to unseen\\nprompts. In this paper, we introduce SnapMoGen, a new text-motion dataset\\nfeaturing high-quality motion capture data paired with accurate, expressive\\ntextual annotations. The dataset comprises 20K motion clips totaling 44 hours,\\naccompanied by 122K detailed textual descriptions averaging 48 words per\\ndescription (vs. 12 words of HumanML3D). Importantly, these motion clips\\npreserve original temporal continuity as they were in long sequences,\\nfacilitating research in long-term motion generation and blending. We also\\nimprove upon previous generative masked modeling approaches. Our model,\\nMoMask++, transforms motion into multi-scale token sequences that better\\nexploit the token capacity, and learns to generate all tokens using a single\\ngenerative masked transformer. MoMask++ achieves state-of-the-art performance\\non both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the\\nability to process casual user prompts by employing an LLM to reformat inputs\\nto align with the expressivity and narration style of SnapMoGen. Project\\nwebpage: https://snap-research.github.io/SnapMoGen/","authors":["Chuan Guo","Inwoo Hwang","Jian Wang","Bing Zhou"],"year":2025,"month":7,"url":"https://arxiv.org/abs/2507.09122","survey":false,"survey_abbr":"","model":true,"model_abbr":"MoMask++","dataset":true,"dataset_abbr":"SnapMoGen","submission":"","submission_year":"","page":"https://snap-research.github.io/SnapMoGen/","repo":"https://github.com/snap-research/SnapMoGen","backbone_tags":"VQ-VAE, Transformer","approach_tags":""},{"arxiv_id":"2507.07095","title":"Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data","abstract":"Generating diverse and natural human motion sequences based on textual\\ndescriptions constitutes a fundamental and challenging research area within the\\ndomains of computer vision, graphics, and robotics. Despite significant\\nadvancements in this field, current methodologies often face challenges\\nregarding zero-shot generalization capabilities, largely attributable to the\\nlimited size of training datasets. Moreover, the lack of a comprehensive\\nevaluation framework impedes the advancement of this task by failing to\\nidentify directions for improvement. In this work, we aim to push\\ntext-to-motion into a new era, that is, to achieve the generalization ability\\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\\nand introduce MotionMillion-the largest human motion dataset to date, featuring\\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\\nresults demonstrate strong generalization to out-of-domain and complex\\ncompositional motions, marking a significant step toward zero-shot human motion\\ngeneration. The code is available at\\nhttps://github.com/VankouF/MotionMillion-Codes.","authors":["Ke Fan","Shunlin Lu","Minyue Dai","Runyi Yu","Lixing Xiao","Zhiyang Dou","Junting Dong","Lizhuang Ma","Jingbo Wang"],"year":2025,"month":7,"url":"https://arxiv.org/abs/2507.07095","survey":false,"survey_abbr":"","model":true,"model_abbr":"GotoZero","dataset":true,"dataset_abbr":"MotionMillion","submission":"ICCV","submission_year":"2025","page":"https://vankouf.github.io/MotionMillion/","repo":"https://github.com/VankouF/MotionMillion-Codes","backbone_tags":"T5, VQ-VAE, Transformer","approach_tags":""},{"arxiv_id":"2507.06590","title":"MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction","abstract":"We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf\\ninteraction, aimed at addressing the persistent challenge of generating human\\nmotion from rare language prompts. While previous approaches struggle with\\ncoarse-grained matching and overlook important semantic cues due to motion\\nredundancy, our key insight lies in leveraging fine-grained clip relationships\\nto mitigate these issues. MOST\'s retrieval stage presents the first formulation\\nof its kind - temporal clip Banzhaf interaction - which precisely quantifies\\ntextual-motion coherence at the clip level. This facilitates direct,\\nfine-grained text-to-motion clip matching and eliminates prevalent redundancy.\\nIn the generation stage, a motion prompt module effectively utilizes retrieved\\nmotion clips to produce semantically consistent movements. Extensive\\nevaluations confirm that MOST achieves state-of-the-art text-to-motion\\nretrieval and generation performance by comprehensively addressing previous\\nchallenges, as demonstrated through quantitative and qualitative results\\nhighlighting its effectiveness, especially for rare prompts.","authors":["Yin Wang","Mu li","Zhiying Leng","Frederick W. B. Li","Xiaohui Liang"],"year":2025,"month":7,"url":"https://arxiv.org/abs/2507.06590","survey":false,"survey_abbr":"","model":true,"model_abbr":"MOST","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Diffusion","approach_tags":""},{"arxiv_id":"2506.24086","title":"MotionGPT3: Human Motion as a Second Modality","abstract":"Though recent advances in multimodal models have demonstrated strong\\ncapabilities and opportunities in unified understanding and generation, the\\ndevelopment of unified motion-language models remains underexplored. To enable\\nsuch models with high-fidelity human motion, two core challenges must be\\naddressed. The first is the reconstruction gap between the continuous motion\\nmodality and discrete representation in an autoregressive manner, and the\\nsecond is the degradation of language intelligence during unified training.\\nInspired by the mixture of experts, we propose MotionGPT3, a bimodal\\nmotion-language model that treats human motion as a second modality, decoupling\\nmotion modeling via separate model parameters and enabling both effective\\ncross-modal interaction and efficient multimodal scaling training. To preserve\\nlanguage intelligence, the text branch retains the original structure and\\nparameters of the pretrained language model, while a new motion branch is\\nintegrated via a shared attention mechanism, enabling bidirectional information\\nflow between two modalities. We first employ a motion Variational Autoencoder\\n(VAE) to encode raw human motion into latent representations. Based on this\\ncontinuous latent space, the motion branch predicts motion latents directly\\nfrom intermediate hidden states using a diffusion head, bypassing discrete\\ntokenization. Extensive experiments show that our approach achieves competitive\\nperformance on both motion understanding and generation tasks while preserving\\nstrong language capabilities, establishing a unified bimodal motion diffusion\\nframework within an autoregressive manner.","authors":["Bingfan Zhu","Biao Jiang","Sunyi Wang","Shixiang Tang","Tao Chen","Linjie Luo","Youyi Zheng","Xin Chen"],"year":2025,"month":6,"url":"https://arxiv.org/abs/2506.24086","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionGPT3","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://motiongpt3.github.io/","repo":"https://github.com/OpenMotionLab/MotionGPT3","backbone_tags":"LLM","approach_tags":""},{"arxiv_id":"2506.21912","title":"Generating Attribute-Aware Human Motions from Textual Prompt","abstract":"Text-driven human motion generation has recently attracted considerable\\nattention, allowing models to generate human motions based on textual\\ndescriptions. However, current methods neglect the influence of human\\nattributes (such as age, gender, weight, and height) which are key factors\\nshaping human motion patterns. This work represents a pilot exploration for\\nbridging this gap. We conceptualize each motion as comprising both attribute\\ninformation and action semantics, where textual descriptions align exclusively\\nwith action semantics. To achieve this, a new framework inspired by Structural\\nCausal Models is proposed to decouple action semantics from human attributes,\\nenabling text-to-semantics prediction and attribute-controlled generation. The\\nresulting model is capable of generating realistic, attribute-aware motion\\naligned with the user\'s text and attribute inputs. For evaluation, we introduce\\nHumanAttr, a comprehensive dataset containing attribute annotations for\\ntext-motion pairs, setting the first benchmark for attribute-aware\\ntext-to-motion generation. Extensive experiments on the new dataset validate\\nour model\'s effectiveness.","authors":["Xinghan Wang","Kun Xu","Fei Li","Cao Sheng","Jiazhong Yu","Yadong Mu"],"year":2025,"month":6,"url":"https://arxiv.org/abs/2506.21912","survey":false,"survey_abbr":"","model":true,"model_abbr":"AttrMoGen","dataset":true,"dataset_abbr":"HumanAttr","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"VQ-VAE, CLIP, Transformer","approach_tags":"Attribute"},{"arxiv_id":"2506.10353","title":"Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation","abstract":"Recent advances in large language models, especially in natural language\\nunderstanding and reasoning, have opened new possibilities for text-to-motion\\ngeneration. Although existing approaches have made notable progress in semantic\\nalignment and motion synthesis, they often rely on end-to-end mapping\\nstrategies that fail to capture deep linguistic structures and logical\\nreasoning. Consequently, generated motions tend to lack controllability,\\nconsistency, and diversity. To address these limitations, we propose Motion-R1,\\na unified motion-language modeling framework that integrates a Chain-of-Thought\\nmechanism. By explicitly decomposing complex textual instructions into\\nlogically structured action paths, Motion-R1 provides high-level semantic\\nguidance for motion generation, significantly enhancing the model\'s ability to\\ninterpret and execute multi-step, long-horizon, and compositionally rich\\ncommands. To train our model, we adopt Group Relative Policy Optimization, a\\nreinforcement learning algorithm designed for large models, which leverages\\nmotion quality feedback to optimize reasoning chains and motion synthesis\\njointly. Extensive experiments across multiple benchmark datasets demonstrate\\nthat Motion-R1 achieves competitive or superior performance compared to\\nstate-of-the-art methods, particularly in scenarios requiring nuanced semantic\\nunderstanding and long-term temporal coherence. The code, model and data will\\nbe publicly available.","authors":["Runqi Ouyang","Haoyun Li","Zhenyuan Zhang","Xiaofeng Wang","Zheng Zhu","Guan Huang","Xingang Wang"],"year":2025,"month":6,"url":"https://arxiv.org/abs/2506.10353","survey":false,"survey_abbr":"","model":true,"model_abbr":"Motion-R1","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://motion-r1.github.io/","repo":"https://github.com/GigaAI-Research/Motion-R1","backbone_tags":"LLM","approach_tags":"GRPO, RL"},{"arxiv_id":"2506.05952","title":"MOGO: Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation","abstract":"Recent advances in transformer-based text-to-motion generation have led to\\nimpressive progress in synthesizing high-quality human motion. Nevertheless,\\njointly achieving high fidelity, streaming capability, real-time\\nresponsiveness, and scalability remains a fundamental challenge. In this paper,\\nwe propose MOGO (Motion Generation with One-pass), a novel autoregressive\\nframework tailored for efficient and real-time 3D motion generation. MOGO\\ncomprises two key components: (1) MoSA-VQ, a motion scale-adaptive residual\\nvector quantization module that hierarchically discretizes motion sequences\\nwith learnable scaling to produce compact yet expressive representations; and\\n(2) RQHC-Transformer, a residual quantized hierarchical causal transformer that\\ngenerates multi-layer motion tokens in a single forward pass, significantly\\nreducing inference latency. To enhance semantic fidelity, we further introduce\\na text condition alignment mechanism that improves motion decoding under\\ntextual control. Extensive experiments on benchmark datasets including\\nHumanML3D, KIT-ML, and CMP demonstrate that MOGO achieves competitive or\\nsuperior generation quality compared to state-of-the-art transformer-based\\nmethods, while offering substantial improvements in real-time performance,\\nstreaming generation, and generalization under zero-shot settings.","authors":["Dongjie Fu","Tengjiao Sun","Pengcheng Fang","Xiaohao Cai","Hansung Kim"],"year":2025,"month":6,"url":"https://arxiv.org/abs/2506.05952","survey":false,"survey_abbr":"","model":true,"model_abbr":"MOGO","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"VQ-VAE, Transformer","approach_tags":""},{"arxiv_id":"2506.02452","title":"ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model","abstract":"While diffusion models advance text-to-motion generation, their static\\nsemantic conditioning ignores temporal-frequency demands: early denoising\\nrequires structural semantics for motion foundations while later stages need\\nlocalized details for text alignment. This mismatch mirrors biological\\nmorphogenesis where developmental phases demand distinct genetic programs.\\nInspired by epigenetic regulation governing morphological specialization, we\\npropose **(ANT)**, an **A**daptive **N**eural **T**emporal-Aware architecture.\\nANT orchestrates semantic granularity through: **(i) Semantic Temporally\\nAdaptive (STA) Module:** Automatically partitions denoising into low-frequency\\nstructural planning and high-frequency refinement via spectral analysis. **(ii)\\nDynamic Classifier-Free Guidance scheduling (DCFG):** Adaptively adjusts\\nconditional to unconditional ratio enhancing efficiency while maintaining\\nfidelity. **(iii) Temporal-semantic reweighting:** Quantitatively aligns text\\ninfluence with phase requirements. Extensive experiments show that ANT can be\\napplied to various baselines, significantly improving model performance, and\\nachieving state-of-the-art semantic alignment on StableMoFusion.","authors":["Wenshuo Chen","Kuimou Yu","Haozhe Jia","Kaishen Yuan","Bowen Tian","Songning Lai","Hongru Xiao","Erhang Zhang","Lei Wang","Yutao Yue"],"year":2025,"month":6,"url":"https://arxiv.org/abs/2506.02452","survey":false,"survey_abbr":"","model":true,"model_abbr":"ANT","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"https://github.com/CCSCovenant/ANT","backbone_tags":"UNet, Transformer","approach_tags":""},{"arxiv_id":"2506.00043","title":"From Motion to Behavior: Hierarchical Modeling of Humanoid Generative Behavior Control","abstract":"Human motion generative modeling or synthesis aims to characterize\\ncomplicated human motions of daily activities in diverse real-world\\nenvironments. However, current research predominantly focuses on either\\nlow-level, short-period motions or high-level action planning, without taking\\ninto account the hierarchical goal-oriented nature of human activities. In this\\nwork, we take a step forward from human motion generation to human behavior\\nmodeling, which is inspired by cognitive science. We present a unified\\nframework, dubbed Generative Behavior Control (GBC), to model diverse human\\nmotions driven by various high-level intentions by aligning motions with\\nhierarchical behavior plans generated by large language models (LLMs). Our\\ninsight is that human motions can be jointly controlled by task and motion\\nplanning in robotics, but guided by LLMs to achieve improved motion diversity\\nand physical fidelity. Meanwhile, to overcome the limitations of existing\\nbenchmarks, i.e., lack of behavioral plans, we propose GBC-100K dataset\\nannotated with a hierarchical granularity of semantic and motion plans driven\\nby target goals. Our experiments demonstrate that GBC can generate more diverse\\nand purposeful high-quality human motions with 10* longer horizons compared\\nwith existing methods when trained on GBC-100K, laying a foundation for future\\nresearch on behavioral modeling of human motions. Our dataset and source code\\nwill be made publicly available.","authors":["Jusheng Zhang","Jinzhou Tang","Sidi Liu","Mingyan Li","Sheng Zhang","Jian Wang","Keze Wang"],"year":2025,"month":5,"url":"https://arxiv.org/abs/2506.00043","survey":false,"survey_abbr":"","model":true,"model_abbr":"PHYLOMAN","dataset":true,"dataset_abbr":"GBC-100K","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"LLM","approach_tags":""},{"arxiv_id":"2505.19377","title":"Absolute Coordinates Make Motion Generation Easy","abstract":"State-of-the-art text-to-motion generation models rely on the\\nkinematic-aware, local-relative motion representation popularized by HumanML3D,\\nwhich encodes motion relative to the pelvis and to the previous frame with\\nbuilt-in redundancy. While this design simplifies training for earlier\\ngeneration models, it introduces critical limitations for diffusion models and\\nhinders applicability to downstream tasks. In this work, we revisit the motion\\nrepresentation and propose a radically simplified and long-abandoned\\nalternative for text-to-motion generation: absolute joint coordinates in global\\nspace. Through systematic analysis of design choices, we show that this\\nformulation achieves significantly higher motion fidelity, improved text\\nalignment, and strong scalability, even with a simple Transformer backbone and\\nno auxiliary kinematic-aware losses. Moreover, our formulation naturally\\nsupports downstream tasks such as text-driven motion control and\\ntemporal/spatial editing without additional task-specific reengineering and\\ncostly classifier guidance generation from control signals. Finally, we\\ndemonstrate promising generalization to directly generate SMPL-H mesh vertices\\nin motion from text, laying a strong foundation for future research and\\nmotion-related applications.","authors":["Zichong Meng","Zeyu Han","Xiaogang Peng","Yiming Xie","Huaizu Jiang"],"year":2025,"month":5,"url":"https://arxiv.org/abs/2505.19377","survey":false,"survey_abbr":"","model":true,"model_abbr":"ACMDM","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://neu-vi.github.io/ACMDM/","repo":"https://github.com/neu-vi/ACMDM","backbone_tags":"CLIP","approach_tags":"RoPE"},{"arxiv_id":"2505.11013","title":"Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion","abstract":"Generating 3D human motion from text descriptions remains challenging due to\\nthe diverse and complex nature of human motion. While existing methods excel\\nwithin the training distribution, they often struggle with out-of-distribution\\nmotions, limiting their applicability in real-world scenarios. Existing\\nVQVAE-based methods often fail to represent novel motions faithfully using\\ndiscrete tokens, which hampers their ability to generalize beyond seen data.\\nMeanwhile, diffusion-based methods operating on continuous representations\\noften lack fine-grained control over individual frames. To address these\\nchallenges, we propose a robust motion generation framework MoMADiff, which\\ncombines masked modeling with diffusion processes to generate motion using\\nframe-level continuous representations. Our model supports flexible\\nuser-provided keyframe specification, enabling precise control over both\\nspatial and temporal aspects of motion synthesis. MoMADiff demonstrates strong\\ngeneralization capability on novel text-to-motion datasets with sparse\\nkeyframes as motion prompts. Extensive experiments on two held-out datasets and\\ntwo standard benchmarks show that our method consistently outperforms\\nstate-of-the-art models in motion quality, instruction fidelity, and keyframe\\nadherence. The code is available at: https://github.com/zzysteve/MoMADiff","authors":["Zongye Zhang","Bohan Kong","Qingjie Liu","Yunhong Wang"],"year":2025,"month":5,"url":"https://arxiv.org/abs/2505.11013","survey":false,"survey_abbr":"","model":true,"model_abbr":"MoMADiff","dataset":false,"dataset_abbr":"","submission":"MM","submission_year":"2025","page":"","repo":"https://github.com/zzysteve/MoMADiff","backbone_tags":"VAE, Diffusion","approach_tags":"Multi-Task"},{"arxiv_id":"2505.04974","title":"ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment","abstract":"Bilingual text-to-motion generation, which synthesizes 3D human motions from\\nbilingual text inputs, holds immense potential for cross-linguistic\\napplications in gaming, film, and robotics. However, this task faces critical\\nchallenges: the absence of bilingual motion-language datasets and the\\nmisalignment between text and motion distributions in diffusion models, leading\\nto semantically inconsistent or low-quality motions. To address these\\nchallenges, we propose BiHumanML3D, a novel bilingual human motion dataset,\\nwhich establishes a crucial benchmark for bilingual text-to-motion generation\\nmodels. Furthermore, we propose a Bilingual Motion Diffusion model (BiMD),\\nwhich leverages cross-lingual aligned representations to capture semantics,\\nthereby achieving a unified bilingual model. Building upon this, we propose\\nReward-guided sampling Alignment (ReAlign) method, comprising a step-aware\\nreward model to assess alignment quality during sampling and a reward-guided\\nstrategy that directs the diffusion process toward an optimally aligned\\ndistribution. This reward model integrates step-aware tokens and combines a\\ntext-aligned module for semantic consistency and a motion-aligned module for\\nrealism, refining noisy motions at each timestep to balance probability density\\nand alignment. Experiments demonstrate that our approach significantly improves\\ntext-motion alignment and motion quality compared to existing state-of-the-art\\nmethods. Project page: https://wengwanjiang.github.io/ReAlign-page/.","authors":["Wanjiang Weng","Xiaofeng Tan","Hongsong Wang","Pan Zhou"],"year":2025,"month":5,"url":"https://arxiv.org/abs/2505.04974","survey":false,"survey_abbr":"","model":true,"model_abbr":"ReAlign","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://wengwanjiang.github.io/ReAlign-page/","repo":"","backbone_tags":"Diffusion","approach_tags":""},{"arxiv_id":"2505.01425","title":"GENMO: A GENeralist Model for Human MOtion","abstract":"Human motion modeling traditionally separates motion generation and\\nestimation into distinct tasks with specialized models. Motion generation\\nmodels focus on creating diverse, realistic motions from inputs like text,\\naudio, or keyframes, while motion estimation models aim to reconstruct accurate\\nmotion trajectories from observations like videos. Despite sharing underlying\\nrepresentations of temporal dynamics and kinematics, this separation limits\\nknowledge transfer between tasks and requires maintaining separate models. We\\npresent GENMO, a unified Generalist Model for Human Motion that bridges motion\\nestimation and generation in a single framework. Our key insight is to\\nreformulate motion estimation as constrained motion generation, where the\\noutput motion must precisely satisfy observed conditioning signals. Leveraging\\nthe synergy between regression and diffusion, GENMO achieves accurate global\\nmotion estimation while enabling diverse motion generation. We also introduce\\nan estimation-guided training objective that exploits in-the-wild videos with\\n2D annotations and text descriptions to enhance generative diversity.\\nFurthermore, our novel architecture handles variable-length motions and mixed\\nmultimodal conditions (text, audio, video) at different time intervals,\\noffering flexible control. This unified approach creates synergistic benefits:\\ngenerative priors improve estimated motions under challenging conditions like\\nocclusions, while diverse video data enhances generation capabilities.\\nExtensive experiments demonstrate GENMO\'s effectiveness as a generalist\\nframework that successfully handles multiple human motion tasks within a single\\nmodel.","authors":["Jiefeng Li","Jinkun Cao","Haotian Zhang","Davis Rempe","Jan Kautz","Umar Iqbal","Ye Yuan"],"year":2025,"month":5,"url":"https://arxiv.org/abs/2505.01425","survey":false,"survey_abbr":"","model":true,"model_abbr":"GENMO","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://research.nvidia.com/labs/dair/genmo/","repo":"","backbone_tags":"Transformer","approach_tags":"Multi-Task, RoPE"},{"arxiv_id":"2505.00998","title":"Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis","abstract":"Human motion synthesis aims to generate plausible human motion sequences,\\nwhich has raised widespread attention in computer animation. Recent score-based\\ngenerative models (SGMs) have demonstrated impressive results on this task.\\nHowever, their training process involves complex curvature trajectories,\\nleading to unstable training process. In this paper, we propose a\\nDeterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for\\nhuman motion synthesis. DSDFM consists of two stages. The first human motion\\nreconstruction stage aims to learn the latent space distribution of human\\nmotions. The second diverse motion generation stage aims to build connections\\nbetween the Gaussian distribution and the latent space distribution of human\\nmotions, thereby enhancing the diversity and accuracy of the generated human\\nmotions. This stage is achieved by the designed deterministic feature mapping\\nprocedure with DerODE and stochastic diverse output generation procedure with\\nDivSDE.DSDFM is easy to train compared to previous SGMs-based methods and can\\nenhance diversity without introducing additional training parameters.Through\\nqualitative and quantitative experiments, DSDFM achieves state-of-the-art\\nresults surpassing the latest methods, validating its superiority in human\\nmotion synthesis.","authors":["Yu Hua","Weiming Liu","Gui Xu","Yaqing Hou","Yew-Soon Ong","Qiang Zhang"],"year":2025,"month":5,"url":"https://arxiv.org/abs/2505.00998","survey":false,"survey_abbr":"","model":true,"model_abbr":"DSDFM","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2025","page":"","repo":"","backbone_tags":"Transformer, GRU","approach_tags":"Score-based"},{"arxiv_id":"2504.12540","title":"UniPhys: Unified Planner and Controller with Diffusion for Flexible Physics-Based Character Control","abstract":"Generating natural and physically plausible character motion remains\\nchallenging, particularly for long-horizon control with diverse guidance\\nsignals. While prior work combines high-level diffusion-based motion planners\\nwith low-level physics controllers, these systems suffer from domain gaps that\\ndegrade motion quality and require task-specific fine-tuning. To tackle this\\nproblem, we introduce UniPhys, a diffusion-based behavior cloning framework\\nthat unifies motion planning and control into a single model. UniPhys enables\\nflexible, expressive character motion conditioned on multi-modal inputs such as\\ntext, trajectories, and goals. To address accumulated prediction errors over\\nlong sequences, UniPhys is trained with the Diffusion Forcing paradigm,\\nlearning to denoise noisy motion histories and handle discrepancies introduced\\nby the physics simulator. This design allows UniPhys to robustly generate\\nphysically plausible, long-horizon motions. Through guided sampling, UniPhys\\ngeneralizes to a wide range of control signals, including unseen ones, without\\nrequiring task-specific fine-tuning. Experiments show that UniPhys outperforms\\nprior methods in motion naturalness, generalization, and robustness across\\ndiverse control tasks.","authors":["Yan Wu","Korrawe Karunratanakul","Zhengyi Luo","Siyu Tang"],"year":2025,"month":4,"url":"https://arxiv.org/abs/2504.12540","survey":false,"survey_abbr":"","model":true,"model_abbr":"UniPhys","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2025","page":"https://wuyan01.github.io/uniphys-project/","repo":"","backbone_tags":"Diffusion, Transformer","approach_tags":"Physical"},{"arxiv_id":"2504.03639","title":"Shape My Moves: Text-Driven Shape-Aware Synthesis of Human Motions","abstract":"We explore how body shapes influence human motion synthesis, an aspect often\\noverlooked in existing text-to-motion generation methods due to the ease of\\nlearning a homogenized, canonical body shape. However, this homogenization can\\ndistort the natural correlations between different body shapes and their motion\\ndynamics. Our method addresses this gap by generating body-shape-aware human\\nmotions from natural language prompts. We utilize a finite scalar\\nquantization-based variational autoencoder (FSQ-VAE) to quantize motion into\\ndiscrete tokens and then leverage continuous body shape information to\\nde-quantize these tokens back into continuous, detailed motion. Additionally,\\nwe harness the capabilities of a pretrained language model to predict both\\ncontinuous shape parameters and motion tokens, facilitating the synthesis of\\ntext-aligned motions and decoding them into shape-aware motions. We evaluate\\nour method quantitatively and qualitatively, and also conduct a comprehensive\\nperceptual study to demonstrate its efficacy in generating shape-aware motions.","authors":["Ting-Hsuan Liao","Yi Zhou","Yu Shen","Chun-Hao Paul Huang","Saayan Mitra","Jia-Bin Huang","Uttaran Bhattacharya"],"year":2025,"month":4,"url":"https://arxiv.org/abs/2504.03639","survey":false,"survey_abbr":"","model":true,"model_abbr":"Shape-Move","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2025","page":"https://shape-move.github.io/","repo":"https://github.com/shape-move/shape-move-public","backbone_tags":"VQ-VAE, LLM","approach_tags":"Attribute"},{"arxiv_id":"2504.02478","title":"MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities","abstract":"Recent motion-aware large language models have demonstrated promising\\npotential in unifying motion comprehension and generation. However, existing\\napproaches primarily focus on coarse-grained motion-text modeling, where text\\ndescribes the overall semantics of an entire motion sequence in just a few\\nwords. This limits their ability to handle fine-grained motion-relevant tasks,\\nsuch as understanding and controlling the movements of specific body parts. To\\novercome this limitation, we pioneer MG-MotionLLM, a unified motion-language\\nmodel for multi-granular motion comprehension and generation. We further\\nintroduce a comprehensive multi-granularity training scheme by incorporating a\\nset of novel auxiliary tasks, such as localizing temporal boundaries of motion\\nsegments via detailed text as well as motion detailed captioning, to facilitate\\nmutual reinforcement for motion-text modeling across various levels of\\ngranularity. Extensive experiments show that our MG-MotionLLM achieves superior\\nperformance on classical text-to-motion and motion-to-text tasks, and exhibits\\npotential in novel fine-grained motion comprehension and editing tasks. Project\\npage: CVI-SZU/MG-MotionLLM","authors":["Bizhu Wu","Jinheng Xie","Keming Shen","Zhe Kong","Jianfeng Ren","Ruibin Bai","Rong Qu","Linlin Shen"],"year":2025,"month":4,"url":"https://arxiv.org/abs/2504.02478","survey":false,"survey_abbr":"","model":true,"model_abbr":"MG-MotionLLM","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2025","page":"","repo":"https://github.com/CVI-SZU/MG-MotionLLM","backbone_tags":"","approach_tags":"Finegrained"},{"arxiv_id":"2504.01338","title":"FlowMotion: Target-Predictive Conditional Flow Matching for Jitter-Reduced Text-Driven Human Motion Generation","abstract":"Achieving high-fidelity and temporally smooth 3D human motion generation\\nremains a challenge, particularly within resource-constrained environments. We\\nintroduce FlowMotion, a novel method leveraging Conditional Flow Matching\\n(CFM). FlowMotion incorporates a training objective within CFM that focuses on\\nmore accurately predicting target motion in 3D human motion generation,\\nresulting in enhanced generation fidelity and temporal smoothness while\\nmaintaining the fast synthesis times characteristic of flow-matching-based\\nmethods. FlowMotion achieves state-of-the-art jitter performance, achieving the\\nbest jitter in the KIT dataset and the second-best jitter in the HumanML3D\\ndataset, and a competitive FID value in both datasets. This combination\\nprovides robust and natural motion sequences, offering a promising equilibrium\\nbetween generation quality and temporal naturalness.","authors":["Manolo Canales Cuba","Vin\xedcius do Carmo Mel\xedcio","Jo\xe3o Paulo Gois"],"year":2025,"month":4,"url":"https://arxiv.org/abs/2504.01338","survey":false,"survey_abbr":"","model":true,"model_abbr":"FlowMotion","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"CLIP, Transformer, Flow Matching","approach_tags":"Jerk"},{"arxiv_id":"2504.01019","title":"MixerMDM: Learnable Composition of Human Motion Diffusion Models","abstract":"Generating human motion guided by conditions such as textual descriptions is\\nchallenging due to the need for datasets with pairs of high-quality motion and\\ntheir corresponding conditions. The difficulty increases when aiming for finer\\ncontrol in the generation. To that end, prior works have proposed to combine\\nseveral motion diffusion models pre-trained on datasets with different types of\\nconditions, thus allowing control with multiple conditions. However, the\\nproposed merging strategies overlook that the optimal way to combine the\\ngeneration processes might depend on the particularities of each pre-trained\\ngenerative model and also the specific textual descriptions. In this context,\\nwe introduce MixerMDM, the first learnable model composition technique for\\ncombining pre-trained text-conditioned human motion diffusion models. Unlike\\nprevious approaches, MixerMDM provides a dynamic mixing strategy that is\\ntrained in an adversarial fashion to learn to combine the denoising process of\\neach model depending on the set of conditions driving the generation. By using\\nMixerMDM to combine single- and multi-person motion diffusion models, we\\nachieve fine-grained control on the dynamics of every person individually, and\\nalso on the overall interaction. Furthermore, we propose a new evaluation\\ntechnique that, for the first time in this task, measures the interaction and\\nindividual quality by computing the alignment between the mixed generated\\nmotions and their conditions as well as the capabilities of MixerMDM to adapt\\nthe mixing throughout the denoising process depending on the motions to mix.","authors":["Pablo Ruiz-Ponce","German Barquero","Cristina Palmero","Sergio Escalera","Jos\xe9 Garc\xeda-Rodr\xedguez"],"year":2025,"month":4,"url":"https://arxiv.org/abs/2504.01019","survey":false,"survey_abbr":"","model":true,"model_abbr":"MixerMDM","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2025","page":"https://pabloruizponce.com/papers/MixerMDM","repo":"https://github.com/pabloruizponce/MixerMDM","backbone_tags":"Transformer, Diffusion","approach_tags":"Diversity"},{"arxiv_id":"2503.20724","title":"Dynamic Motion Blending for Versatile Motion Editing","abstract":"Text-guided motion editing enables high-level semantic control and iterative\\nmodifications beyond traditional keyframe animation. Existing methods rely on\\nlimited pre-collected training triplets, which severely hinders their\\nversatility in diverse editing scenarios. We introduce MotionCutMix, an online\\ndata augmentation technique that dynamically generates training triplets by\\nblending body part motions based on input text. While MotionCutMix effectively\\nexpands the training distribution, the compositional nature introduces\\nincreased randomness and potential body part incoordination. To model such a\\nrich distribution, we present MotionReFit, an auto-regressive diffusion model\\nwith a motion coordinator. The auto-regressive architecture facilitates\\nlearning by decomposing long sequences, while the motion coordinator mitigates\\nthe artifacts of motion composition. Our method handles both spatial and\\ntemporal motion edits directly from high-level human instructions, without\\nrelying on additional specifications or Large Language Models. Through\\nextensive experiments, we show that MotionReFit achieves state-of-the-art\\nperformance in text-guided motion editing.","authors":["Nan Jiang","Hongjie Li","Ziye Yuan","Zimo He","Yixin Chen","Tengyu Liu","Yixin Zhu","Siyuan Huang"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.20724","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionReFit","dataset":true,"dataset_abbr":"STANCE","submission":"CVPR","submission_year":"2025","page":"https://awfuact.github.io/motionrefit/","repo":"https://github.com/emptybulebox1/motionRefit/","backbone_tags":"Diffusion, Transformer, CLIP","approach_tags":"Editing"},{"arxiv_id":"2503.19557","title":"Dance Like a Chicken: Low-Rank Stylization for Human Motion Diffusion","abstract":"Text-to-motion generative models span a wide range of 3D human actions but\\nstruggle with nuanced stylistic attributes such as a \\"Chicken\\" style. Due to\\nthe scarcity of style-specific data, existing approaches pull the generative\\nprior towards a reference style, which often results in out-of-distribution low\\nquality generations. In this work, we introduce LoRA-MDM, a lightweight\\nframework for motion stylization that generalizes to complex actions while\\nmaintaining editability. Our key insight is that adapting the generative prior\\nto include the style, while preserving its overall distribution, is more\\neffective than modifying each individual motion during generation. Building on\\nthis idea, LoRA-MDM learns to adapt the prior to include the reference style\\nusing only a few samples. The style can then be used in the context of\\ndifferent textual prompts for generation. The low-rank adaptation shifts the\\nmotion manifold in a semantically meaningful way, enabling realistic style\\ninfusion even for actions not present in the reference samples. Moreover,\\npreserving the distribution structure enables advanced operations such as style\\nblending and motion editing. We compare LoRA-MDM to state-of-the-art stylized\\nmotion generation methods and demonstrate a favorable balance between text\\nfidelity and style consistency.","authors":["Haim Sawdayee","Chuan Guo","Guy Tevet","Bing Zhou","Jian Wang","Amit H. Bermano"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.19557","survey":false,"survey_abbr":"","model":true,"model_abbr":"LoRA-MDM","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://haimsaw.github.io/LoRA-MDM/","repo":"https://github.com/haimsaw/LoRA-MDM","backbone_tags":"Diffusion","approach_tags":"Trajectory, Diversity"},{"arxiv_id":"2503.18674","title":"Human Motion Unlearning","abstract":"We introduce the task of human motion unlearning to prevent the synthesis of\\ntoxic animations while preserving the general text-to-motion generative\\nperformance. Unlearning toxic motions is challenging as those can be generated\\nfrom explicit text prompts and from implicit toxic combinations of safe motions\\n(e.g., ``kicking\\" is ``loading and swinging a leg\\"). We propose the first\\nmotion unlearning benchmark by filtering toxic motions from the large and\\nrecent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines,\\nby adapting state-of-the-art image unlearning techniques to process\\nspatio-temporal signals. Finally, we propose a novel motion unlearning model\\nbased on Latent Code Replacement, which we dub LCR. LCR is training-free and\\nsuitable to the discrete latent spaces of state-of-the-art text-to-motion\\ndiffusion models. LCR is simple and consistently outperforms baselines\\nqualitatively and quantitatively. Project page:\\n\\\\href{https://www.pinlab.org/hmu}{https://www.pinlab.org/hmu}.","authors":["Edoardo De Matteis","Matteo Migliarini","Alessio Sampieri","Indro Spinelli","Fabio Galasso"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.18674","survey":false,"survey_abbr":"","model":true,"model_abbr":"HMU","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://www.pinlab.org/hmu","repo":"https://github.com/edodema/human-motion-unlearning","backbone_tags":"VQ-VAE","approach_tags":""},{"arxiv_id":"2503.18211","title":"SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity Prediction","abstract":"Text-based 3D human motion editing is a critical yet challenging task in\\ncomputer vision and graphics. While training-free approaches have been\\nexplored, the recent release of the MotionFix dataset, which includes\\nsource-text-motion triplets, has opened new avenues for training, yielding\\npromising results. However, existing methods struggle with precise control,\\noften leading to misalignment between motion semantics and language\\ninstructions. In this paper, we introduce a related task, motion similarity\\nprediction, and propose a multi-task training paradigm, where we train the\\nmodel jointly on motion editing and motion similarity prediction to foster the\\nlearning of semantically meaningful representations. To complement this task,\\nwe design an advanced Diffusion-Transformer-based architecture that separately\\nhandles motion similarity prediction and motion editing. Extensive experiments\\ndemonstrate the state-of-the-art performance of our approach in both editing\\nalignment and fidelity.","authors":["Zhengyuan Li","Kai Cheng","Anindita Ghosh","Uttaran Bhattacharya","Liangyan Gui","Aniket Bera"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.18211","survey":false,"survey_abbr":"","model":true,"model_abbr":"SimMotionEdit","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2025","page":"https://ideas.cs.purdue.edu/research/projects/sim-motion-edit/","repo":"https://github.com/lzhyu/SimMotionEdit","backbone_tags":"CLIP, Diffusion, Transformer","approach_tags":"Multi-Task, Editing"},{"arxiv_id":"2503.15451","title":"MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space","abstract":"This paper addresses the challenge of text-conditioned streaming motion\\ngeneration, which requires us to predict the next-step human pose based on\\nvariable-length historical motions and incoming texts. Existing methods\\nstruggle to achieve streaming motion generation, e.g., diffusion models are\\nconstrained by pre-defined motion lengths, while GPT-based methods suffer from\\ndelayed response and error accumulation problem due to discretized non-causal\\ntokenization. To solve these problems, we propose MotionStreamer, a novel\\nframework that incorporates a continuous causal latent space into a\\nprobabilistic autoregressive model. The continuous latents mitigate information\\nloss caused by discretization and effectively reduce error accumulation during\\nlong-term autoregressive generation. In addition, by establishing temporal\\ncausal dependencies between current and historical motion latents, our model\\nfully utilizes the available information to achieve accurate online motion\\ndecoding. Experiments show that our method outperforms existing approaches\\nwhile offering more applications, including multi-round generation, long-term\\ngeneration, and dynamic motion composition. Project Page:\\nhttps://zju3dv.github.io/MotionStreamer/","authors":["Lixing Xiao","Shunlin Lu","Huaijin Pi","Ke Fan","Liang Pan","Yueer Zhou","Ziyong Feng","Xiaowei Zhou","Sida Peng","Jingbo Wang"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.15451","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionStreamer","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2025","page":"https://zju3dv.github.io/MotionStreamer/","repo":"https://github.com/zju3dv/MotionStreamer/","backbone_tags":"Diffusion, Transformer","approach_tags":""},{"arxiv_id":"2503.14919","title":"GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation","abstract":"Scaling up motion datasets is crucial to enhance motion generation\\ncapabilities. However, training on large-scale multi-source datasets introduces\\ndata heterogeneity challenges due to variations in motion content. To address\\nthis, we propose Generative Pretrained Multi-path Motion Model (GenM\\\\(^3\\\\)), a\\ncomprehensive framework designed to learn unified motion representations.\\nGenM\\\\(^3\\\\) comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that\\nadapts to different dataset distributions to learn a unified discrete motion\\nrepresentation, and 2) a Multi-path Motion Transformer (MMT) that improves\\nintra-modal representations by using separate modality-specific pathways, each\\nwith densely activated experts to accommodate variations within that modality,\\nand improves inter-modal alignment by the text-motion shared pathway. To enable\\nlarge-scale training, we integrate and unify 11 high-quality motion datasets\\n(approximately 220 hours of motion data) and augment it with textual\\nannotations (nearly 10,000 motion sequences labeled by a large language model\\nand 300+ by human experts). After training on our integrated dataset,\\nGenM\\\\(^3\\\\) achieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark,\\nsurpassing state-of-the-art methods by a large margin. It also demonstrates\\nstrong zero-shot generalization on IDEA400 dataset, highlighting its\\neffectiveness and adaptability across diverse motion scenarios.","authors":["Junyu Shi","Lijiang Liu","Yong Sun","Zhiyuan Zhang","Jinni Zhou","Qiang Nie"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.14919","survey":false,"survey_abbr":"","model":true,"model_abbr":"GenM3","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"VQ-VAE, CLIP, Transformer","approach_tags":"Pre-Train"},{"arxiv_id":"2503.14637","title":"Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control","abstract":"How do humans move? The quest to understand human motion has broad\\napplications in numerous fields, ranging from computer animation and motion\\nsynthesis to neuroscience, human prosthetics and rehabilitation. Although\\nadvances in reinforcement learning (RL) have produced impressive results in\\ncapturing human motion using simplified humanoids, controlling physiologically\\naccurate models of the body remains an open challenge. In this work, we present\\na model-free motion imitation framework (KINESIS) to advance the understanding\\nof muscle-based motor control. Using a musculoskeletal model of the lower body\\nwith 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves\\nstrong imitation performance on 1.9 hours of motion capture data, is\\ncontrollable by natural language through pre-trained text-to-motion generative\\nmodels, and can be fine-tuned to carry out high-level tasks such as target goal\\nreaching. Importantly, KINESIS generates muscle activity patterns that\\ncorrelate well with human EMG activity. The physiological plausibility makes\\nKINESIS a promising model for tackling challenging problems in human motor\\ncontrol theory, which we highlight by investigating Bernstein\'s redundancy\\nproblem in the context of locomotion. Code, videos and benchmarks will be\\navailable at https://github.com/amathislab/Kinesis.","authors":["Merkourios Simos","Alberto Silvio Chiappa","Alexander Mathis"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.14637","survey":false,"survey_abbr":"","model":true,"model_abbr":"Kinesis","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"https://github.com/amathislab/Kinesis","backbone_tags":"","approach_tags":"Imitation Learning, RL"},{"arxiv_id":"2503.13859","title":"Less is More: Improving Motion Diffusion Models with Sparse Keyframes","abstract":"Recent advances in motion diffusion models have led to remarkable progress in\\ndiverse motion generation tasks, including text-to-motion synthesis. However,\\nexisting approaches represent motions as dense frame sequences, requiring the\\nmodel to process redundant or less informative frames. The processing of dense\\nanimation frames imposes significant training complexity, especially when\\nlearning intricate distributions of large motion datasets even with modern\\nneural architectures. This severely limits the performance of generative motion\\nmodels for downstream tasks. Inspired by professional animators who mainly\\nfocus on sparse keyframes, we propose a novel diffusion framework explicitly\\ndesigned around sparse and geometrically meaningful keyframes. Our method\\nreduces computation by masking non-keyframes and efficiently interpolating\\nmissing frames. We dynamically refine the keyframe mask during inference to\\nprioritize informative frames in later diffusion steps. Extensive experiments\\nshow that our approach consistently outperforms state-of-the-art methods in\\ntext alignment and motion realism, while also effectively maintaining high\\nperformance at significantly fewer diffusion steps. We further validate the\\nrobustness of our framework by using it as a generative prior and adapting it\\nto different downstream tasks.","authors":["Jinseok Bae","Inwoo Hwang","Young Yoon Lee","Ziyu Guo","Joseph Liu","Yizhak Ben-Shabat","Young Min Kim","Mubbasir Kapadia"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.13859","survey":false,"survey_abbr":"","model":true,"model_abbr":"sMDM","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Diffusion","approach_tags":""},{"arxiv_id":"2503.13836","title":"SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing","abstract":"Text-driven motion generation has advanced significantly with the rise of\\ndenoising diffusion models. However, previous methods often oversimplify\\nrepresentations for the skeletal joints, temporal frames, and textual words,\\nlimiting their ability to fully capture the information within each modality\\nand their interactions. Moreover, when using pre-trained models for downstream\\ntasks, such as editing, they typically require additional efforts, including\\nmanual interventions, optimization, or fine-tuning. In this paper, we introduce\\na skeleton-aware latent diffusion (SALAD), a model that explicitly captures the\\nintricate inter-relationships between joints, frames, and words. Furthermore,\\nby leveraging cross-attention maps produced during the generation process, we\\nenable attention-based zero-shot text-driven motion editing using a pre-trained\\nSALAD model, requiring no additional user input beyond text prompts. Our\\napproach significantly outperforms previous methods in terms of text-motion\\nalignment without compromising generation quality, and demonstrates practical\\nversatility by providing diverse editing capabilities beyond generation. Code\\nis available at project page.","authors":["Seokhyeon Hong","Chaelin Kim","Serin Yoon","Junghyun Nam","Sihun Cha","Junyong Noh"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.13836","survey":false,"survey_abbr":"","model":true,"model_abbr":"SALAD","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2025","page":"https://seokhyeonhong.github.io/projects/salad/","repo":"https://github.com/seokhyeonhong/salad","backbone_tags":"CLIP, VAE","approach_tags":"Finegrained, Editing"},{"arxiv_id":"2503.13300","title":"Progressive Human Motion Generation Based on Text and Few Motion Frames","abstract":"Although existing text-to-motion (T2M) methods can produce realistic human\\nmotion from text description, it is still difficult to align the generated\\nmotion with the desired postures since using text alone is insufficient for\\nprecisely describing diverse postures. To achieve more controllable generation,\\nan intuitive way is to allow the user to input a few motion frames describing\\nprecise desired postures. Thus, we explore a new Text-Frame-to-Motion (TF2M)\\ngeneration task that aims to generate motions from text and very few given\\nframes. Intuitively, the closer a frame is to a given frame, the lower the\\nuncertainty of this frame is when conditioned on this given frame. Hence, we\\npropose a novel Progressive Motion Generation (PMG) method to progressively\\ngenerate a motion from the frames with low uncertainty to those with high\\nuncertainty in multiple stages. During each stage, new frames are generated by\\na Text-Frame Guided Generator conditioned on frame-aware semantics of the text,\\ngiven frames, and frames generated in previous stages. Additionally, to\\nalleviate the train-test gap caused by multi-stage accumulation of incorrectly\\ngenerated frames during testing, we propose a Pseudo-frame Replacement Strategy\\nfor training. Experimental results show that our PMG outperforms existing T2M\\ngeneration methods by a large margin with even one given frame, validating the\\neffectiveness of our PMG. Code is available at\\nhttps://github.com/qinghuannn/PMG.","authors":["Ling-An Zeng","Gaojie Wu","Ancong Wu","Jian-Fang Hu","Wei-Shi Zheng"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.13300","survey":false,"survey_abbr":"","model":true,"model_abbr":"PMG","dataset":false,"dataset_abbr":"","submission":"TCSVT","submission_year":"2025","page":"","repo":"https://github.com/qinghuannn/PMG","backbone_tags":"Transformer","approach_tags":""},{"arxiv_id":"2503.07390","title":"PersonaBooth: Personalized Text-to-Motion Generation","abstract":"This paper introduces Motion Personalization, a new task that generates\\npersonalized motions aligned with text descriptions using several basic motions\\ncontaining Persona. To support this novel task, we introduce a new large-scale\\nmotion dataset called PerMo (PersonaMotion), which captures the unique personas\\nof multiple actors. We also propose a multi-modal finetuning method of a\\npretrained motion diffusion model called PersonaBooth. PersonaBooth addresses\\ntwo main challenges: i) A significant distribution gap between the\\npersona-focused PerMo dataset and the pretraining datasets, which lack\\npersona-specific data, and ii) the difficulty of capturing a consistent persona\\nfrom the motions vary in content (action type). To tackle the dataset\\ndistribution gap, we introduce a persona token to accept new persona features\\nand perform multi-modal adaptation for both text and visuals during finetuning.\\nTo capture a consistent persona, we incorporate a contrastive learning\\ntechnique to enhance intra-cohesion among samples with the same persona.\\nFurthermore, we introduce a context-aware fusion mechanism to maximize the\\nintegration of persona cues from multiple input motions. PersonaBooth\\noutperforms state-of-the-art motion style transfer methods, establishing a new\\nbenchmark for motion personalization.","authors":["Boeun Kim","Hea In Jeong","JungHoon Sung","Yihua Cheng","Jeongmin Lee","Ju Yong Chang","Sang-Il Choi","Younggeun Choi","Saim Shin","Jungho Kim","Hyung Jin Chang"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.07390","survey":false,"survey_abbr":"","model":true,"model_abbr":"PersonaBooth","dataset":true,"dataset_abbr":"PerMo","submission":"CVPR","submission_year":"2025","page":"https://boeun-kim.github.io/page-PersonaBooth/","repo":"https://github.com/Boeun-Kim/PersonaBooth","backbone_tags":"CLIP, Diffusion, Transformer","approach_tags":""},{"arxiv_id":"2503.06955","title":"Motion Anything: Any to Motion Generation","abstract":"Conditional motion generation has been extensively studied in computer\\nvision, yet two critical challenges remain. First, while masked autoregressive\\nmethods have recently outperformed diffusion-based approaches, existing masking\\nmodels lack a mechanism to prioritize dynamic frames and body parts based on\\ngiven conditions. Second, existing methods for different conditioning\\nmodalities often fail to integrate multiple modalities effectively, limiting\\ncontrol and coherence in generated motion. To address these challenges, we\\npropose Motion Anything, a multimodal motion generation framework that\\nintroduces an Attention-based Mask Modeling approach, enabling fine-grained\\nspatial and temporal control over key frames and actions. Our model adaptively\\nencodes multimodal conditions, including text and music, improving\\ncontrollability. Additionally, we introduce Text-Music-Dance (TMD), a new\\nmotion dataset consisting of 2,153 pairs of text, music, and dance, making it\\ntwice the size of AIST++, thereby filling a critical gap in the community.\\nExtensive experiments demonstrate that Motion Anything surpasses\\nstate-of-the-art methods across multiple benchmarks, achieving a 15%\\nimprovement in FID on HumanML3D and showing consistent performance gains on\\nAIST++ and TMD. See our project website\\nhttps://steve-zeyu-zhang.github.io/MotionAnything","authors":["Zeyu Zhang","Yiran Wang","Wei Mao","Danning Li","Rui Zhao","Biao Wu","Zirui Song","Bohan Zhuang","Ian Reid","Richard Hartley"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.06955","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionAnything","dataset":true,"dataset_abbr":"TMD","submission":"","submission_year":"","page":"https://steve-zeyu-zhang.github.io/MotionAnything/","repo":"","backbone_tags":"VQ-VAE, CLIP, Transformer","approach_tags":""},{"arxiv_id":"2503.06151","title":"Biomechanics-Guided Residual Approach to Generalizable Human Motion Generation and Estimation","abstract":"Human pose, action, and motion generation are critical for applications in\\ndigital humans, character animation, and humanoid robotics. However, many\\nexisting methods struggle to produce physically plausible movements that are\\nconsistent with biomechanical principles. Although recent autoregressive and\\ndiffusion models deliver impressive visual quality, they often neglect key\\nbiodynamic features and fail to ensure physically realistic motions.\\nReinforcement Learning (RL) approaches can address these shortcomings but are\\nhighly dependent on simulation environments, limiting their generalizability.\\nTo overcome these challenges, we propose BioVAE, a biomechanics-aware framework\\nwith three core innovations: (1) integration of muscle electromyography (EMG)\\nsignals and kinematic features with acceleration constraints to enable\\nphysically plausible motion without simulations; (2) seamless coupling with\\ndiffusion models for stable end-to-end training; and (3) biomechanical priors\\nthat promote strong generalization across diverse motion generation and\\nestimation tasks. Extensive experiments demonstrate that BioVAE achieves\\nstate-of-the-art performance on multiple benchmarks, bridging the gap between\\ndata-driven motion synthesis and biomechanical authenticity while setting new\\nstandards for physically accurate motion generation and pose estimation.","authors":["Zixi Kang","Xinghan Wang","Yadong Mu"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.06151","survey":false,"survey_abbr":"","model":true,"model_abbr":"BioVAE","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"UNet","approach_tags":"Physical"},{"arxiv_id":"2503.06119","title":"Unlocking Pretrained LLMs for Motion-Related Multimodal Generation: A Fine-Tuning Approach to Unify Diffusion and Next-Token Prediction","abstract":"In this paper, we propose a unified framework that leverages a single\\npretrained LLM for Motion-related Multimodal Generation, referred to as MoMug.\\nMoMug integrates diffusion-based continuous motion generation with the model\'s\\ninherent autoregressive discrete text prediction capabilities by fine-tuning a\\npretrained LLM. This enables seamless switching between continuous motion\\noutput and discrete text token prediction within a single model architecture,\\neffectively combining the strengths of both diffusion- and LLM-based\\napproaches. Experimental results show that, compared to the most recent\\nLLM-based baseline, MoMug improves FID by 38% and mean accuracy across seven\\nmetrics by 16.61% on the text-to-motion task. Additionally, it improves mean\\naccuracy across eight metrics by 8.44% on the text-to-motion task. To the best\\nof our knowledge, this is the first approach to integrate diffusion- and\\nLLM-based generation within a single model for motion-related multimodal tasks\\nwhile maintaining low training costs. This establishes a foundation for future\\nadvancements in motion-related generation, paving the way for high-quality yet\\ncost-efficient motion synthesis.","authors":["Shinichi Tanaka","Zhao Wang","Yoichi Kato","Jun Ohya"],"year":2025,"month":3,"url":"https://arxiv.org/abs/2503.06119","survey":false,"survey_abbr":"","model":true,"model_abbr":"MoMug","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Diffusion, Transformer, LLM","approach_tags":"LoRA"},{"arxiv_id":"2502.05534","title":"Fg-T2M++: LLMs-Augmented Fine-Grained Text Driven Human Motion Generation","abstract":"We address the challenging problem of fine-grained text-driven human motion\\ngeneration. Existing works generate imprecise motions that fail to accurately\\ncapture relationships specified in text due to: (1) lack of effective text\\nparsing for detailed semantic cues regarding body parts, (2) not fully modeling\\nlinguistic structures between words to comprehend text comprehensively. To\\ntackle these limitations, we propose a novel fine-grained framework Fg-T2M++\\nthat consists of: (1) an LLMs semantic parsing module to extract body part\\ndescriptions and semantics from text, (2) a hyperbolic text representation\\nmodule to encode relational information between text units by embedding the\\nsyntactic dependency graph into hyperbolic space, and (3) a multi-modal fusion\\nmodule to hierarchically fuse text and motion features. Extensive experiments\\non HumanML3D and KIT-ML datasets demonstrate that Fg-T2M++ outperforms SOTA\\nmethods, validating its ability to accurately generate motions adhering to\\ncomprehensive text semantics.","authors":["Yin Wang","Mu Li","Jiapeng Liu","Zhiying Leng","Frederick W. B. Li","Ziyao Zhang","Xiaohui Liang"],"year":2025,"month":2,"url":"https://arxiv.org/abs/2502.05534","survey":false,"survey_abbr":"","model":true,"model_abbr":"Fg-T2M++","dataset":false,"dataset_abbr":"","submission":"IJCV","submission_year":"2025","page":"","repo":"","backbone_tags":"LLM, Diffusion","approach_tags":"Graph"},{"arxiv_id":"2502.02358","title":"MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm","abstract":"Human motion generation and editing are key components of computer vision.\\nHowever, current approaches in this field tend to offer isolated solutions\\ntailored to specific tasks, which can be inefficient and impractical for\\nreal-world applications. While some efforts have aimed to unify motion-related\\ntasks, these methods simply use different modalities as conditions to guide\\nmotion generation. Consequently, they lack editing capabilities, fine-grained\\ncontrol, and fail to facilitate knowledge sharing across tasks. To address\\nthese limitations and provide a versatile, unified framework capable of\\nhandling both human motion generation and editing, we introduce a novel\\nparadigm: \\\\textbf{Motion-Condition-Motion}, which enables the unified\\nformulation of diverse tasks with three concepts: source motion, condition, and\\ntarget motion. Based on this paradigm, we propose a unified framework,\\n\\\\textbf{MotionLab}, which incorporates rectified flows to learn the mapping\\nfrom source motion to target motion, guided by the specified conditions. In\\nMotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional\\ngeneration and editing without task-specific modules; 2) Aligned Rotational\\nPosition Encoding to guarantee the time synchronization between source motion\\nand target motion; 3) Task Specified Instruction Modulation; and 4) Motion\\nCurriculum Learning for effective multi-task learning and knowledge sharing\\nacross tasks. Notably, our MotionLab demonstrates promising generalization\\ncapabilities and inference efficiency across multiple benchmarks for human\\nmotion. Our code and additional video results are available at:\\nhttps://diouo.github.io/motionlab.github.io/.","authors":["Ziyan Guo","Zeyu Hu","De Wen Soh","Na Zhao"],"year":2025,"month":2,"url":"https://arxiv.org/abs/2502.02358","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionLab","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2025","page":"https://diouo.github.io/motionlab.github.io/","repo":"https://github.com/Diouo/MotionLab","backbone_tags":"Transformer","approach_tags":"Editing, Multi-Task"},{"arxiv_id":"2502.02063","title":"CASIM: Composite Aware Semantic Injection for Text to Motion Generation","abstract":"Recent advances in generative modeling and tokenization have driven\\nsignificant progress in text-to-motion generation, leading to enhanced quality\\nand realism in generated motions. However, effectively leveraging textual\\ninformation for conditional motion generation remains an open challenge. We\\nobserve that current approaches, primarily relying on fixed-length text\\nembeddings (e.g., CLIP) for global semantic injection, struggle to capture the\\ncomposite nature of human motion, resulting in suboptimal motion quality and\\ncontrollability. To address this limitation, we propose the Composite Aware\\nSemantic Injection Mechanism (CASIM), comprising a composite-aware semantic\\nencoder and a text-motion aligner that learns the dynamic correspondence\\nbetween text and motion tokens. Notably, CASIM is model and\\nrepresentation-agnostic, readily integrating with both autoregressive and\\ndiffusion-based methods. Experiments on HumanML3D and KIT benchmarks\\ndemonstrate that CASIM consistently improves motion quality, text-motion\\nalignment, and retrieval scores across state-of-the-art methods. Qualitative\\nanalyses further highlight the superiority of our composite-aware approach over\\nfixed-length semantic injection, enabling precise motion control from text\\nprompts and stronger generalization to unseen text inputs.","authors":["Che-Jui Chang","Qingze Tony Liu","Honglu Zhou","Vladimir Pavlovic","Mubbasir Kapadia"],"year":2025,"month":2,"url":"https://arxiv.org/abs/2502.02063","survey":false,"survey_abbr":"","model":true,"model_abbr":"CASIM","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://cjerry1243.github.io/casim_t2m/","repo":"https://github.com/cjerry1243/casim_t2m","backbone_tags":"Transformer, Diffusion","approach_tags":"Component"},{"arxiv_id":"2501.19083","title":"MotionPCM: Real-Time Motion Synthesis with Phased Consistency Model","abstract":"Diffusion models have become a popular choice for human motion synthesis due\\nto their powerful generative capabilities. However, their high computational\\ncomplexity and large sampling steps pose challenges for real-time applications.\\nFortunately, the Consistency Model (CM) provides a solution to greatly reduce\\nthe number of sampling steps from hundreds to a few, typically fewer than four,\\nsignificantly accelerating the synthesis of diffusion models. However, applying\\nCM to text-conditioned human motion synthesis in latent space yields\\nunsatisfactory generation results. In this paper, we introduce\\n\\\\textbf{MotionPCM}, a phased consistency model-based approach designed to\\nimprove the quality and efficiency for real-time motion synthesis in latent\\nspace. Experimental results on the HumanML3D dataset show that our model\\nachieves real-time inference at over 30 frames per second in a single sampling\\nstep while outperforming the previous state-of-the-art with a 38.9\\\\%\\nimprovement in FID. The code will be available for reproduction.","authors":["Lei Jiang","Ye Wei","Hao Ni"],"year":2025,"month":1,"url":"https://arxiv.org/abs/2501.19083","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionPCM","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Transformer, VAE, Diffusion","approach_tags":"Efficient"},{"arxiv_id":"2501.18232","title":"Free-T2M: Frequency Enhanced Text-to-Motion Diffusion Model With Consistency Loss","abstract":"Rapid progress in text-to-motion generation has been largely driven by\\ndiffusion models. However, existing methods focus solely on temporal modeling,\\nthereby overlooking frequency-domain analysis. We identify two key phases in\\nmotion denoising: the **semantic planning stage** and the **fine-grained\\nimproving stage**. To address these phases effectively, we propose\\n**Fre**quency **e**nhanced **t**ext-**to**-**m**otion diffusion model\\n(**Free-T2M**), incorporating stage-specific consistency losses that enhance\\nthe robustness of static features and improve fine-grained accuracy. Extensive\\nexperiments demonstrate the effectiveness of our method. Specifically, on\\nStableMoFusion, our method reduces the FID from **0.189** to **0.051**,\\nestablishing a new SOTA performance within the diffusion architecture. These\\nfindings highlight the importance of incorporating frequency-domain insights\\ninto text-to-motion generation for more precise and robust results.","authors":["Wenshuo Chen","Haozhe Jia","Songning Lai","Keming Wu","Hongru Xiao","Lijie Hu","Yutao Yue"],"year":2025,"month":1,"url":"https://arxiv.org/abs/2501.18232","survey":false,"survey_abbr":"","model":true,"model_abbr":"Free-T2M","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"","approach_tags":"Component, Frequency"},{"arxiv_id":"2501.16778","title":"FlexMotion: Lightweight, Physics-Aware, and Controllable Human Motion Generation","abstract":"Lightweight, controllable, and physically plausible human motion synthesis is\\ncrucial for animation, virtual reality, robotics, and human-computer\\ninteraction applications. Existing methods often compromise between\\ncomputational efficiency, physical realism, or spatial controllability. We\\npropose FlexMotion, a novel framework that leverages a computationally\\nlightweight diffusion model operating in the latent space, eliminating the need\\nfor physics simulators and enabling fast and efficient training. FlexMotion\\nemploys a multimodal pre-trained Transformer encoder-decoder, integrating joint\\nlocations, contact forces, joint actuations and muscle activations to ensure\\nthe physical plausibility of the generated motions. FlexMotion also introduces\\na plug-and-play module, which adds spatial controllability over a range of\\nmotion parameters (e.g., joint locations, joint actuations, contact forces, and\\nmuscle activations). Our framework achieves realistic motion generation with\\nimproved efficiency and control, setting a new benchmark for human motion\\nsynthesis. We evaluate FlexMotion on extended datasets and demonstrate its\\nsuperior performance in terms of realism, physical plausibility, and\\ncontrollability.","authors":["Arvin Tashakori","Arash Tashakori","Gongbo Yang","Z. Jane Wang","Peyman Servati"],"year":2025,"month":1,"url":"https://arxiv.org/abs/2501.16778","survey":false,"survey_abbr":"","model":true,"model_abbr":"FlexMotion","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://openreview.net/forum?id=7652tHbbVE","repo":"","backbone_tags":"Diffusion","approach_tags":"Physical, Efficient, Finegrained"},{"arxiv_id":"2501.16551","title":"PackDiT: Joint Human Motion and Text Generation via Mutual Prompting","abstract":"Human motion generation has advanced markedly with the advent of diffusion\\nmodels. Most recent studies have concentrated on generating motion sequences\\nbased on text prompts, commonly referred to as text-to-motion generation.\\nHowever, the bidirectional generation of motion and text, enabling tasks such\\nas motion-to-text alongside text-to-motion, has been largely unexplored. This\\ncapability is essential for aligning diverse modalities and supports\\nunconditional generation. In this paper, we introduce PackDiT, the first\\ndiffusion-based generative model capable of performing various tasks\\nsimultaneously, including motion generation, motion prediction, text\\ngeneration, text-to-motion, motion-to-text, and joint motion-text generation.\\nOur core innovation leverages mutual blocks to integrate multiple diffusion\\ntransformers (DiTs) across different modalities seamlessly. We train PackDiT on\\nthe HumanML3D dataset, achieving state-of-the-art text-to-motion performance\\nwith an FID score of 0.106, along with superior results in motion prediction\\nand in-between tasks. Our experiments further demonstrate that diffusion models\\nare effective for motion-to-text generation, achieving performance comparable\\nto that of autoregressive models.","authors":["Zhongyu Jiang","Wenhao Chai","Zhuoran Zhou","Cheng-Yen Yang","Hsiang-Wei Huang","Jenq-Neng Hwang"],"year":2025,"month":1,"url":"https://arxiv.org/abs/2501.16551","survey":false,"survey_abbr":"","model":true,"model_abbr":"PackDiT","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Diffusion, Transformer","approach_tags":"Multi-Task"},{"arxiv_id":"2501.05098","title":"Motion-X++: A Large-Scale Multimodal 3D Whole-body Human Motion Dataset","abstract":"In this paper, we introduce Motion-X++, a large-scale multimodal 3D\\nexpressive whole-body human motion dataset. Existing motion datasets\\npredominantly capture body-only poses, lacking facial expressions, hand\\ngestures, and fine-grained pose descriptions, and are typically limited to lab\\nsettings with manually labeled text descriptions, thereby restricting their\\nscalability. To address this issue, we develop a scalable annotation pipeline\\nthat can automatically capture 3D whole-body human motion and comprehensive\\ntextural labels from RGB videos and build the Motion-X dataset comprising 81.1K\\ntext-motion pairs. Furthermore, we extend Motion-X into Motion-X++ by improving\\nthe annotation pipeline, introducing more data modalities, and scaling up the\\ndata quantities. Motion-X++ provides 19.5M 3D whole-body pose annotations\\ncovering 120.5K motion sequences from massive scenes, 80.8K RGB videos, 45.3K\\naudios, 19.5M frame-level whole-body pose descriptions, and 120.5K\\nsequence-level semantic labels. Comprehensive experiments validate the accuracy\\nof our annotation pipeline and highlight Motion-X++\'s significant benefits for\\ngenerating expressive, precise, and natural motion with paired multimodal\\nlabels supporting several downstream tasks, including text-driven whole-body\\nmotion generation,audio-driven motion generation, 3D whole-body human mesh\\nrecovery, and 2D whole-body keypoints estimation, etc.","authors":["Yuhong Zhang","Jing Lin","Ailing Zeng","Guanlin Wu","Shunlin Lu","Yurong Fu","Yuanhao Cai","Ruimao Zhang","Haoqian Wang","Lei Zhang"],"year":2025,"month":1,"url":"https://arxiv.org/abs/2501.05098","survey":false,"survey_abbr":"","model":false,"model_abbr":"","dataset":true,"dataset_abbr":"Motion-X++","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"","approach_tags":""},{"arxiv_id":"2501.01449","title":"LS-GAN: Human Motion Synthesis with Latent-space GANs","abstract":"Human motion synthesis conditioned on textual input has gained significant\\nattention in recent years due to its potential applications in various domains\\nsuch as gaming, film production, and virtual reality. Conditioned Motion\\nsynthesis takes a text input and outputs a 3D motion corresponding to the text.\\nWhile previous works have explored motion synthesis using raw motion data and\\nlatent space representations with diffusion models, these approaches often\\nsuffer from high training and inference times. In this paper, we introduce a\\nnovel framework that utilizes Generative Adversarial Networks (GANs) in the\\nlatent space to enable faster training and inference while achieving results\\ncomparable to those of the state-of-the-art diffusion methods. We perform\\nexperiments on the HumanML3D, HumanAct12 benchmarks and demonstrate that a\\nremarkably simple GAN in the latent space achieves a FID of 0.482 with more\\nthan 91% in FLOPs reduction compared to latent diffusion model. Our work opens\\nup new possibilities for efficient and high-quality motion synthesis using\\nlatent space GANs.","authors":["Avinash Amballa","Gayathri Akkinapalli","Vinitra Muralikrishnan"],"year":2024,"month":12,"url":"https://arxiv.org/abs/2501.01449","survey":false,"survey_abbr":"","model":true,"model_abbr":"LS-GAN","dataset":false,"dataset_abbr":"","submission":"WACV","submission_year":"2025","page":"","repo":"https://github.com/AmballaAvinash/motion-latent-diffusion","backbone_tags":"GAN","approach_tags":"Efficient"},{"arxiv_id":"2412.14706","title":"EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space","abstract":"Diffusion models, particularly latent diffusion models, have demonstrated\\nremarkable success in text-driven human motion generation. However, it remains\\nchallenging for latent diffusion models to effectively compose multiple\\nsemantic concepts into a single, coherent motion sequence. To address this\\nissue, we propose EnergyMoGen, which includes two spectrums of Energy-Based\\nModels: (1) We interpret the diffusion model as a latent-aware energy-based\\nmodel that generates motions by composing a set of diffusion models in latent\\nspace; (2) We introduce a semantic-aware energy model based on cross-attention,\\nwhich enables semantic composition and adaptive gradient descent for text\\nembeddings. To overcome the challenges of semantic inconsistency and motion\\ndistortion across these two spectrums, we introduce Synergistic Energy Fusion.\\nThis design allows the motion latent diffusion model to synthesize\\nhigh-quality, complex motions by combining multiple energy terms corresponding\\nto textual descriptions. Experiments show that our approach outperforms\\nexisting state-of-the-art models on various motion generation tasks, including\\ntext-to-motion generation, compositional motion generation, and multi-concept\\nmotion generation. Additionally, we demonstrate that our method can be used to\\nextend motion datasets and improve the text-to-motion task.","authors":["Jianrong Zhang","Hehe Fan","Yi Yang"],"year":2024,"month":12,"url":"https://arxiv.org/abs/2412.14706","survey":false,"survey_abbr":"","model":true,"model_abbr":"EnergyMoGen","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2025","page":"https://jiro-zhang.github.io/EnergyMoGen/","repo":"","backbone_tags":"VAE, Diffusion","approach_tags":"Energy-based, Diversity"},{"arxiv_id":"2412.14559","title":"ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation Model","abstract":"The scaling law has been validated in various domains, such as natural\\nlanguage processing (NLP) and massive computer vision tasks; however, its\\napplication to motion generation remains largely unexplored. In this paper, we\\nintroduce a scalable motion generation framework that includes the motion\\ntokenizer Motion FSQ-VAE and a text-prefix autoregressive transformer. Through\\ncomprehensive experiments, we observe the scaling behavior of this system. For\\nthe first time, we confirm the existence of scaling laws within the context of\\nmotion generation. Specifically, our results demonstrate that the normalized\\ntest loss of our prefix autoregressive models adheres to a logarithmic law in\\nrelation to compute budgets. Furthermore, we also confirm the power law between\\nNon-Vocabulary Parameters, Vocabulary Parameters, and Data Tokens with respect\\nto compute budgets respectively. Leveraging the scaling law, we predict the\\noptimal transformer size, vocabulary size, and data requirements for a compute\\nbudget of $1e18$. The test loss of the system, when trained with the optimal\\nmodel size, vocabulary size, and required data, aligns precisely with the\\npredicted test loss, thereby validating the scaling law.","authors":["Shunlin Lu","Jingbo Wang","Zeyu Lu","Ling-Hao Chen","Wenxun Dai","Junting Dong","Zhiyang Dou","Bo Dai","Ruimao Zhang"],"year":2024,"month":12,"url":"https://arxiv.org/abs/2412.14559","survey":false,"survey_abbr":"","model":true,"model_abbr":"ScaMo","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2025","page":"https://shunlinlu.github.io/ScaMo/","repo":"https://github.com/shunlinlu/ScaMo_code","backbone_tags":"VQ-VAE, Transformer","approach_tags":""},{"arxiv_id":"2412.11193","title":"Light-T2M: A Lightweight and Fast Model for Text-to-motion Generation","abstract":"Despite the significant role text-to-motion (T2M) generation plays across\\nvarious applications, current methods involve a large number of parameters and\\nsuffer from slow inference speeds, leading to high usage costs. To address\\nthis, we aim to design a lightweight model to reduce usage costs. First, unlike\\nexisting works that focus solely on global information modeling, we recognize\\nthe importance of local information modeling in the T2M task by reconsidering\\nthe intrinsic properties of human motion, leading us to propose a lightweight\\nLocal Information Modeling Module. Second, we introduce Mamba to the T2M task,\\nreducing the number of parameters and GPU memory demands, and we have designed\\na novel Pseudo-bidirectional Scan to replicate the effects of a bidirectional\\nscan without increasing parameter count. Moreover, we propose a novel Adaptive\\nTextual Information Injector that more effectively integrates textual\\ninformation into the motion during generation. By integrating the\\naforementioned designs, we propose a lightweight and fast model named\\nLight-T2M. Compared to the state-of-the-art method, MoMask, our Light-T2M model\\nfeatures just 10\\\\% of the parameters (4.48M vs 44.85M) and achieves a 16\\\\%\\nfaster inference time (0.152s vs 0.180s), while surpassing MoMask with an FID\\nof \\\\textbf{0.040} (vs. 0.045) on HumanML3D dataset and 0.161 (vs. 0.228) on\\nKIT-ML dataset. The code is available at\\nhttps://github.com/qinghuannn/light-t2m.","authors":["Ling-An Zeng","Guohong Huang","Gaojie Wu","Wei-Shi Zheng"],"year":2024,"month":12,"url":"https://arxiv.org/abs/2412.11193","survey":false,"survey_abbr":"","model":true,"model_abbr":"Light-T2M","dataset":false,"dataset_abbr":"","submission":"AAAI","submission_year":"2025","page":"","repo":"https://github.com/qinghuannn/light-t2m","backbone_tags":"Mamba","approach_tags":"Efficient"},{"arxiv_id":"2412.10523","title":"The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion","abstract":"Human communication is inherently multimodal, involving a combination of\\nverbal and non-verbal cues such as speech, facial expressions, and body\\ngestures. Modeling these behaviors is essential for understanding human\\ninteraction and for creating virtual characters that can communicate naturally\\nin applications like games, films, and virtual reality. However, existing\\nmotion generation models are typically limited to specific input modalities --\\neither speech, text, or motion data -- and cannot fully leverage the diversity\\nof available data. In this paper, we propose a novel framework that unifies\\nverbal and non-verbal language using multimodal language models for human\\nmotion understanding and generation. This model is flexible in taking text,\\nspeech, and motion or any combination of them as input. Coupled with our novel\\npre-training strategy, our model not only achieves state-of-the-art performance\\non co-speech gesture generation but also requires much less data for training.\\nOur model also unlocks an array of novel tasks such as editable gesture\\ngeneration and emotion prediction from motion. We believe unifying the verbal\\nand non-verbal language of human motion is essential for real-world\\napplications, and language models offer a powerful approach to achieving this\\ngoal. Project page: languageofmotion.github.io.","authors":["Changan Chen","Juze Zhang","Shrinidhi K. Lakshmikanth","Yusu Fang","Ruizhi Shao","Gordon Wetzstein","Li Fei-Fei","Ehsan Adeli"],"year":2024,"month":12,"url":"https://arxiv.org/abs/2412.10523","survey":false,"survey_abbr":"","model":true,"model_abbr":"LoM","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://languageofmotion.github.io/","repo":"https://github.com/Juzezhang/language_of_motion","backbone_tags":"VQ-VAE","approach_tags":"Multi-Task"},{"arxiv_id":"2412.07320","title":"CoMA: Compositional Human Motion Generation with Multi-modal Agents","abstract":"3D human motion generation has seen substantial advancement in recent years.\\nWhile state-of-the-art approaches have improved performance significantly, they\\nstill struggle with complex and detailed motions unseen in training data,\\nlargely due to the scarcity of motion datasets and the prohibitive cost of\\ngenerating new training examples. To address these challenges, we introduce\\nCoMA, an agent-based solution for complex human motion generation, editing, and\\ncomprehension. CoMA leverages multiple collaborative agents powered by large\\nlanguage and vision models, alongside a mask transformer-based motion generator\\nfeaturing body part-specific encoders and codebooks for fine-grained control.\\nOur framework enables generation of both short and long motion sequences with\\ndetailed instructions, text-guided motion editing, and self-correction for\\nimproved quality. Evaluations on the HumanML3D dataset demonstrate competitive\\nperformance against state-of-the-art methods. Additionally, we create a set of\\ncontext-rich, compositional, and long text prompts, where user studies show our\\nmethod significantly outperforms existing approaches.","authors":["Shanlin Sun","Gabriel De Araujo","Jiaqi Xu","Shenghan Zhou","Hanwen Zhang","Ziheng Huang","Chenyu You","Xiaohui Xie"],"year":2024,"month":12,"url":"https://arxiv.org/abs/2412.07320","survey":false,"survey_abbr":"","model":true,"model_abbr":"CoMA","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://gabrie-l.github.io/coma-page/","repo":"https://github.com/Siwensun/CoMA","backbone_tags":"LLM, VQ-VAE, CLIP, Transformer","approach_tags":"Editing"},{"arxiv_id":"2412.05277","title":"Text to Blind Motion","abstract":"People who are blind perceive the world differently than those who are\\nsighted, which can result in distinct motion characteristics. For instance,\\nwhen crossing at an intersection, blind individuals may have different patterns\\nof movement, such as veering more from a straight path or using touch-based\\nexploration around curbs and obstacles. These behaviors may appear less\\npredictable to motion models embedded in technologies such as autonomous\\nvehicles. Yet, the ability of 3D motion models to capture such behavior has not\\nbeen previously studied, as existing datasets for 3D human motion currently\\nlack diversity and are biased toward people who are sighted. In this work, we\\nintroduce BlindWays, the first multimodal motion benchmark for pedestrians who\\nare blind. We collect 3D motion data using wearable sensors with 11 blind\\nparticipants navigating eight different routes in a real-world urban setting.\\nAdditionally, we provide rich textual descriptions that capture the distinctive\\nmovement characteristics of blind pedestrians and their interactions with both\\nthe navigation aid (e.g., a white cane or a guide dog) and the environment. We\\nbenchmark state-of-the-art 3D human prediction models, finding poor performance\\nwith off-the-shelf and pre-training-based methods for our novel task. To\\ncontribute toward safer and more reliable systems that can seamlessly reason\\nover diverse human movements in their environments, our text-and-motion\\nbenchmark is available at https://blindways.github.io.","authors":["Hee Jae Kim","Kathakoli Sengupta","Masaki Kuribayashi","Hernisa Kacorri","Eshed Ohn-Bar"],"year":2024,"month":12,"url":"https://arxiv.org/abs/2412.05277","survey":false,"survey_abbr":"","model":false,"model_abbr":"","dataset":true,"dataset_abbr":"BlindWays","submission":"NeurIPS","submission_year":"2024","page":"https://blindways.github.io/","repo":"","backbone_tags":"","approach_tags":""},{"arxiv_id":"2412.05095","title":"SoPo: Text-to-Motion Generation Using Semi-Online Preference Optimization","abstract":"Text-to-motion generation is essential for advancing the creative industry\\nbut often presents challenges in producing consistent, realistic motions. To\\naddress this, we focus on fine-tuning text-to-motion models to consistently\\nfavor high-quality, human-preferred motions, a critical yet largely unexplored\\nproblem. In this work, we theoretically investigate the DPO under both online\\nand offline settings, and reveal their respective limitation: overfitting in\\noffline DPO, and biased sampling in online DPO. Building on our theoretical\\ninsights, we introduce Semi-online Preference Optimization (SoPo), a DPO-based\\nmethod for training text-to-motion models using \\"semi-online\\" data pair,\\nconsisting of unpreferred motion from online distribution and preferred motion\\nin offline datasets. This method leverages both online and offline DPO,\\nallowing each to compensate for the other\'s limitations. Extensive experiments\\ndemonstrate that SoPo outperforms other preference alignment methods, with an\\nMM-Dist of 3.25% (vs e.g. 0.76% of MoDiPO) on the MLD model, 2.91% (vs e.g.\\n0.66% of MoDiPO) on MDM model, respectively. Additionally, the MLD model\\nfine-tuned by our SoPo surpasses the SoTA model in terms of R-precision and MM\\nDist. Visualization results also show the efficacy of our SoPo in preference\\nalignment. Project page: https://xiaofeng-tan.github.io/projects/SoPo/ .","authors":["Xiaofeng Tan","Hongsong Wang","Xin Geng","Pan Zhou"],"year":2024,"month":12,"url":"https://arxiv.org/abs/2412.05095","survey":false,"survey_abbr":"","model":true,"model_abbr":"SoPo","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://xiaofeng-tan.github.io/projects/SoPo/","repo":"https://github.com/Xiaofeng-Tan/SoPO","backbone_tags":"","approach_tags":"RL, DPO"},{"arxiv_id":"2412.04343","title":"RMD: A Simple Baseline for More General Human Motion Generation via Training-free Retrieval-Augmented Motion Diffuse","abstract":"While motion generation has made substantial progress, its practical\\napplication remains constrained by dataset diversity and scale, limiting its\\nability to handle out-of-distribution scenarios. To address this, we propose a\\nsimple and effective baseline, RMD, which enhances the generalization of motion\\ngeneration through retrieval-augmented techniques. Unlike previous\\nretrieval-based methods, RMD requires no additional training and offers three\\nkey advantages: (1) the external retrieval database can be flexibly replaced;\\n(2) body parts from the motion database can be reused, with an LLM facilitating\\nsplitting and recombination; and (3) a pre-trained motion diffusion model\\nserves as a prior to improve the quality of motions obtained through retrieval\\nand direct combination. Without any training, RMD achieves state-of-the-art\\nperformance, with notable advantages on out-of-distribution data.","authors":["Zhouyingcheng Liao","Mingyuan Zhang","Wenjia Wang","Lei Yang","Taku Komura"],"year":2024,"month":12,"url":"https://arxiv.org/abs/2412.04343","survey":false,"survey_abbr":"","model":true,"model_abbr":"RMD","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Diffusion, LLM","approach_tags":"Retrieval"},{"arxiv_id":"2412.00112","title":"BiPO: Bidirectional Partial Occlusion Network for Text-to-Motion Synthesis","abstract":"Generating natural and expressive human motions from textual descriptions is\\nchallenging due to the complexity of coordinating full-body dynamics and\\ncapturing nuanced motion patterns over extended sequences that accurately\\nreflect the given text. To address this, we introduce BiPO, Bidirectional\\nPartial Occlusion Network for Text-to-Motion Synthesis, a novel model that\\nenhances text-to-motion synthesis by integrating part-based generation with a\\nbidirectional autoregressive architecture. This integration allows BiPO to\\nconsider both past and future contexts during generation while enhancing\\ndetailed control over individual body parts without requiring ground-truth\\nmotion length. To relax the interdependency among body parts caused by the\\nintegration, we devise the Partial Occlusion technique, which probabilistically\\noccludes the certain motion part information during training. In our\\ncomprehensive experiments, BiPO achieves state-of-the-art performance on the\\nHumanML3D dataset, outperforming recent methods such as ParCo, MoMask, and BAMM\\nin terms of FID scores and overall motion quality. Notably, BiPO excels not\\nonly in the text-to-motion generation task but also in motion editing tasks\\nthat synthesize motion based on partially generated motion sequences and\\ntextual descriptions. These results reveal the BiPO\'s effectiveness in\\nadvancing text-to-motion synthesis and its potential for practical\\napplications.","authors":["Seong-Eun Hong","Soobin Lim","Juyeong Hwang","Minwook Chang","Hyeongyeop Kang"],"year":2024,"month":11,"url":"https://arxiv.org/abs/2412.00112","survey":false,"survey_abbr":"","model":true,"model_abbr":"BiPO","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"CLIP, Transformer","approach_tags":"Finegrained, Editing, Multi-Task"},{"arxiv_id":"2411.19786","title":"MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks","abstract":"Recently, human motion analysis has experienced great improvement due to\\ninspiring generative models such as the denoising diffusion model and large\\nlanguage model. While the existing approaches mainly focus on generating\\nmotions with textual descriptions and overlook the reciprocal task. In this\\npaper, we present~\\\\textbf{MoTe}, a unified multi-modal model that could handle\\ndiverse tasks by learning the marginal, conditional, and joint distributions of\\nmotion and text simultaneously. MoTe enables us to handle the paired\\ntext-motion generation, motion captioning, and text-driven motion generation by\\nsimply modifying the input context. Specifically, MoTe is composed of three\\ncomponents: Motion Encoder-Decoder (MED), Text Encoder-Decoder (TED), and\\nMoti-on-Text Diffusion Model (MTDM). In particular, MED and TED are trained for\\nextracting latent embeddings, and subsequently reconstructing the motion\\nsequences and textual descriptions from the extracted embeddings, respectively.\\nMTDM, on the other hand, performs an iterative denoising process on the input\\ncontext to handle diverse tasks. Experimental results on the benchmark datasets\\ndemonstrate the superior performance of our proposed method on text-to-motion\\ngeneration and competitive performance on motion captioning.","authors":["Yiming Wu","Wei Ji","Kecheng Zheng","Zicheng Wang","Dong Xu"],"year":2024,"month":11,"url":"https://arxiv.org/abs/2411.19786","survey":false,"survey_abbr":"","model":true,"model_abbr":"MoTE","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Diffusion, CLIP, Transformer","approach_tags":""},{"arxiv_id":"2411.17532","title":"FTMoMamba: Motion Generation with Frequency and Text State Space Models","abstract":"Diffusion models achieve impressive performance in human motion generation.\\nHowever, current approaches typically ignore the significance of\\nfrequency-domain information in capturing fine-grained motions within the\\nlatent space (e.g., low frequencies correlate with static poses, and high\\nfrequencies align with fine-grained motions). Additionally, there is a semantic\\ndiscrepancy between text and motion, leading to inconsistency between the\\ngenerated motions and the text descriptions. In this work, we propose a novel\\ndiffusion-based FTMoMamba framework equipped with a Frequency State Space Model\\n(FreqSSM) and a Text State Space Model (TextSSM). Specifically, to learn\\nfine-grained representation, FreqSSM decomposes sequences into low-frequency\\nand high-frequency components, guiding the generation of static pose (e.g.,\\nsits, lay) and fine-grained motions (e.g., transition, stumble), respectively.\\nTo ensure the consistency between text and motion, TextSSM encodes text\\nfeatures at the sentence level, aligning textual semantics with sequential\\nfeatures. Extensive experiments show that FTMoMamba achieves superior\\nperformance on the text-to-motion generation task, especially gaining the\\nlowest FID of 0.181 (rather lower than 0.421 of MLD) on the HumanML3D dataset.","authors":["Chengjian Li","Xiangbo Shu","Qiongjie Cui","Yazhou Yao","Jinhui Tang"],"year":2024,"month":11,"url":"https://arxiv.org/abs/2411.17532","survey":false,"survey_abbr":"","model":true,"model_abbr":"FTMoMamba","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Mamba, Diffusion","approach_tags":"Frequency, Finegrained"},{"arxiv_id":"2411.17335","title":"VersatileMotion: A Unified Framework for Motion Synthesis and Comprehension","abstract":"Large language models (LLMs) are, by design, inherently capable of multi-task\\nlearning: through a unified next-token prediction paradigm, they can naturally\\naddress a wide variety of downstream tasks. Prior work in the motion domain has\\ndemonstrated some generality by adapting LLMs via a Motion Tokenizer coupled\\nwith an autoregressive Transformer to generate and understand human motion.\\nHowever, this generality remains limited in scope and yields only modest\\nperformance gains. We introduce VersatileMotion, a unified multimodal motion\\nLLM that combines a novel motion tokenizer, integrating VQ-VAE with flow\\nmatching, and an autoregressive transformer backbone to seamlessly support at\\nleast nine distinct motion-related tasks. VersatileMotion is the first method\\nto handle single-agent and multi-agent motions in a single framework and enable\\ncross-modal conversion between motion, text, music, and speech, achieving\\nstate-of-the-art performance on seven of these tasks. Each sequence in\\nMotionHub may include one or more of the following annotations:\\nnatural-language captions, music or audio clips, speech transcripts, and\\nmulti-agent interaction data. To facilitate evaluation, we define and release\\nbenchmark splits covering nine core tasks. Extensive experiments demonstrate\\nthe superior performance, versatility, and potential of VersatileMotion as a\\nfoundational model for future understanding and generation of motion.","authors":["Zeyu Ling","Bo Han","Shiyang Li","Jikang Cheng","Hongdeng Shen","Changqing Zou"],"year":2024,"month":11,"url":"https://arxiv.org/abs/2411.17335","survey":false,"survey_abbr":"","model":true,"model_abbr":"VersatileMotion","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"LLM, VQ-VAE, Flow Matching, Transformer","approach_tags":""},{"arxiv_id":"2411.16575","title":"Rethinking Diffusion for Text-Driven Human Motion Generation: Redundant Representations, Evaluation, and Masked Autoregression","abstract":"Since 2023, Vector Quantization (VQ)-based discrete generation methods have\\nrapidly dominated human motion generation, primarily surpassing diffusion-based\\ncontinuous generation methods in standard performance metrics. However,\\nVQ-based methods have inherent limitations. Representing continuous motion data\\nas limited discrete tokens leads to inevitable information loss, reduces the\\ndiversity of generated motions, and restricts their ability to function\\neffectively as motion priors or generation guidance. In contrast, the\\ncontinuous space generation nature of diffusion-based methods makes them\\nwell-suited to address these limitations and with even potential for model\\nscalability. In this work, we systematically investigate why current VQ-based\\nmethods perform well and explore the limitations of existing diffusion-based\\nmethods from the perspective of motion data representation and distribution.\\nDrawing on these insights, we preserve the inherent strengths of a\\ndiffusion-based human motion generation model and gradually optimize it with\\ninspiration from VQ-based approaches. Our approach introduces a human motion\\ndiffusion model enabled to perform masked autoregression, optimized with a\\nreformed data representation and distribution. Additionally, we propose a more\\nrobust evaluation method to assess different approaches. Extensive experiments\\non various datasets demonstrate our method outperforms previous methods and\\nachieves state-of-the-art performances.","authors":["Zichong Meng","Yiming Xie","Xiaogang Peng","Zeyu Han","Huaizu Jiang"],"year":2024,"month":11,"url":"https://arxiv.org/abs/2411.16575","survey":false,"survey_abbr":"","model":true,"model_abbr":"MARDM","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2025","page":"https://neu-vi.github.io/MARDM/","repo":"https://github.com/neu-vi/MARDM","backbone_tags":"CLIP, Diffusion, Transformer","approach_tags":""},{"arxiv_id":"2411.14951","title":"Morph: A Motion-free Physics Optimization Framework for Human Motion Generation","abstract":"Human motion generation plays a vital role in applications such as digital\\nhumans and humanoid robot control. However, most existing approaches disregard\\nphysics constraints, leading to the frequent production of physically\\nimplausible motions with pronounced artifacts such as floating and foot\\nsliding. In this paper, we propose \\\\textbf{Morph}, a\\n\\\\textbf{Mo}tion-f\\\\textbf{r}ee \\\\textbf{ph}ysics optimization framework,\\ncomprising a Motion Generator and a Motion Physics Refinement module, for\\nenhancing physical plausibility without relying on costly real-world motion\\ndata. Specifically, the Motion Generator is responsible for providing\\nlarge-scale synthetic motion data, while the Motion Physics Refinement Module\\nutilizes these synthetic data to train a motion imitator within a physics\\nsimulator, enforcing physical constraints to project the noisy motions into a\\nphysically-plausible space. These physically refined motions, in turn, are used\\nto fine-tune the Motion Generator, further enhancing its capability.\\nExperiments on both text-to-motion and music-to-dance generation tasks\\ndemonstrate that our framework achieves state-of-the-art motion generation\\nquality while improving physical plausibility drastically.","authors":["Zhuo Li","Mingshuang Luo","Ruibing Hou","Xin Zhao","Hao Liu","Hong Chang","Zimo Liu","Chen Li"],"year":2024,"month":11,"url":"https://arxiv.org/abs/2411.14951","survey":false,"survey_abbr":"","model":true,"model_abbr":"Morph","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2025","page":"","repo":"","backbone_tags":"","approach_tags":"Physical, RL, Component"},{"arxiv_id":"2411.06481","title":"KMM: Key Frame Mask Mamba for Extended Motion Generation","abstract":"Human motion generation is a cut-edge area of research in generative computer\\nvision, with promising applications in video creation, game development, and\\nrobotic manipulation. The recent Mamba architecture shows promising results in\\nefficiently modeling long and complex sequences, yet two significant challenges\\nremain: Firstly, directly applying Mamba to extended motion generation is\\nineffective, as the limited capacity of the implicit memory leads to memory\\ndecay. Secondly, Mamba struggles with multimodal fusion compared to\\nTransformers, and lack alignment with textual queries, often confusing\\ndirections (left or right) or omitting parts of longer text queries. To address\\nthese challenges, our paper presents three key contributions: Firstly, we\\nintroduce KMM, a novel architecture featuring Key frame Masking Modeling,\\ndesigned to enhance Mamba\'s focus on key actions in motion segments. This\\napproach addresses the memory decay problem and represents a pioneering method\\nin customizing strategic frame-level masking in SSMs. Additionally, we designed\\na contrastive learning paradigm for addressing the multimodal fusion problem in\\nMamba and improving the motion-text alignment. Finally, we conducted extensive\\nexperiments on the go-to dataset, BABEL, achieving state-of-the-art performance\\nwith a reduction of more than 57% in FID and 70% parameters compared to\\nprevious state-of-the-art methods. See project website:\\nhttps://steve-zeyu-zhang.github.io/KMM","authors":["Zeyu Zhang","Hang Gao","Akide Liu","Qi Chen","Feng Chen","Yiran Wang","Danning Li","Rui Zhao","Zhenming Li","Zhongwen Zhou","Hao Tang","Bohan Zhuang"],"year":2024,"month":11,"url":"https://arxiv.org/abs/2411.06481","survey":false,"survey_abbr":"","model":true,"model_abbr":"KMM","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://steve-zeyu-zhang.github.io/KMM/","repo":"","backbone_tags":"VQ-VAE, Mamba, CLIP","approach_tags":""},{"arxiv_id":"2410.21747","title":"MotionGPT-2: A General-Purpose Motion-Language Model for Motion Generation and Understanding","abstract":"Generating lifelike human motions from descriptive texts has experienced\\nremarkable research focus in the recent years, propelled by the emerging\\nrequirements of digital humans.Despite impressive advances, existing approaches\\nare often constrained by limited control modalities, task specificity, and\\nfocus solely on body motion representations.In this paper, we present\\nMotionGPT-2, a unified Large Motion-Language Model (LMLM) that addresses these\\nlimitations. MotionGPT-2 accommodates multiple motion-relevant tasks and\\nsupporting multimodal control conditions through pre-trained Large Language\\nModels (LLMs). It quantizes multimodal inputs-such as text and single-frame\\nposes-into discrete, LLM-interpretable tokens, seamlessly integrating them into\\nthe LLM\'s vocabulary. These tokens are then organized into unified prompts,\\nguiding the LLM to generate motion outputs through a\\npretraining-then-finetuning paradigm. We also show that the proposed\\nMotionGPT-2 is highly adaptable to the challenging 3D holistic motion\\ngeneration task, enabled by the innovative motion discretization framework,\\nPart-Aware VQVAE, which ensures fine-grained representations of body and hand\\nmovements. Extensive experiments and visualizations validate the effectiveness\\nof our method, demonstrating the adaptability of MotionGPT-2 across motion\\ngeneration, motion captioning, and generalized motion completion tasks.","authors":["Yuan Wang","Di Huang","Yaqi Zhang","Wanli Ouyang","Jile Jiao","Xuetao Feng","Yan Zhou","Pengfei Wan","Shixiang Tang","Dan Xu"],"year":2024,"month":10,"url":"https://arxiv.org/abs/2410.21747","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionGPT-2","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"LLM, VQ-VAE","approach_tags":"Multi-Task, LoRA"},{"arxiv_id":"2410.18977","title":"Pay Attention and Move Better: Harnessing Attention for Interactive Motion Generation and Training-free Editing","abstract":"This research delves into the problem of interactive editing of human motion\\ngeneration. Previous motion diffusion models lack explicit modeling of the\\nword-level text-motion correspondence and good explainability, hence\\nrestricting their fine-grained editing ability. To address this issue, we\\npropose an attention-based motion diffusion model, namely MotionCLR, with CLeaR\\nmodeling of attention mechanisms. Technically, MotionCLR models the in-modality\\nand cross-modality interactions with self-attention and cross-attention,\\nrespectively. More specifically, the self-attention mechanism aims to measure\\nthe sequential similarity between frames and impacts the order of motion\\nfeatures. By contrast, the cross-attention mechanism works to find the\\nfine-grained word-sequence correspondence and activate the corresponding\\ntimesteps in the motion sequence. Based on these key properties, we develop a\\nversatile set of simple yet effective motion editing methods via manipulating\\nattention maps, such as motion (de-)emphasizing, in-place motion replacement,\\nand example-based motion generation, etc. For further verification of the\\nexplainability of the attention mechanism, we additionally explore the\\npotential of action-counting and grounded motion generation ability via\\nattention maps. Our experimental results show that our method enjoys good\\ngeneration and editing ability with good explainability.","authors":["Ling-Hao Chen","Shunlin Lu","Wenxun Dai","Zhiyang Dou","Xuan Ju","Jingbo Wang","Taku Komura","Lei Zhang"],"year":2024,"month":10,"url":"https://arxiv.org/abs/2410.18977","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionCLR","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://lhchen.top/MotionCLR/","repo":"","backbone_tags":"Transformer","approach_tags":"Finegrained, Editing"},{"arxiv_id":"2410.14508","title":"LEAD: Latent Realignment for Human Motion Diffusion","abstract":"Our goal is to generate realistic human motion from natural language. Modern\\nmethods often face a trade-off between model expressiveness and text-to-motion\\nalignment. Some align text and motion latent spaces but sacrifice\\nexpressiveness; others rely on diffusion models producing impressive motions,\\nbut lacking semantic meaning in their latent space. This may compromise\\nrealism, diversity, and applicability. Here, we address this by combining\\nlatent diffusion with a realignment mechanism, producing a novel, semantically\\nstructured space that encodes the semantics of language. Leveraging this\\ncapability, we introduce the task of textual motion inversion to capture novel\\nmotion concepts from a few examples. For motion synthesis, we evaluate LEAD on\\nHumanML3D and KIT-ML and show comparable performance to the state-of-the-art in\\nterms of realism, diversity, and text-motion consistency. Our qualitative\\nanalysis and user study reveal that our synthesized motions are sharper, more\\nhuman-like and comply better with the text compared to modern methods. For\\nmotion textual inversion, our method demonstrates improved capacity in\\ncapturing out-of-distribution characteristics in comparison to traditional\\nVAEs.","authors":["Nefeli Andreou","Xi Wang","Victoria Fern\xe1ndez Abrevaya","Marie-Paule Cani","Yiorgos Chrysanthou","Vicky Kalogeiton"],"year":2024,"month":10,"url":"https://arxiv.org/abs/2410.14508","survey":false,"survey_abbr":"","model":true,"model_abbr":"LEAD","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"VQ-VAE","approach_tags":"Component"},{"arxiv_id":"2410.10780","title":"MaskControl: Spatio-Temporal Control for Masked Motion Synthesis","abstract":"Recent advances in motion diffusion models have enabled spatially\\ncontrollable text-to-motion generation. However, these models struggle to\\nachieve high-precision control while maintaining high-quality motion\\ngeneration. To address these challenges, we propose MaskControl, the first\\napproach to introduce controllability to the generative masked motion model.\\nOur approach introduces two key innovations. First, \\\\textit{Logits Regularizer}\\nimplicitly perturbs logits at training time to align the distribution of motion\\ntokens with the controlled joint positions, while regularizing the categorical\\ntoken prediction to ensure high-fidelity generation. Second, \\\\textit{Logit\\nOptimization} explicitly optimizes the predicted logits during inference time,\\ndirectly reshaping the token distribution that forces the generated motion to\\naccurately align with the controlled joint positions. Moreover, we introduce\\n\\\\textit{Differentiable Expectation Sampling (DES)} to combat the\\nnon-differential distribution sampling process encountered by logits\\nregularizer and optimization. Extensive experiments demonstrate that\\nMaskControl outperforms state-of-the-art methods, achieving superior motion\\nquality (FID decreases by ~77\\\\%) and higher control precision (average error\\n0.91 vs. 1.08). Additionally, MaskControl enables diverse applications,\\nincluding any-joint-any-frame control, body-part timeline control, and\\nzero-shot objective control. Video visualization can be found at\\nhttps://www.ekkasit.com/ControlMM-page/","authors":["Ekkasit Pinyoanuntapong","Muhammad Usama Saleem","Korrawe Karunratanakul","Pu Wang","Hongfei Xue","Chen Chen","Chuan Guo","Junli Cao","Jian Ren","Sergey Tulyakov"],"year":2024,"month":10,"url":"https://arxiv.org/abs/2410.10780","survey":false,"survey_abbr":"","model":true,"model_abbr":"MaskControl","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2025","page":"https://www.ekkasit.com/ControlMM-page/","repo":"https://github.com/exitudio/MaskControl/","backbone_tags":"Transformer","approach_tags":"Trajectory"},{"arxiv_id":"2410.07296","title":"ReinDiffuse: Crafting Physically Plausible Motions with Reinforced Diffusion Model","abstract":"Generating human motion from textual descriptions is a challenging task.\\nExisting methods either struggle with physical credibility or are limited by\\nthe complexities of physics simulations. In this paper, we present\\n\\\\emph{ReinDiffuse} that combines reinforcement learning with motion diffusion\\nmodel to generate physically credible human motions that align with textual\\ndescriptions. Our method adapts Motion Diffusion Model to output a\\nparameterized distribution of actions, making them compatible with\\nreinforcement learning paradigms. We employ reinforcement learning with the\\nobjective of maximizing physically plausible rewards to optimize motion\\ngeneration for physical fidelity. Our approach outperforms existing\\nstate-of-the-art models on two major datasets, HumanML3D and KIT-ML, achieving\\nsignificant improvements in physical plausibility and motion quality. Project:\\nhttps://reindiffuse.github.io/","authors":["Gaoge Han","Mingjiang Liang","Jinglei Tang","Yongkang Cheng","Wei Liu","Shaoli Huang"],"year":2024,"month":10,"url":"https://arxiv.org/abs/2410.07296","survey":false,"survey_abbr":"","model":true,"model_abbr":"ReinDiffuse","dataset":false,"dataset_abbr":"","submission":"WACV","submission_year":"2025","page":"https://reindiffuse.github.io/","repo":"","backbone_tags":"Diffusion","approach_tags":"RL, Physical"},{"arxiv_id":"2410.07093","title":"LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning","abstract":"Language plays a vital role in the realm of human motion. Existing methods\\nhave largely depended on CLIP text embeddings for motion generation, yet they\\nfall short in effectively aligning language and motion due to CLIP\'s\\npretraining on static image-text pairs. This work introduces LaMP, a novel\\nLanguage-Motion Pretraining model, which transitions from a language-vision to\\na more suitable language-motion latent space. It addresses key limitations by\\ngenerating motion-informative text embeddings, significantly enhancing the\\nrelevance and semantics of generated motion sequences. With LaMP, we advance\\nthree key tasks: text-to-motion generation, motion-text retrieval, and motion\\ncaptioning through aligned language-motion representation learning. For\\ngeneration, we utilize LaMP to provide the text condition instead of CLIP, and\\nan autoregressive masked prediction is designed to achieve mask modeling\\nwithout rank collapse in transformers. For retrieval, motion features from\\nLaMP\'s motion transformer interact with query tokens to retrieve text features\\nfrom the text transformer, and vice versa. For captioning, we finetune a large\\nlanguage model with the language-informative motion features to develop a\\nstrong motion captioning model. In addition, we introduce the LaMP-BertScore\\nmetric to assess the alignment of generated motions with textual descriptions.\\nExtensive experimental results on multiple datasets demonstrate substantial\\nimprovements over previous methods across all three tasks. The code of our\\nmethod will be made public.","authors":["Zhe Li","Weihao Yuan","Yisheng He","Lingteng Qiu","Shenhao Zhu","Xiaodong Gu","Weichao Shen","Yuan Dong","Zilong Dong","Laurence T. Yang"],"year":2024,"month":10,"url":"https://arxiv.org/abs/2410.07093","survey":false,"survey_abbr":"","model":true,"model_abbr":"LaMP","dataset":false,"dataset_abbr":"","submission":"ICLR","submission_year":"2025","page":"https://aigc3d.github.io/LaMP/","repo":"https://github.com/gentlefress/LaMP","backbone_tags":"Transformer","approach_tags":"Pre-Train"},{"arxiv_id":"2410.06513","title":"MotionRL: Align Text-to-Motion Generation to Human Preferences with Multi-Reward Reinforcement Learning","abstract":"We introduce MotionRL, the first approach to utilize Multi-Reward\\nReinforcement Learning (RL) for optimizing text-to-motion generation tasks and\\naligning them with human preferences. Previous works focused on improving\\nnumerical performance metrics on the given datasets, often neglecting the\\nvariability and subjectivity of human feedback. In contrast, our novel approach\\nuses reinforcement learning to fine-tune the motion generator based on human\\npreferences prior knowledge of the human perception model, allowing it to\\ngenerate motions that better align human preferences. In addition, MotionRL\\nintroduces a novel multi-objective optimization strategy to approximate Pareto\\noptimality between text adherence, motion quality, and human preferences.\\nExtensive experiments and user studies demonstrate that MotionRL not only\\nallows control over the generated results across different objectives but also\\nsignificantly enhances performance across these metrics compared to other\\nalgorithms.","authors":["Xiaoyang Liu","Yunyao Mao","Wengang Zhou","Houqiang Li"],"year":2024,"month":10,"url":"https://arxiv.org/abs/2410.06513","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionRL","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Transformer","approach_tags":"PPO, RL"},{"arxiv_id":"2410.05628","title":"A Unified Framework for Motion Reasoning and Generation in Human Interaction","abstract":"Recent advancements in large language models (LLMs) have significantly\\nimproved their ability to generate natural and contextually relevant text,\\nenabling more human-like AI interactions. However, generating and understanding\\ninteractive human-like motion, where multiple individuals engage in coordinated\\nmovements, remains challenging due to the complexity of modeling these\\ninteractions. Additionally, a unified and versatile model is needed to handle\\ndiverse interactive scenarios, such as chat systems that dynamically adapt to\\nuser instructions and assigned roles. To address these challenges, we introduce\\nVIM, the Versatile Interactive Motion-language model, which integrates both\\nlanguage and motion modalities to effectively understand, generate, and control\\ninteractive motions in multi-turn conversational contexts. Unlike previous\\nstudies that primarily focus on uni-directional tasks such as text-to-motion or\\nmotion-to-text, VIM employs a unified architecture capable of simultaneously\\nunderstanding and generating both motion and text modalities. Given the absence\\nof an appropriate dataset to support this task, we introduce Inter-MT2, a\\nlarge-scale instruction-tuning dataset containing 82.7K multi-turn interactive\\nmotion instructions, covering 153K interactive motion samples. Inter-MT2 spans\\ndiverse instructional scenarios, including motion editing, question answering,\\nand story generation, leveraging off-the-shelf large language models and motion\\ndiffusion models to construct a broad set of interactive motion instructions.\\nWe extensively evaluate the versatility of VIM across multiple interactive\\nmotion-related tasks, including motion-to-text, text-to-motion, reaction\\ngeneration, motion editing, and reasoning about motion sequences.","authors":["Jeongeun Park","Sungjoon Choi","Sangdoo Yun"],"year":2024,"month":10,"url":"https://arxiv.org/abs/2410.05628","survey":false,"survey_abbr":"","model":true,"model_abbr":"VIM","dataset":true,"dataset_abbr":"Inter-MT2","submission":"ICCV","submission_year":"2025","page":"https://vim-motion-language.github.io/","repo":"","backbone_tags":"LLM","approach_tags":"Multi-Task"},{"arxiv_id":"2410.05260","title":"DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control","abstract":"Text-conditioned human motion generation, which allows for user interaction\\nthrough natural language, has become increasingly popular. Existing methods\\ntypically generate short, isolated motions based on a single input sentence.\\nHowever, human motions are continuous and can extend over long periods,\\ncarrying rich semantics. Creating long, complex motions that precisely respond\\nto streams of text descriptions, particularly in an online and real-time\\nsetting, remains a significant challenge. Furthermore, incorporating spatial\\nconstraints into text-conditioned motion generation presents additional\\nchallenges, as it requires aligning the motion semantics specified by text\\ndescriptions with geometric information, such as goal locations and 3D scene\\ngeometry. To address these limitations, we propose DartControl, in short DART,\\na Diffusion-based Autoregressive motion primitive model for Real-time\\nText-driven motion control. Our model effectively learns a compact motion\\nprimitive space jointly conditioned on motion history and text inputs using\\nlatent diffusion models. By autoregressively generating motion primitives based\\non the preceding history and current text input, DART enables real-time,\\nsequential motion generation driven by natural language descriptions.\\nAdditionally, the learned motion primitive space allows for precise spatial\\nmotion control, which we formulate either as a latent noise optimization\\nproblem or as a Markov decision process addressed through reinforcement\\nlearning. We present effective algorithms for both approaches, demonstrating\\nour model\'s versatility and superior performance in various motion synthesis\\ntasks. Experiments show our method outperforms existing baselines in motion\\nrealism, efficiency, and controllability. Video results are available on the\\nproject page: https://zkf1997.github.io/DART/.","authors":["Kaifeng Zhao","Gen Li","Siyu Tang"],"year":2024,"month":10,"url":"https://arxiv.org/abs/2410.05260","survey":false,"survey_abbr":"","model":true,"model_abbr":"DART","dataset":false,"dataset_abbr":"","submission":"ICLR","submission_year":"2025","page":"https://zkf1997.github.io/DART/","repo":"https://github.com/zkf1997/DART","backbone_tags":"CLIP, Transformer, VAE, Diffusion","approach_tags":"Multi-Task, Efficient"},{"arxiv_id":"2410.04534","title":"UniMuMo: Unified Text, Music and Motion Generation","abstract":"We introduce UniMuMo, a unified multimodal model capable of taking arbitrary\\ntext, music, and motion data as input conditions to generate outputs across all\\nthree modalities. To address the lack of time-synchronized data, we align\\nunpaired music and motion data based on rhythmic patterns to leverage existing\\nlarge-scale music-only and motion-only datasets. By converting music, motion,\\nand text into token-based representation, our model bridges these modalities\\nthrough a unified encoder-decoder transformer architecture. To support multiple\\ngeneration tasks within a single framework, we introduce several architectural\\nimprovements. We propose encoding motion with a music codebook, mapping motion\\ninto the same feature space as music. We introduce a music-motion parallel\\ngeneration scheme that unifies all music and motion generation tasks into a\\nsingle transformer decoder architecture with a single training task of\\nmusic-motion joint generation. Moreover, the model is designed by fine-tuning\\nexisting pre-trained single-modality models, significantly reducing\\ncomputational demands. Extensive experiments demonstrate that UniMuMo achieves\\ncompetitive results on all unidirectional generation benchmarks across music,\\nmotion, and text modalities. Quantitative results are available in the\\n\\\\href{https://hanyangclarence.github.io/unimumo_demo/}{project page}.","authors":["Han Yang","Kun Su","Yutong Zhang","Jiaben Chen","Kaizhi Qian","Gaowen Liu","Chuang Gan"],"year":2024,"month":10,"url":"https://arxiv.org/abs/2410.04534","survey":false,"survey_abbr":"","model":true,"model_abbr":"UniMuMo","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://hanyangclarence.github.io/unimumo_demo/","repo":"https://github.com/hanyangclarence/UniMuMo","backbone_tags":"Transformer","approach_tags":"Multi-Task"},{"arxiv_id":"2410.03311","title":"Scaling Large Motion Models with Million-Level Human Motions","abstract":"Inspired by the recent success of LLMs, the field of human motion\\nunderstanding has increasingly shifted toward developing large motion models.\\nDespite some progress, current efforts remain far from achieving truly\\ngeneralist models, primarily due to the lack of massive high-quality data. To\\naddress this gap, we present MotionLib, the first million-level dataset for\\nmotion generation, which is at least 15$\\\\times$ larger than existing\\ncounterparts and enriched with hierarchical text descriptions. Using MotionLib,\\nwe train a large motion model named \\\\projname, demonstrating robust performance\\nacross a wide range of human activities, including unseen ones. Through\\nsystematic investigation, for the first time, we highlight the importance of\\nscaling both data and model size for advancing motion generation, along with\\nkey insights to achieve this goal. To better integrate the motion modality, we\\npropose Motionbook, an innovative motion encoding approach including (1) a\\ncompact yet lossless feature to represent motions; (2) a novel 2D lookup-free\\nmotion tokenizer that preserves fine-grained motion details while expanding\\ncodebook capacity, significantly enhancing the representational power of motion\\ntokens. We believe this work lays the groundwork for developing more versatile\\nand powerful motion generation models in the future. For further details, visit\\nhttps://beingbeyond.github.io/Being-M0/.","authors":["Ye Wang","Sipeng Zheng","Bin Cao","Qianshan Wei","Weishuai Zeng","Qin Jin","Zongqing Lu"],"year":2024,"month":10,"url":"https://arxiv.org/abs/2410.03311","survey":false,"survey_abbr":"","model":true,"model_abbr":"Being-M0","dataset":true,"dataset_abbr":"MotionLib","submission":"ICML","submission_year":"2025","page":"https://beingbeyond.github.io/Being-M0/","repo":"https://github.com/BeingBeyond/Being-M0","backbone_tags":"VQ-VAE, LLM","approach_tags":""},{"arxiv_id":"2409.19686","title":"Text-driven Human Motion Generation with Motion Masked Diffusion Model","abstract":"Text-driven human motion generation is a multimodal task that synthesizes\\nhuman motion sequences conditioned on natural language. It requires the model\\nto satisfy textual descriptions under varying conditional inputs, while\\ngenerating plausible and realistic human actions with high diversity. Existing\\ndiffusion model-based approaches have outstanding performance in the diversity\\nand multimodality of generation. However, compared to autoregressive methods\\nthat train motion encoders before inference, diffusion methods lack in fitting\\nthe distribution of human motion features which leads to an unsatisfactory FID\\nscore. One insight is that the diffusion model lack the ability to learn the\\nmotion relations among spatio-temporal semantics through contextual reasoning.\\nTo solve this issue, in this paper, we proposed Motion Masked Diffusion Model\\n\\\\textbf{(MMDM)}, a novel human motion masked mechanism for diffusion model to\\nexplicitly enhance its ability to learn the spatio-temporal relationships from\\ncontextual joints among motion sequences. Besides, considering the complexity\\nof human motion data with dynamic temporal characteristics and spatial\\nstructure, we designed two mask modeling strategies: \\\\textbf{time frames mask}\\nand \\\\textbf{body parts mask}. During training, MMDM masks certain tokens in the\\nmotion embedding space. Then, the diffusion decoder is designed to learn the\\nwhole motion sequence from masked embedding in each sampling step, this allows\\nthe model to recover a complete sequence from incomplete representations.\\nExperiments on HumanML3D and KIT-ML dataset demonstrate that our mask strategy\\nis effective by balancing motion quality and text-motion consistency.","authors":["Xingyu Chen"],"year":2024,"month":9,"url":"https://arxiv.org/abs/2409.19686","survey":false,"survey_abbr":"","model":true,"model_abbr":"MMDM","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Diffusion","approach_tags":""},{"arxiv_id":"2409.18127","title":"EgoLM: Multi-Modal Language Model of Egocentric Motions","abstract":"As the prevalence of wearable devices, learning egocentric motions becomes\\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\\nframework that tracks and understands egocentric motions from multi-modal\\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\\ncontexts for the disambiguation of egomotion tracking and understanding, which\\nare ill-posed under single modality conditions. To facilitate the versatile and\\nmulti-modal framework, our key insight is to model the joint distribution of\\negocentric motions and natural languages using large language models (LLM).\\nMulti-modal sensor inputs are encoded and projected to the joint latent space\\nof language models, and used to prompt motion generation or text generation for\\negomotion tracking or understanding, respectively. Extensive experiments on\\nlarge-scale multi-modal human motion dataset validate the effectiveness of\\nEgoLM as a generalist model for universal egocentric learning.","authors":["Fangzhou Hong","Vladimir Guzov","Hyo Jin Kim","Yuting Ye","Richard Newcombe","Ziwei Liu","Lingni Ma"],"year":2024,"month":9,"url":"https://arxiv.org/abs/2409.18127","survey":false,"survey_abbr":"","model":true,"model_abbr":"EgoLM","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2025","page":"https://hongfz16.github.io/projects/EgoLM","repo":"","backbone_tags":"LLM, VAE","approach_tags":"Multi-Task"},{"arxiv_id":"2409.17686","title":"MoGenTS: Motion Generation based on Spatial-Temporal Joint Modeling","abstract":"Motion generation from discrete quantization offers many advantages over\\ncontinuous regression, but at the cost of inevitable approximation errors.\\nPrevious methods usually quantize the entire body pose into one code, which not\\nonly faces the difficulty in encoding all joints within one vector but also\\nloses the spatial relationship between different joints. Differently, in this\\nwork we quantize each individual joint into one vector, which i) simplifies the\\nquantization process as the complexity associated with a single joint is\\nmarkedly lower than that of the entire pose; ii) maintains a spatial-temporal\\nstructure that preserves both the spatial relationships among joints and the\\ntemporal movement patterns; iii) yields a 2D token map, which enables the\\napplication of various 2D operations widely used in 2D images. Grounded in the\\n2D motion quantization, we build a spatial-temporal modeling framework, where\\n2D joint VQVAE, temporal-spatial 2D masking technique, and spatial-temporal 2D\\nattention are proposed to take advantage of spatial-temporal signals among the\\n2D tokens. Extensive experiments demonstrate that our method significantly\\noutperforms previous methods across different datasets, with a 26.6% decrease\\nof FID on HumanML3D and a 29.9% decrease on KIT-ML. Project page:\\nhttps://aigc3d.github.io/mogents.","authors":["Weihao Yuan","Weichao Shen","Yisheng He","Yuan Dong","Xiaodong Gu","Zilong Dong","Liefeng Bo","Qixing Huang"],"year":2024,"month":9,"url":"https://arxiv.org/abs/2409.17686","survey":false,"survey_abbr":"","model":true,"model_abbr":"MoGenTS","dataset":false,"dataset_abbr":"","submission":"NeurIPS","submission_year":"2024","page":"https://aigc3d.github.io/mogents/","repo":"https://github.com/weihaosky/mogents","backbone_tags":"VQ-VAE","approach_tags":"Editing"},{"arxiv_id":"2409.15904","title":"Unimotion: Unifying 3D Human Motion Synthesis and Understanding","abstract":"We introduce Unimotion, the first unified multi-task human motion model\\ncapable of both flexible motion control and frame-level motion understanding.\\nWhile existing works control avatar motion with global text conditioning, or\\nwith fine-grained per frame scripts, none can do both at once. In addition,\\nnone of the existing works can output frame-level text paired with the\\ngenerated poses. In contrast, Unimotion allows to control motion with global\\ntext, or local frame-level text, or both at once, providing more flexible\\ncontrol for users. Importantly, Unimotion is the first model which by design\\noutputs local text paired with the generated poses, allowing users to know what\\nmotion happens and when, which is necessary for a wide range of applications.\\nWe show Unimotion opens up new applications: 1.) Hierarchical control, allowing\\nusers to specify motion at different levels of detail, 2.) Obtaining motion\\ntext descriptions for existing MoCap data or YouTube videos 3.) Allowing for\\neditability, generating motion from text, and editing the motion via text\\nedits. Moreover, Unimotion attains state-of-the-art results for the frame-level\\ntext-to-motion task on the established HumanML3D dataset. The pre-trained model\\nand code are available available on our project page at\\nhttps://coral79.github.io/uni-motion/.","authors":["Chuqiao Li","Julian Chibane","Yannan He","Naama Pearl","Andreas Geiger","Gerard Pons-moll"],"year":2024,"month":9,"url":"https://arxiv.org/abs/2409.15904","survey":false,"survey_abbr":"","model":true,"model_abbr":"UniMotion","dataset":false,"dataset_abbr":"","submission":"3DV","submission_year":"2025","page":"https://coral79.github.io/uni-motion/","repo":"https://github.com/Coral79/Unimotion","backbone_tags":"CLIP, Transformer","approach_tags":"Editing, Finegrained, Multi-Task"},{"arxiv_id":"2409.14393","title":"MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting","abstract":"Crafting a single, versatile physics-based controller that can breathe life\\ninto interactive characters across a wide spectrum of scenarios represents an\\nexciting frontier in character animation. An ideal controller should support\\ndiverse control modalities, such as sparse target keyframes, text instructions,\\nand scene information. While previous works have proposed physically simulated,\\nscene-aware control models, these systems have predominantly focused on\\ndeveloping controllers that each specializes in a narrow set of tasks and\\ncontrol modalities. This work presents MaskedMimic, a novel approach that\\nformulates physics-based character control as a general motion inpainting\\nproblem. Our key insight is to train a single unified model to synthesize\\nmotions from partial (masked) motion descriptions, such as masked keyframes,\\nobjects, text descriptions, or any combination thereof. This is achieved by\\nleveraging motion tracking data and designing a scalable training method that\\ncan effectively utilize diverse motion descriptions to produce coherent\\nanimations. Through this process, our approach learns a physics-based\\ncontroller that provides an intuitive control interface without requiring\\ntedious reward engineering for all behaviors of interest. The resulting\\ncontroller supports a wide range of control modalities and enables seamless\\ntransitions between disparate tasks. By unifying character control through\\nmotion inpainting, MaskedMimic creates versatile virtual characters. These\\ncharacters can dynamically adapt to complex scenes and compose diverse motions\\non demand, enabling more interactive and immersive experiences.","authors":["Chen Tessler","Yunrong Guo","Ofir Nabati","Gal Chechik","Xue Bin Peng"],"year":2024,"month":9,"url":"https://arxiv.org/abs/2409.14393","survey":false,"survey_abbr":"","model":true,"model_abbr":"MaskedMimic","dataset":false,"dataset_abbr":"","submission":"SIGGRAPH Asia","submission_year":"2024","page":"https://xbpeng.github.io/projects/MaskedMimic/index.html","repo":"https://github.com/NVlabs/ProtoMotions","backbone_tags":"","approach_tags":"Multi-Task, Physical, RL"},{"arxiv_id":"2409.13251","title":"T2M-X: Learning Expressive Text-to-Motion Generation from Partially Annotated Data","abstract":"The generation of humanoid animation from text prompts can profoundly impact\\nanimation production and AR/VR experiences. However, existing methods only\\ngenerate body motion data, excluding facial expressions and hand movements.\\nThis limitation, primarily due to a lack of a comprehensive whole-body motion\\ndataset, inhibits their readiness for production use. Recent attempts to create\\nsuch a dataset have resulted in either motion inconsistency among different\\nbody parts in the artificially augmented data or lower quality in the data\\nextracted from RGB videos. In this work, we propose T2M-X, a two-stage method\\nthat learns expressive text-to-motion generation from partially annotated data.\\nT2M-X trains three separate Vector Quantized Variational AutoEncoders (VQ-VAEs)\\nfor body, hand, and face on respective high-quality data sources to ensure\\nhigh-quality motion outputs, and a Multi-indexing Generative Pretrained\\nTransformer (GPT) model with motion consistency loss for motion generation and\\ncoordination among different body parts. Our results show significant\\nimprovements over the baselines both quantitatively and qualitatively,\\ndemonstrating its robustness against the dataset limitations.","authors":["Mingdian Liu","Yilin Liu","Gurunandan Krishnan","Karl S Bayer","Bing Zhou"],"year":2024,"month":9,"url":"https://arxiv.org/abs/2409.13251","survey":false,"survey_abbr":"","model":true,"model_abbr":"T2M-X","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"VQ-VAE, Transformer","approach_tags":""},{"arxiv_id":"2409.12140","title":"MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion","abstract":"We introduce MoRAG, a novel multi-part fusion based retrieval-augmented\\ngeneration strategy for text-based human motion generation. The method enhances\\nmotion diffusion models by leveraging additional knowledge obtained through an\\nimproved motion retrieval process. By effectively prompting large language\\nmodels (LLMs), we address spelling errors and rephrasing issues in motion\\nretrieval. Our approach utilizes a multi-part retrieval strategy to improve the\\ngeneralizability of motion retrieval across the language space. We create\\ndiverse samples through the spatial composition of the retrieved motions.\\nFurthermore, by utilizing low-level, part-specific motion information, we can\\nconstruct motion samples for unseen text descriptions. Our experiments\\ndemonstrate that our framework can serve as a plug-and-play module, improving\\nthe performance of motion diffusion models. Code, pretrained models and sample\\nvideos are available at: https://motion-rag.github.io/","authors":["Sai Shashank Kalakonda","Shubh Maheshwari","Ravi Kiran Sarvadevabhatla"],"year":2024,"month":9,"url":"https://arxiv.org/abs/2409.12140","survey":false,"survey_abbr":"","model":true,"model_abbr":"MoRAG","dataset":false,"dataset_abbr":"","submission":"WACV","submission_year":"2025","page":"https://motion-rag.github.io/","repo":"https://github.com/Motion-RAG/MoRAG","backbone_tags":"LLM, Diffusion","approach_tags":""},{"arxiv_id":"2409.10847","title":"BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation","abstract":"Autoregressive models excel in modeling sequential dependencies by enforcing\\ncausal constraints, yet they struggle to capture complex bidirectional patterns\\ndue to their unidirectional nature. In contrast, mask-based models leverage\\nbidirectional context, enabling richer dependency modeling. However, they often\\nassume token independence during prediction, which undermines the modeling of\\nsequential dependencies. Additionally, the corruption of sequences through\\nmasking or absorption can introduce unnatural distortions, complicating the\\nlearning process. To address these issues, we propose Bidirectional\\nAutoregressive Diffusion (BAD), a novel approach that unifies the strengths of\\nautoregressive and mask-based generative models. BAD utilizes a\\npermutation-based corruption technique that preserves the natural sequence\\nstructure while enforcing causal dependencies through randomized ordering,\\nenabling the effective capture of both sequential and bidirectional\\nrelationships. Comprehensive experiments show that BAD outperforms\\nautoregressive and mask-based models in text-to-motion generation, suggesting a\\nnovel pre-training strategy for sequence modeling. The codebase for BAD is\\navailable on https://github.com/RohollahHS/BAD.","authors":["S. Rohollah Hosseyni","Ali Ahmad Rahmani","S. Jamal Seyedmohammadi","Sanaz Seyedin","Arash Mohammadi"],"year":2024,"month":9,"url":"https://arxiv.org/abs/2409.10847","survey":false,"survey_abbr":"","model":true,"model_abbr":"BAD","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://rohollahhs.github.io/BAD-page/","repo":"https://github.com/rohollahhs/bad/","backbone_tags":"CLIP, Transformer","approach_tags":""},{"arxiv_id":"2409.01522","title":"Lagrangian Motion Fields for Long-term Motion Generation","abstract":"Long-term motion generation is a challenging task that requires producing\\ncoherent and realistic sequences over extended durations. Current methods\\nprimarily rely on framewise motion representations, which capture only static\\nspatial details and overlook temporal dynamics. This approach leads to\\nsignificant redundancy across the temporal dimension, complicating the\\ngeneration of effective long-term motion. To overcome these limitations, we\\nintroduce the novel concept of Lagrangian Motion Fields, specifically designed\\nfor long-term motion generation. By treating each joint as a Lagrangian\\nparticle with uniform velocity over short intervals, our approach condenses\\nmotion representations into a series of \\"supermotions\\" (analogous to\\nsuperpixels). This method seamlessly integrates static spatial information with\\ninterpretable temporal dynamics, transcending the limitations of existing\\nnetwork architectures and motion sequence content types. Our solution is\\nversatile and lightweight, eliminating the need for neural network\\npreprocessing. Our approach excels in tasks such as long-term music-to-dance\\ngeneration and text-to-motion generation, offering enhanced efficiency,\\nsuperior generation quality, and greater diversity compared to existing\\nmethods. Additionally, the adaptability of Lagrangian Motion Fields extends to\\napplications like infinite motion looping and fine-grained controlled motion\\ngeneration, highlighting its broad utility. Video demonstrations are available\\nat \\\\url{https://plyfager.github.io/LaMoG}.","authors":["Yifei Yang","Zikai Huang","Chenshu Xu","Shengfeng He"],"year":2024,"month":9,"url":"https://arxiv.org/abs/2409.01522","survey":false,"survey_abbr":"","model":true,"model_abbr":"LaMoG","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://plyfager.github.io/LaMoG/","repo":"","backbone_tags":"","approach_tags":"Component, Multi-Task"},{"arxiv_id":"2408.03302","title":"TextIM: Part-aware Interactive Motion Synthesis from Text","abstract":"In this work, we propose TextIM, a novel framework for synthesizing\\nTEXT-driven human Interactive Motions, with a focus on the precise alignment of\\npart-level semantics. Existing methods often overlook the critical roles of\\ninteractive body parts and fail to adequately capture and align part-level\\nsemantics, resulting in inaccuracies and even erroneous movement outcomes. To\\naddress these issues, TextIM utilizes a decoupled conditional diffusion\\nframework to enhance the detailed alignment between interactive movements and\\ncorresponding semantic intents from textual descriptions. Our approach\\nleverages large language models, functioning as a human brain, to identify\\ninteracting human body parts and to comprehend interaction semantics to\\ngenerate complicated and subtle interactive motion. Guided by the refined\\nmovements of the interacting parts, TextIM further extends these movements into\\na coherent whole-body motion. We design a spatial coherence module to\\ncomplement the entire body movements while maintaining consistency and harmony\\nacross body parts using a part graph convolutional network. For training and\\nevaluation, we carefully selected and re-labeled interactive motions from\\nHUMANML3D to develop a specialized dataset. Experimental results demonstrate\\nthat TextIM produces semantically accurate human interactive motions,\\nsignificantly enhancing the realism and applicability of synthesized\\ninteractive motions in diverse scenarios, even including interactions with\\ndeformable and dynamically changing objects.","authors":["Siyuan Fan","Bo Du","Xiantao Cai","Bo Peng","Longling Sun"],"year":2024,"month":8,"url":"https://arxiv.org/abs/2408.03302","survey":false,"survey_abbr":"","model":true,"model_abbr":"TextIM","dataset":false,"dataset_abbr":"","submission":"EUROGRAPHICS","submission_year":"2025","page":"","repo":"","backbone_tags":"Transformer","approach_tags":"Finegrained"},{"arxiv_id":"2408.00712","title":"MotionFix: Text-Driven 3D Human Motion Editing","abstract":"The focus of this paper is on 3D motion editing. Given a 3D human motion and\\na textual description of the desired modification, our goal is to generate an\\nedited motion as described by the text. The key challenges include the scarcity\\nof training data and the need to design a model that accurately edits the\\nsource motion. In this paper, we address both challenges. We propose a\\nmethodology to semi-automatically collect a dataset of triplets comprising (i)\\na source motion, (ii) a target motion, and (iii) an edit text, introducing the\\nnew MotionFix dataset. Access to this data allows us to train a conditional\\ndiffusion model, TMED, that takes both the source motion and the edit text as\\ninput. We develop several baselines to evaluate our model, comparing it against\\nmodels trained solely on text-motion pair datasets, and demonstrate the\\nsuperior performance of our model trained on triplets. We also introduce new\\nretrieval-based metrics for motion editing, establishing a benchmark on the\\nevaluation set of MotionFix. Our results are promising, paving the way for\\nfurther research in fine-grained motion generation. Code, models, and data are\\navailable at https://motionfix.is.tue.mpg.de/ .","authors":["Nikos Athanasiou","Alp\xe1r Cseke","Markos Diomataris","Michael J. Black","G\xfcl Varol"],"year":2024,"month":8,"url":"https://arxiv.org/abs/2408.00712","survey":false,"survey_abbr":"","model":true,"model_abbr":"TMED","dataset":true,"dataset_abbr":"MotionFix","submission":"SIGGRAPH Asia","submission_year":"2024","page":"https://motionfix.is.tue.mpg.de/","repo":"https://github.com/atnikos/motionfix","backbone_tags":"CLIP, Diffusion, Transformer","approach_tags":"Editing"},{"arxiv_id":"2408.00352","title":"Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion","abstract":"Human motion generation driven by deep generative models has enabled\\ncompelling applications, but the ability of text-to-motion (T2M) models to\\nproduce realistic motions from text prompts raises security concerns if\\nexploited maliciously. Despite growing interest in T2M, few methods focus on\\nsafeguarding these models against adversarial attacks, with existing work on\\ntext-to-image models proving insufficient for the unique motion domain. In the\\npaper, we propose ALERT-Motion, an autonomous framework leveraging large\\nlanguage models (LLMs) to craft targeted adversarial attacks against black-box\\nT2M models. Unlike prior methods modifying prompts through predefined rules,\\nALERT-Motion uses LLMs\' knowledge of human motion to autonomously generate\\nsubtle yet powerful adversarial text descriptions. It comprises two key\\nmodules: an adaptive dispatching module that constructs an LLM-based agent to\\niteratively refine and search for adversarial prompts; and a multimodal\\ninformation contrastive module that extracts semantically relevant motion\\ninformation to guide the agent\'s search. Through this LLM-driven approach,\\nALERT-Motion crafts adversarial prompts querying victim models to produce\\noutputs closely matching targeted motions, while avoiding obvious\\nperturbations. Evaluations across popular T2M models demonstrate ALERT-Motion\'s\\nsuperiority over previous methods, achieving higher attack success rates with\\nstealthier adversarial prompts. This pioneering work on T2M adversarial attacks\\nhighlights the urgency of developing defensive measures as motion generation\\ntechnology advances, urging further research into safe and responsible\\ndeployment.","authors":["Honglei Miao","Fan Ma","Ruijie Quan","Kun Zhan","Yi Yang"],"year":2024,"month":8,"url":"https://arxiv.org/abs/2408.00352","survey":false,"survey_abbr":"","model":true,"model_abbr":"ALERT-Motion","dataset":false,"dataset_abbr":"","submission":"AAAI","submission_year":"2025","page":"","repo":"","backbone_tags":"","approach_tags":"Attack"},{"arxiv_id":"2407.21136","title":"MotionCraft: Crafting Whole-Body Motion with Plug-and-Play Multimodal Controls","abstract":"Whole-body multimodal motion generation, controlled by text, speech, or\\nmusic, has numerous applications including video generation and character\\nanimation. However, employing a unified model to achieve various generation\\ntasks with different condition modalities presents two main challenges: motion\\ndistribution drifts across different tasks (e.g., co-speech gestures and\\ntext-driven daily actions) and the complex optimization of mixed conditions\\nwith varying granularities (e.g., text and audio). Additionally, inconsistent\\nmotion formats across different tasks and datasets hinder effective training\\ntoward multimodal motion generation. In this paper, we propose MotionCraft, a\\nunified diffusion transformer that crafts whole-body motion with plug-and-play\\nmultimodal control. Our framework employs a coarse-to-fine training strategy,\\nstarting with the first stage of text-to-motion semantic pre-training, followed\\nby the second stage of multimodal low-level control adaptation to handle\\nconditions of varying granularities. To effectively learn and transfer motion\\nknowledge across different distributions, we design MC-Attn for parallel\\nmodeling of static and dynamic human topology graphs. To overcome the motion\\nformat inconsistency of existing benchmarks, we introduce MC-Bench, the first\\navailable multimodal whole-body motion generation benchmark based on the\\nunified SMPL-X format. Extensive experiments show that MotionCraft achieves\\nstate-of-the-art performance on various standard motion generation tasks.","authors":["Yuxuan Bian","Ailing Zeng","Xuan Ju","Xian Liu","Zhaoyang Zhang","Wei Liu","Qiang Xu"],"year":2024,"month":7,"url":"https://arxiv.org/abs/2407.21136","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionCraft","dataset":false,"dataset_abbr":"","submission":"AAAI","submission_year":"2025","page":"https://cure-lab.github.io/MotionCraft/","repo":"https://github.com/cure-lab/MotionCraft","backbone_tags":"Diffusion, Transformer","approach_tags":"Multi-Task"},{"arxiv_id":"2407.14502","title":"M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models","abstract":"We introduce the Multi-Motion Discrete Diffusion Models (M2D2M), a novel\\napproach for human motion generation from textual descriptions of multiple\\nactions, utilizing the strengths of discrete diffusion models. This approach\\nadeptly addresses the challenge of generating multi-motion sequences, ensuring\\nseamless transitions of motions and coherence across a series of actions. The\\nstrength of M2D2M lies in its dynamic transition probability within the\\ndiscrete diffusion model, which adapts transition probabilities based on the\\nproximity between motion tokens, encouraging mixing between different modes.\\nComplemented by a two-phase sampling strategy that includes independent and\\njoint denoising steps, M2D2M effectively generates long-term, smooth, and\\ncontextually coherent human motion sequences, utilizing a model trained for\\nsingle-motion generation. Extensive experiments demonstrate that M2D2M\\nsurpasses current state-of-the-art benchmarks for motion generation from text\\ndescriptions, showcasing its efficacy in interpreting language semantics and\\ngenerating dynamic, realistic motions.","authors":["Seunggeun Chi","Hyung-gun Chi","Hengbo Ma","Nakul Agarwal","Faizan Siddiqui","Karthik Ramani","Kwonjoon Lee"],"year":2024,"month":7,"url":"https://arxiv.org/abs/2407.14502","survey":false,"survey_abbr":"","model":true,"model_abbr":"M2D2M","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2024","page":"","repo":"","backbone_tags":"VQ-VAE, Transformer","approach_tags":"Diversity"},{"arxiv_id":"2407.12783","title":"SMooDi: Stylized Motion Diffusion Model","abstract":"We introduce a novel Stylized Motion Diffusion model, dubbed SMooDi, to\\ngenerate stylized motion driven by content texts and style motion sequences.\\nUnlike existing methods that either generate motion of various content or\\ntransfer style from one sequence to another, SMooDi can rapidly generate motion\\nacross a broad range of content and diverse styles. To this end, we tailor a\\npre-trained text-to-motion model for stylization. Specifically, we propose\\nstyle guidance to ensure that the generated motion closely matches the\\nreference style, alongside a lightweight style adaptor that directs the motion\\ntowards the desired style while ensuring realism. Experiments across various\\napplications demonstrate that our proposed framework outperforms existing\\nmethods in stylized motion generation.","authors":["Lei Zhong","Yiming Xie","Varun Jampani","Deqing Sun","Huaizu Jiang"],"year":2024,"month":7,"url":"https://arxiv.org/abs/2407.12783","survey":false,"survey_abbr":"","model":true,"model_abbr":"SMooDi","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2024","page":"https://neu-vi.github.io/SMooDi/","repo":"https://github.com/neu-vi/SMooDi","backbone_tags":"Diffusion","approach_tags":"Style"},{"arxiv_id":"2407.10528","title":"Local Action-Guided Motion Diffusion Model for Text-to-Motion Generation","abstract":"Text-to-motion generation requires not only grounding local actions in\\nlanguage but also seamlessly blending these individual actions to synthesize\\ndiverse and realistic global motions. However, existing motion generation\\nmethods primarily focus on the direct synthesis of global motions while\\nneglecting the importance of generating and controlling local actions. In this\\npaper, we propose the local action-guided motion diffusion model, which\\nfacilitates global motion generation by utilizing local actions as fine-grained\\ncontrol signals. Specifically, we provide an automated method for reference\\nlocal action sampling and leverage graph attention networks to assess the\\nguiding weight of each local action in the overall motion synthesis. During the\\ndiffusion process for synthesizing global motion, we calculate the local-action\\ngradient to provide conditional guidance. This local-to-global paradigm reduces\\nthe complexity associated with direct global motion generation and promotes\\nmotion diversity via sampling diverse actions as conditions. Extensive\\nexperiments on two human motion datasets, i.e., HumanML3D and KIT, demonstrate\\nthe effectiveness of our method. Furthermore, our method provides flexibility\\nin seamlessly combining various local actions and continuous guiding weight\\nadjustment, accommodating diverse user preferences, which may hold potential\\nsignificance for the community. The project page is available at\\nhttps://jpthu17.github.io/GuidedMotion-project/.","authors":["Peng Jin","Hao Li","Zesen Cheng","Kehan Li","Runyi Yu","Chang Liu","Xiangyang Ji","Li Yuan","Jie Chen"],"year":2024,"month":7,"url":"https://arxiv.org/abs/2407.10528","survey":false,"survey_abbr":"","model":true,"model_abbr":"GuidedMotion","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://jpthu17.github.io/GuidedMotion-project/","repo":"","backbone_tags":"Diffusion, Transformer","approach_tags":"Finegrained, Graph"},{"arxiv_id":"2407.10481","title":"SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation","abstract":"Physically-simulated models for human motion can generate high-quality\\nresponsive character animations, often in real-time. Natural language serves as\\na flexible interface for controlling these models, allowing expert and\\nnon-expert users to quickly create and edit their animations. Many recent\\nphysics-based animation methods, including those that use text interfaces,\\ntrain control policies using reinforcement learning (RL). However, scaling\\nthese methods beyond several hundred motions has remained challenging.\\nMeanwhile, kinematic animation models are able to successfully learn from\\nthousands of diverse motions by leveraging supervised learning methods.\\nInspired by these successes, in this work we introduce SuperPADL, a scalable\\nframework for physics-based text-to-motion that leverages both RL and\\nsupervised learning to train controllers on thousands of diverse motion clips.\\nSuperPADL is trained in stages using progressive distillation, starting with a\\nlarge number of specialized experts using RL. These experts are then\\niteratively distilled into larger, more robust policies using a combination of\\nreinforcement learning and supervised learning. Our final SuperPADL controller\\nis trained on a dataset containing over 5000 skills and runs in real time on a\\nconsumer GPU. Moreover, our policy can naturally transition between skills,\\nallowing for users to interactively craft multi-stage animations. We\\nexperimentally demonstrate that SuperPADL significantly outperforms RL-based\\nbaselines at this large data scale.","authors":["Jordan Juravsky","Yunrong Guo","Sanja Fidler","Xue Bin Peng"],"year":2024,"month":7,"url":"https://arxiv.org/abs/2407.10481","survey":false,"survey_abbr":"","model":true,"model_abbr":"SuperPADL","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"","approach_tags":"RL, Physical"},{"arxiv_id":"2407.10061","title":"InfiniMotion: Mamba Boosts Memory in Transformer for Arbitrary Long Motion Generation","abstract":"Text-to-motion generation holds potential for film, gaming, and robotics, yet\\ncurrent methods often prioritize short motion generation, making it challenging\\nto produce long motion sequences effectively: (1) Current methods struggle to\\nhandle long motion sequences as a single input due to prohibitively high\\ncomputational cost; (2) Breaking down the generation of long motion sequences\\ninto shorter segments can result in inconsistent transitions and requires\\ninterpolation or inpainting, which lacks entire sequence modeling. To solve\\nthese challenges, we propose InfiniMotion, a method that generates continuous\\nmotion sequences of arbitrary length within an autoregressive framework. We\\nhighlight its groundbreaking capability by generating a continuous 1-hour human\\nmotion with around 80,000 frames. Specifically, we introduce the Motion Memory\\nTransformer with Bidirectional Mamba Memory, enhancing the transformer\'s memory\\nto process long motion sequences effectively without overwhelming computational\\nresources. Notably our method achieves over 30% improvement in FID and 6 times\\nlonger demonstration compared to previous state-of-the-art methods, showcasing\\nsignificant advancements in long motion generation. See project webpage:\\nhttps://steve-zeyu-zhang.github.io/InfiniMotion/","authors":["Zeyu Zhang","Akide Liu","Qi Chen","Feng Chen","Ian Reid","Richard Hartley","Bohan Zhuang","Hao Tang"],"year":2024,"month":7,"url":"https://arxiv.org/abs/2407.10061","survey":false,"survey_abbr":"","model":true,"model_abbr":"InfiniMotion","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"VQ-VAE, Transformer, Mamba","approach_tags":""},{"arxiv_id":"2407.08443","title":"Infinite Motion: Extended Motion Generation via Long Text Instructions","abstract":"In the realm of motion generation, the creation of long-duration,\\nhigh-quality motion sequences remains a significant challenge. This paper\\npresents our groundbreaking work on \\"Infinite Motion\\", a novel approach that\\nleverages long text to extended motion generation, effectively bridging the gap\\nbetween short and long-duration motion synthesis. Our core insight is the\\nstrategic extension and reassembly of existing high-quality text-motion\\ndatasets, which has led to the creation of a novel benchmark dataset to\\nfacilitate the training of models for extended motion sequences. A key\\ninnovation of our model is its ability to accept arbitrary lengths of text as\\ninput, enabling the generation of motion sequences tailored to specific\\nnarratives or scenarios. Furthermore, we incorporate the timestamp design for\\ntext which allows precise editing of local segments within the generated\\nsequences, offering unparalleled control and flexibility in motion synthesis.\\nWe further demonstrate the versatility and practical utility of \\"Infinite\\nMotion\\" through three specific applications: natural language interactive\\nediting, motion sequence editing within long sequences and splicing of\\nindependent motion sequences. Each application highlights the adaptability of\\nour approach and broadens the spectrum of possibilities for research and\\ndevelopment in motion generation. Through extensive experiments, we demonstrate\\nthe superior performance of our model in generating long sequence motions\\ncompared to existing methods.Project page:\\nhttps://shuochengzhai.github.io/Infinite-motion.github.io/","authors":["Mengtian Li","Chengshuo Zhai","Shengxiang Yao","Zhifeng Xie","Keyu Chen","Yu-Gang Jiang"],"year":2024,"month":7,"url":"https://arxiv.org/abs/2407.08443","survey":false,"survey_abbr":"","model":true,"model_abbr":"InfiniteMotion","dataset":true,"dataset_abbr":"HumanML3D-Extend","submission":"","submission_year":"","page":"https://shuochengzhai.github.io/Infinite-motion.github.io/","repo":"","backbone_tags":"","approach_tags":"Editing"},{"arxiv_id":"2407.02272","title":"Aligning Human Motion Generation with Human Perceptions","abstract":"Human motion generation is a critical task with a wide range of applications.\\nAchieving high realism in generated motions requires naturalness, smoothness,\\nand plausibility. Despite rapid advancements in the field, current generation\\nmethods often fall short of these goals. Furthermore, existing evaluation\\nmetrics typically rely on ground-truth-based errors, simple heuristics, or\\ndistribution distances, which do not align well with human perceptions of\\nmotion quality. In this work, we propose a data-driven approach to bridge this\\ngap by introducing a large-scale human perceptual evaluation dataset,\\nMotionPercept, and a human motion critic model, MotionCritic, that capture\\nhuman perceptual preferences. Our critic model offers a more accurate metric\\nfor assessing motion quality and could be readily integrated into the motion\\ngeneration pipeline to enhance generation quality. Extensive experiments\\ndemonstrate the effectiveness of our approach in both evaluating and improving\\nthe quality of generated human motions by aligning with human perceptions. Code\\nand data are publicly available at https://motioncritic.github.io/.","authors":["Haoru Wang","Wentao Zhu","Luyi Miao","Yishu Xu","Feng Gao","Qi Tian","Yizhou Wang"],"year":2024,"month":7,"url":"https://arxiv.org/abs/2407.02272","survey":false,"survey_abbr":"","model":false,"model_abbr":"","dataset":true,"dataset_abbr":"MotionPercept","submission":"ICLR","submission_year":"2025","page":"https://motioncritic.github.io/","repo":"https://github.com/ou524u/MotionCritic","backbone_tags":"","approach_tags":""},{"arxiv_id":"2406.01867","title":"MoLA: Motion Generation and Editing with Latent Diffusion Enhanced by Adversarial Training","abstract":"In text-to-motion generation, controllability as well as generation quality\\nand speed has become increasingly critical. The controllability challenges\\ninclude generating a motion of a length that matches the given textual\\ndescription and editing the generated motions according to control signals,\\nsuch as the start-end positions and the pelvis trajectory. In this paper, we\\npropose MoLA, which provides fast, high-quality, variable-length motion\\ngeneration and can also deal with multiple editing tasks in a single framework.\\nOur approach revisits the motion representation used as inputs and outputs in\\nthe model, incorporating an activation variable to enable variable-length\\nmotion generation. Additionally, we integrate a variational autoencoder and a\\nlatent diffusion model, further enhanced through adversarial training, to\\nachieve high-quality and fast generation. Moreover, we apply a training-free\\nguided generation framework to achieve various editing tasks with motion\\ncontrol inputs. We quantitatively show the effectiveness of adversarial\\nlearning in text-to-motion generation, and demonstrate the applicability of our\\nediting framework to multiple editing tasks in the motion domain.","authors":["Kengo Uchida","Takashi Shibuya","Yuhta Takida","Naoki Murata","Julian Tanke","Shusuke Takahashi","Yuki Mitsufuji"],"year":2024,"month":6,"url":"https://arxiv.org/abs/2406.01867","survey":false,"survey_abbr":"","model":true,"model_abbr":"MoLA","dataset":false,"dataset_abbr":"","submission":"CVPR Workshop","submission_year":"2025","page":"https://kengouchida-sony.github.io/MoLA-demo/","repo":"https://github.com/sony/MoLA","backbone_tags":"VAE, GAN","approach_tags":"Trajectory, Multi-Task"},{"arxiv_id":"2406.00636","title":"T2LM: Long-Term 3D Human Motion Generation from Multiple Sentences","abstract":"In this paper, we address the challenging problem of long-term 3D human\\nmotion generation. Specifically, we aim to generate a long sequence of smoothly\\nconnected actions from a stream of multiple sentences (i.e., paragraph).\\nPrevious long-term motion generating approaches were mostly based on recurrent\\nmethods, using previously generated motion chunks as input for the next step.\\nHowever, this approach has two drawbacks: 1) it relies on sequential datasets,\\nwhich are expensive; 2) these methods yield unrealistic gaps between motions\\ngenerated at each step. To address these issues, we introduce simple yet\\neffective T2LM, a continuous long-term generation framework that can be trained\\nwithout sequential data. T2LM comprises two components: a 1D-convolutional\\nVQVAE, trained to compress motion to sequences of latent vectors, and a\\nTransformer-based Text Encoder that predicts a latent sequence given an input\\ntext. At inference, a sequence of sentences is translated into a continuous\\nstream of latent vectors. This is then decoded into a motion by the VQVAE\\ndecoder; the use of 1D convolutions with a local temporal receptive field\\navoids temporal inconsistencies between training and generated sequences. This\\nsimple constraint on the VQ-VAE allows it to be trained with short sequences\\nonly and produces smoother transitions. T2LM outperforms prior long-term\\ngeneration models while overcoming the constraint of requiring sequential data;\\nit is also competitive with SOTA single-action generation models.","authors":["Taeryung Lee","Fabien Baradel","Thomas Lucas","Kyoung Mu Lee","Gregory Rogez"],"year":2024,"month":6,"url":"https://arxiv.org/abs/2406.00636","survey":false,"survey_abbr":"","model":true,"model_abbr":"T2LM","dataset":false,"dataset_abbr":"","submission":"CVPR Workshop","submission_year":"2024","page":"","repo":"","backbone_tags":"VQ-VAE, Transformer","approach_tags":""},{"arxiv_id":"2405.19283","title":"Programmable Motion Generation for Open-Set Motion Control Tasks","abstract":"Character animation in real-world scenarios necessitates a variety of\\nconstraints, such as trajectories, key-frames, interactions, etc. Existing\\nmethodologies typically treat single or a finite set of these constraint(s) as\\nseparate control tasks. They are often specialized, and the tasks they address\\nare rarely extendable or customizable. We categorize these as solutions to the\\nclose-set motion control problem. In response to the complexity of practical\\nmotion control, we propose and attempt to solve the open-set motion control\\nproblem. This problem is characterized by an open and fully customizable set of\\nmotion control tasks. To address this, we introduce a new paradigm,\\nprogrammable motion generation. In this paradigm, any given motion control task\\nis broken down into a combination of atomic constraints. These constraints are\\nthen programmed into an error function that quantifies the degree to which a\\nmotion sequence adheres to them. We utilize a pre-trained motion generation\\nmodel and optimize its latent code to minimize the error function of the\\ngenerated motion. Consequently, the generated motion not only inherits the\\nprior of the generative model but also satisfies the required constraints.\\nExperiments show that we can generate high-quality motions when addressing a\\nwide range of unseen tasks. These tasks encompass motion control by motion\\ndynamics, geometric constraints, physical laws, interactions with scenes,\\nobjects or the character own body parts, etc. All of these are achieved in a\\nunified approach, without the need for ad-hoc paired training data collection\\nor specialized network designs. During the programming of novel tasks, we\\nobserved the emergence of new skills beyond those of the prior model. With the\\nassistance of large language models, we also achieved automatic programming. We\\nhope that this work will pave the way for the motion control of general AI\\nagents.","authors":["Hanchao Liu","Xiaohang Zhan","Shaoli Huang","Tai-Jiang Mu","Ying Shan"],"year":2024,"month":5,"url":"https://arxiv.org/abs/2405.19283","survey":false,"survey_abbr":"","model":true,"model_abbr":"ProgMoGen","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2024","page":"https://hanchaoliu.github.io/Prog-MoGen/","repo":"https://github.com/HanchaoLiu/ProgMoGen","backbone_tags":"","approach_tags":"Finegrained, Trajectory, MultiTask"},{"arxiv_id":"2405.17013","title":"Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs","abstract":"While previous approaches to 3D human motion generation have achieved notable\\nsuccess, they often rely on extensive training and are limited to specific\\ntasks. To address these challenges, we introduce Motion-Agent, an efficient\\nconversational framework designed for general human motion generation, editing,\\nand understanding. Motion-Agent employs an open-source pre-trained language\\nmodel to develop a generative agent, MotionLLM, that bridges the gap between\\nmotion and text. This is accomplished by encoding and quantizing motions into\\ndiscrete tokens that align with the language model\'s vocabulary. With only\\n1--3\\\\% of the model\'s parameters fine-tuned using adapters, MotionLLM delivers\\nperformance on par with diffusion models and other transformer-based methods\\ntrained from scratch. By integrating MotionLLM with GPT-4 without additional\\ntraining, Motion-Agent is able to generate highly complex motion sequences\\nthrough multi-turn conversations, a capability that previous models have\\nstruggled to achieve. Motion-Agent supports a wide range of motion-language\\ntasks, offering versatile capabilities for generating and customizing human\\nmotion through interactive conversational exchanges. Project page:\\nhttps://knoxzhao.github.io/Motion-Agent","authors":["Qi Wu","Yubo Zhao","Yifan Wang","Xinhang Liu","Yu-Wing Tai","Chi-Keung Tang"],"year":2024,"month":5,"url":"https://arxiv.org/abs/2405.17013","survey":false,"survey_abbr":"","model":true,"model_abbr":"Motion-Agent","dataset":false,"dataset_abbr":"","submission":"ICLR","submission_year":"2025","page":"https://knoxzhao.github.io/Motion-Agent/","repo":"https://github.com/szqwu/Motion-Agent","backbone_tags":"LLM","approach_tags":"LoRA, Editing"},{"arxiv_id":"2405.16909","title":"A Cross-Dataset Study for Text-based 3D Human Motion Retrieval","abstract":"We provide results of our study on text-based 3D human motion retrieval and\\nparticularly focus on cross-dataset generalization. Due to practical reasons\\nsuch as dataset-specific human body representations, existing works typically\\nbenchmarkby training and testing on partitions from the same dataset. Here, we\\nemploy a unified SMPL body format for all datasets, which allows us to perform\\ntraining on one dataset, testing on the other, as well as training on a\\ncombination of datasets. Our results suggest that there exist dataset biases in\\nstandard text-motion benchmarks such as HumanML3D, KIT Motion-Language, and\\nBABEL. We show that text augmentations help close the domain gap to some\\nextent, but the gap remains. We further provide the first zero-shot action\\nrecognition results on BABEL, without using categorical action labels during\\ntraining, opening up a new avenue for future research.","authors":["L\xe9ore Bensabath","Mathis Petrovich","G\xfcl Varol"],"year":2024,"month":5,"url":"https://arxiv.org/abs/2405.16909","survey":false,"survey_abbr":"","model":true,"model_abbr":"TMR++","dataset":false,"dataset_abbr":"","submission":"CVPR Workshop","submission_year":"2024","page":"https://imagine.enpc.fr/~leore.bensabath/TMR++/","repo":"https://github.com/leorebensabath/TMRPlusPlus","backbone_tags":"","approach_tags":"Retrieval"},{"arxiv_id":"2405.16273","title":"M$^3$GPT: An Advanced Multimodal, Multitask Framework for Motion Comprehension and Generation","abstract":"This paper presents M$^3$GPT, an advanced $\\\\textbf{M}$ultimodal,\\n$\\\\textbf{M}$ultitask framework for $\\\\textbf{M}$otion comprehension and\\ngeneration. M$^3$GPT operates on three fundamental principles. The first\\nfocuses on creating a unified representation space for various motion-relevant\\nmodalities. We employ discrete vector quantization for multimodal conditional\\nsignals, such as text, music and motion/dance, enabling seamless integration\\ninto a large language model (LLM) with a single vocabulary. The second involves\\nmodeling motion generation directly in the raw motion space. This strategy\\ncircumvents the information loss associated with a discrete tokenizer,\\nresulting in more detailed and comprehensive motion generation. Third, M$^3$GPT\\nlearns to model the connections and synergies among various motion-relevant\\ntasks. Text, the most familiar and well-understood modality for LLMs, is\\nutilized as a bridge to establish connections between different motion tasks,\\nfacilitating mutual reinforcement. To our knowledge, M$^3$GPT is the first\\nmodel capable of comprehending and generating motions based on multiple\\nsignals. Extensive experiments highlight M$^3$GPT\'s superior performance across\\nvarious motion-relevant tasks and its powerful zero-shot generalization\\ncapabilities for extremely challenging tasks. Project page:\\n\\\\url{https://github.com/luomingshuang/M3GPT}.","authors":["Mingshuang Luo","Ruibing Hou","Zhuo Li","Hong Chang","Zimo Liu","Yaowei Wang","Shiguang Shan"],"year":2024,"month":5,"url":"https://arxiv.org/abs/2405.16273","survey":false,"survey_abbr":"","model":true,"model_abbr":"M3GPT","dataset":false,"dataset_abbr":"","submission":"NeurIPS","submission_year":"2024","page":"https://luomingshuang.github.io/M3GPT/","repo":"https://github.com/luomingshuang/M3GPT","backbone_tags":"LLM","approach_tags":"Multi-Task"},{"arxiv_id":"2405.15541","title":"Learning Generalizable Human Motion Generator with Reinforcement Learning","abstract":"Text-driven human motion generation, as one of the vital tasks in\\ncomputer-aided content creation, has recently attracted increasing attention.\\nWhile pioneering research has largely focused on improving numerical\\nperformance metrics on given datasets, practical applications reveal a common\\nchallenge: existing methods often overfit specific motion expressions in the\\ntraining data, hindering their ability to generalize to novel descriptions like\\nunseen combinations of motions. This limitation restricts their broader\\napplicability. We argue that the aforementioned problem primarily arises from\\nthe scarcity of available motion-text pairs, given the many-to-many nature of\\ntext-driven motion generation. To tackle this problem, we formulate\\ntext-to-motion generation as a Markov decision process and present\\n\\\\textbf{InstructMotion}, which incorporate the trail and error paradigm in\\nreinforcement learning for generalizable human motion generation. Leveraging\\ncontrastive pre-trained text and motion encoders, we delve into optimizing\\nreward design to enable InstructMotion to operate effectively on both paired\\ndata, enhancing global semantic level text-motion alignment, and synthetic\\ntext-only data, facilitating better generalization to novel prompts without the\\nneed for ground-truth motion supervision. Extensive experiments on prevalent\\nbenchmarks and also our synthesized unpaired dataset demonstrate that the\\nproposed InstructMotion achieves outstanding performance both quantitatively\\nand qualitatively.","authors":["Yunyao Mao","Xiaoyang Liu","Wengang Zhou","Zhenbo Lu","Houqiang Li"],"year":2024,"month":5,"url":"https://arxiv.org/abs/2405.15541","survey":false,"survey_abbr":"","model":true,"model_abbr":"InstructMotion","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"CLIP, Transformer, VQ-VAE","approach_tags":"RL"},{"arxiv_id":"2405.06778","title":"Shape Conditioned Human Motion Generation with Diffusion Model","abstract":"Human motion synthesis is an important task in computer graphics and computer\\nvision. While focusing on various conditioning signals such as text, action\\nclass, or audio to guide the generation process, most existing methods utilize\\nskeleton-based pose representation, requiring additional skinning to produce\\nrenderable meshes. Given that human motion is a complex interplay of bones,\\njoints, and muscles, considering solely the skeleton for generation may neglect\\ntheir inherent interdependency, which can limit the variability and precision\\nof the generated results. To address this issue, we propose a Shape-conditioned\\nMotion Diffusion model (SMD), which enables the generation of motion sequences\\ndirectly in mesh format, conditioned on a specified target mesh. In SMD, the\\ninput meshes are transformed into spectral coefficients using graph Laplacian,\\nto efficiently represent meshes. Subsequently, we propose a Spectral-Temporal\\nAutoencoder (STAE) to leverage cross-temporal dependencies within the spectral\\ndomain. Extensive experimental evaluations show that SMD not only produces\\nvivid and realistic motions but also achieves competitive performance in\\ntext-to-motion and action-to-motion tasks when compared to state-of-the-art\\nmethods.","authors":["Kebing Xue","Hyewon Seo"],"year":2024,"month":5,"url":"https://arxiv.org/abs/2405.06778","survey":false,"survey_abbr":"","model":true,"model_abbr":"SMD","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Diffusion","approach_tags":"Frequency, Attribute"},{"arxiv_id":"2405.05691","title":"StableMoFusion: Towards Robust and Efficient Diffusion-based Motion Generation Framework","abstract":"Thanks to the powerful generative capacity of diffusion models, recent years\\nhave witnessed rapid progress in human motion generation. Existing\\ndiffusion-based methods employ disparate network architectures and training\\nstrategies. The effect of the design of each component is still unclear. In\\naddition, the iterative denoising process consumes considerable computational\\noverhead, which is prohibitive for real-time scenarios such as virtual\\ncharacters and humanoid robots. For this reason, we first conduct a\\ncomprehensive investigation into network architectures, training strategies,\\nand inference processs. Based on the profound analysis, we tailor each\\ncomponent for efficient high-quality human motion generation. Despite the\\npromising performance, the tailored model still suffers from foot skating which\\nis an ubiquitous issue in diffusion-based solutions. To eliminate footskate, we\\nidentify foot-ground contact and correct foot motions along the denoising\\nprocess. By organically combining these well-designed components together, we\\npresent StableMoFusion, a robust and efficient framework for human motion\\ngeneration. Extensive experimental results show that our StableMoFusion\\nperforms favorably against current state-of-the-art methods. Project page:\\nhttps://h-y1heng.github.io/StableMoFusion-page/","authors":["Yiheng Huang","Hui Yang","Chuanchen Luo","Yuxi Wang","Shibiao Xu","Zhaoxiang Zhang","Man Zhang","Junran Peng"],"year":2024,"month":5,"url":"https://arxiv.org/abs/2405.05691","survey":false,"survey_abbr":"","model":true,"model_abbr":"StableMoFusion","dataset":false,"dataset_abbr":"","submission":"MM","submission_year":"2024","page":"https://h-y1heng.github.io/StableMoFusion-page/","repo":"https://github.com/Linketic/StableMoFusion","backbone_tags":"UNet, Diffusion, Transformer","approach_tags":""},{"arxiv_id":"2405.04771","title":"Exploring Vision Transformers for 3D Human Motion-Language Models with Motion Patches","abstract":"To build a cross-modal latent space between 3D human motion and language,\\nacquiring large-scale and high-quality human motion data is crucial. However,\\nunlike the abundance of image data, the scarcity of motion data has limited the\\nperformance of existing motion-language models. To counter this, we introduce\\n\\"motion patches\\", a new representation of motion sequences, and propose using\\nVision Transformers (ViT) as motion encoders via transfer learning, aiming to\\nextract useful knowledge from the image domain and apply it to the motion\\ndomain. These motion patches, created by dividing and sorting skeleton joints\\nbased on body parts in motion sequences, are robust to varying skeleton\\nstructures, and can be regarded as color image patches in ViT. We find that\\ntransfer learning with pre-trained weights of ViT obtained through training\\nwith 2D image data can boost the performance of motion analysis, presenting a\\npromising direction for addressing the issue of limited motion data. Our\\nextensive experiments show that the proposed motion patches, used jointly with\\nViT, achieve state-of-the-art performance in the benchmarks of text-to-motion\\nretrieval, and other novel challenging tasks, such as cross-skeleton\\nrecognition, zero-shot motion classification, and human interaction\\nrecognition, which are currently impeded by the lack of data.","authors":["Qing Yu","Mikihiro Tanaka","Kent Fujiwara"],"year":2024,"month":5,"url":"https://arxiv.org/abs/2405.04771","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionPatches","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2024","page":"https://yu1ut.com/MotionPatches-HP/","repo":"https://github.com/line/MotionPatches","backbone_tags":"Transformer","approach_tags":"Retrieval"},{"arxiv_id":"2405.03803","title":"MoDiPO: text-to-motion alignment via AI-feedback-driven Direct Preference Optimization","abstract":"Diffusion Models have revolutionized the field of human motion generation by\\noffering exceptional generation quality and fine-grained controllability\\nthrough natural language conditioning. Their inherent stochasticity, that is\\nthe ability to generate various outputs from a single input, is key to their\\nsuccess. However, this diversity should not be unrestricted, as it may lead to\\nunlikely generations. Instead, it should be confined within the boundaries of\\ntext-aligned and realistic generations. To address this issue, we propose\\nMoDiPO (Motion Diffusion DPO), a novel methodology that leverages Direct\\nPreference Optimization (DPO) to align text-to-motion models. We streamline the\\nlaborious and expensive process of gathering human preferences needed in DPO by\\nleveraging AI feedback instead. This enables us to experiment with novel DPO\\nstrategies, using both online and offline generated motion-preference pairs. To\\nfoster future research we contribute with a motion-preference dataset which we\\ndub Pick-a-Move. We demonstrate, both qualitatively and quantitatively, that\\nour proposed method yields significantly more realistic motions. In particular,\\nMoDiPO substantially improves Frechet Inception Distance (FID) while retaining\\nthe same RPrecision and Multi-Modality performances.","authors":["Massimiliano Pappa","Luca Collorone","Giovanni Ficarra","Indro Spinelli","Fabio Galasso"],"year":2024,"month":5,"url":"https://arxiv.org/abs/2405.03803","survey":false,"survey_abbr":"","model":true,"model_abbr":"MoDiPO","dataset":true,"dataset_abbr":"PaM","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Diffusion","approach_tags":"Preference"},{"arxiv_id":"2405.03485","title":"LGTM: Local-to-Global Text-Driven Human Motion Diffusion Model","abstract":"In this paper, we introduce LGTM, a novel Local-to-Global pipeline for\\nText-to-Motion generation. LGTM utilizes a diffusion-based architecture and\\naims to address the challenge of accurately translating textual descriptions\\ninto semantically coherent human motion in computer animation. Specifically,\\ntraditional methods often struggle with semantic discrepancies, particularly in\\naligning specific motions to the correct body parts. To address this issue, we\\npropose a two-stage pipeline to overcome this challenge: it first employs large\\nlanguage models (LLMs) to decompose global motion descriptions into\\npart-specific narratives, which are then processed by independent body-part\\nmotion encoders to ensure precise local semantic alignment. Finally, an\\nattention-based full-body optimizer refines the motion generation results and\\nguarantees the overall coherence. Our experiments demonstrate that LGTM gains\\nsignificant improvements in generating locally accurate, semantically-aligned\\nhuman motion, marking a notable advancement in text-to-motion applications.\\nCode and data for this paper are available at https://github.com/L-Sun/LGTM","authors":["Haowen Sun","Ruikun Zheng","Haibin Huang","Chongyang Ma","Hui Huang","Ruizhen Hu"],"year":2024,"month":5,"url":"https://arxiv.org/abs/2405.03485","survey":false,"survey_abbr":"","model":true,"model_abbr":"LGTM","dataset":false,"dataset_abbr":"","submission":"SIGGRAPH","submission_year":"2024","page":"","repo":"https://github.com/L-Sun/LGTM","backbone_tags":"Transformer","approach_tags":"Finegrained"},{"arxiv_id":"2405.02791","title":"Efficient Text-driven Motion Generation via Latent Consistency Training","abstract":"Text-driven human motion generation based on diffusion strategies establishes\\na reliable foundation for multimodal applications in human-computer\\ninteractions. However, existing advances face significant efficiency challenges\\ndue to the substantial computational overhead of iteratively solving for\\nnonlinear reverse diffusion trajectories during the inference phase. To this\\nend, we propose the motion latent consistency training framework (MLCT), which\\nprecomputes reverse diffusion trajectories from raw data in the training phase\\nand enables few-step or single-step inference via self-consistency constraints\\nin the inference phase. Specifically, a motion autoencoder with quantization\\nconstraints is first proposed for constructing concise and bounded solution\\ndistributions for motion diffusion processes. Subsequently, a classifier-free\\nguidance format is constructed via an additional unconditional loss function to\\naccomplish the precomputation of conditional diffusion trajectories in the\\ntraining phase. Finally, a clustering guidance module based on the\\nK-nearest-neighbor algorithm is developed for the chain-conduction optimization\\nmechanism of self-consistency constraints, which provides additional references\\nof solution distributions at a small query cost. By combining these\\nenhancements, we achieve stable and consistency training in non-pixel modality\\nand latent representation spaces. Benchmark experiments demonstrate that our\\nmethod significantly outperforms traditional consistency distillation methods\\nwith reduced training cost and enhances the consistency model to perform\\ncomparably to state-of-the-art models with lower inference costs.","authors":["Mengxian Hu","Minghao Zhu","Xun Zhou","Qingqing Yan","Shu Li","Chengju Liu","Qijun Chen"],"year":2024,"month":5,"url":"https://arxiv.org/abs/2405.02791","survey":false,"survey_abbr":"","model":true,"model_abbr":"MLCT","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Diffusion","approach_tags":"Score-based"},{"arxiv_id":"2405.01461","title":"SATO: Stable Text-to-Motion Framework","abstract":"Is the Text to Motion model robust? Recent advancements in Text to Motion\\nmodels primarily stem from more accurate predictions of specific actions.\\nHowever, the text modality typically relies solely on pre-trained Contrastive\\nLanguage-Image Pretraining (CLIP) models. Our research has uncovered a\\nsignificant issue with the text-to-motion model: its predictions often exhibit\\ninconsistent outputs, resulting in vastly different or even incorrect poses\\nwhen presented with semantically similar or identical text inputs. In this\\npaper, we undertake an analysis to elucidate the underlying causes of this\\ninstability, establishing a clear link between the unpredictability of model\\noutputs and the erratic attention patterns of the text encoder module.\\nConsequently, we introduce a formal framework aimed at addressing this issue,\\nwhich we term the Stable Text-to-Motion Framework (SATO). SATO consists of\\nthree modules, each dedicated to stable attention, stable prediction, and\\nmaintaining a balance between accuracy and robustness trade-off. We present a\\nmethodology for constructing an SATO that satisfies the stability of attention\\nand prediction. To verify the stability of the model, we introduced a new\\ntextual synonym perturbation dataset based on HumanML3D and KIT-ML. Results\\nshow that SATO is significantly more stable against synonyms and other slight\\nperturbations while keeping its high accuracy performance.","authors":["Wenshuo Chen","Hongru Xiao","Erhang Zhang","Lijie Hu","Lei Wang","Mengyuan Liu","Chen Chen"],"year":2024,"month":5,"url":"https://arxiv.org/abs/2405.01461","survey":false,"survey_abbr":"","model":true,"model_abbr":"SATO","dataset":false,"dataset_abbr":"","submission":"MM","submission_year":"2024","page":"https://sato-team.github.io/Stable-Text-to-Motion-Framework/","repo":"https://github.com/sato-team/Stable-Text-to-Motion-Framework","backbone_tags":"CLIP, Transformer","approach_tags":""},{"arxiv_id":"2404.19759","title":"MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model","abstract":"This work introduces MotionLCM, extending controllable motion generation to a\\nreal-time level. Existing methods for spatial-temporal control in\\ntext-conditioned motion generation suffer from significant runtime\\ninefficiency. To address this issue, we first propose the motion latent\\nconsistency model (MotionLCM) for motion generation, building on the motion\\nlatent diffusion model. By adopting one-step (or few-step) inference, we\\nfurther improve the runtime efficiency of the motion latent diffusion model for\\nmotion generation. To ensure effective controllability, we incorporate a motion\\nControlNet within the latent space of MotionLCM and enable explicit control\\nsignals (i.e., initial motions) in the vanilla motion space to further provide\\nsupervision for the training process. By employing these techniques, our\\napproach can generate human motions with text and control signals in real-time.\\nExperimental results demonstrate the remarkable generation and controlling\\ncapabilities of MotionLCM while maintaining real-time runtime efficiency.","authors":["Wenxun Dai","Ling-Hao Chen","Jingbo Wang","Jinpeng Liu","Bo Dai","Yansong Tang"],"year":2024,"month":4,"url":"https://arxiv.org/abs/2404.19759","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionLCM","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2024","page":"https://dai-wenxun.github.io/MotionLCM-page/","repo":"https://github.com/Dai-Wenxun/MotionLCM","backbone_tags":"","approach_tags":"Trajectory, Efficient"},{"arxiv_id":"2404.14745","title":"You Think, You ACT: The New Task of Arbitrary Text to Motion Generation","abstract":"Text to Motion aims to generate human motions from texts. Existing settings\\nrely on limited Action Texts that include action labels, which limits\\nflexibility and practicability in scenarios difficult to describe directly.\\nThis paper extends limited Action Texts to arbitrary ones. Scene texts without\\nexplicit action labels can enhance the practicality of models in complex and\\ndiverse industries such as virtual human interaction, robot behavior\\ngeneration, and film production, while also supporting the exploration of\\npotential implicit behavior patterns. However, newly introduced Scene Texts may\\nyield multiple reasonable output results, causing significant challenges in\\nexisting data, framework, and evaluation. To address this practical issue, we\\nfirst create a new dataset HUMANML3D++ by extending texts of the largest\\nexisting dataset HUMANML3D. Secondly, we propose a simple yet effective\\nframework that extracts action instructions from arbitrary texts and\\nsubsequently generates motions. Furthermore, we also benchmark this new setting\\nwith multi-solution metrics to address the inadequacies of existing\\nsingle-solution metrics. Extensive experiments indicate that Text to Motion in\\nthis realistic setting is challenging, fostering new research in this practical\\ndirection.","authors":["Runqi Wang","Caoyuan Ma","Guopeng Li","Hanrui Xu","Yuke Li","Zheng Wang"],"year":2024,"month":4,"url":"https://arxiv.org/abs/2404.14745","survey":false,"survey_abbr":"","model":true,"model_abbr":"TAAT","dataset":true,"dataset_abbr":"HumanML3D++","submission":"ICCV","submission_year":"2025","page":"","repo":"","backbone_tags":"LLM, Transformer, VQ-VAE","approach_tags":"LoRA"},{"arxiv_id":"2404.12886","title":"MCM: Multi-condition Motion Synthesis Framework","abstract":"Conditional human motion synthesis (HMS) aims to generate human motion\\nsequences that conform to specific conditions. Text and audio represent the two\\npredominant modalities employed as HMS control conditions. While existing\\nresearch has primarily focused on single conditions, the multi-condition human\\nmotion synthesis remains underexplored. In this study, we propose a\\nmulti-condition HMS framework, termed MCM, based on a dual-branch structure\\ncomposed of a main branch and a control branch. This framework effectively\\nextends the applicability of the diffusion model, which is initially predicated\\nsolely on textual conditions, to auditory conditions. This extension\\nencompasses both music-to-dance and co-speech HMS while preserving the\\nintrinsic quality of motion and the capabilities for semantic association\\ninherent in the original model. Furthermore, we propose the implementation of a\\nTransformer-based diffusion model, designated as MWNet, as the main branch.\\nThis model adeptly apprehends the spatial intricacies and inter-joint\\ncorrelations inherent in motion sequences, facilitated by the integration of\\nmulti-wise self-attention modules. Extensive experiments show that our method\\nachieves competitive results in single-condition and multi-condition HMS tasks.","authors":["Zeyu Ling","Bo Han","Yongkang Wongkan","Han Lin","Mohan Kankanhalli","Weidong Geng"],"year":2024,"month":4,"url":"https://arxiv.org/abs/2404.12886","survey":false,"survey_abbr":"","model":true,"model_abbr":"MCM","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Transformer","approach_tags":"Multi-Task"},{"arxiv_id":"2404.09445","title":"Exploring Text-to-Motion Generation with Human Preference","abstract":"This paper presents an exploration of preference learning in text-to-motion\\ngeneration. We find that current improvements in text-to-motion generation\\nstill rely on datasets requiring expert labelers with motion capture systems.\\nInstead, learning from human preference data does not require motion capture\\nsystems; a labeler with no expertise simply compares two generated motions.\\nThis is particularly efficient because evaluating the model\'s output is easier\\nthan gathering the motion that performs a desired task (e.g. backflip). To\\npioneer the exploration of this paradigm, we annotate 3,528 preference pairs\\ngenerated by MotionGPT, marking the first effort to investigate various\\nalgorithms for learning from preference data. In particular, our exploration\\nhighlights important design choices when using preference data. Additionally,\\nour experimental results show that preference learning has the potential to\\ngreatly improve current text-to-motion generative models. Our code and dataset\\nare publicly available at\\nhttps://github.com/THU-LYJ-Lab/InstructMotion}{https://github.com/THU-LYJ-Lab/InstructMotion\\nto further facilitate research in this area.","authors":["Jenny Sheng","Matthieu Lin","Andrew Zhao","Kevin Pruvost","Yu-Hui Wen","Yangguang Li","Gao Huang","Yong-Jin Liu"],"year":2024,"month":4,"url":"https://arxiv.org/abs/2404.09445","survey":false,"survey_abbr":"","model":true,"model_abbr":"InstructMotion","dataset":false,"dataset_abbr":"","submission":"CVPR Workshop","submission_year":"2024","page":"","repo":"https://github.com/THU-LYJ-Lab/InstructMotion","backbone_tags":"","approach_tags":"Preference, RL"},{"arxiv_id":"2404.01700","title":"MotionChain: Conversational Motion Controllers via Multimodal Prompts","abstract":"Recent advancements in language models have demonstrated their adeptness in\\nconducting multi-turn dialogues and retaining conversational context. However,\\nthis proficiency remains largely unexplored in other multimodal generative\\nmodels, particularly in human motion models. By integrating multi-turn\\nconversations in controlling continuous virtual human movements, generative\\nhuman motion models can achieve an intuitive and step-by-step process of human\\ntask execution for humanoid robotics, game agents, or other embodied systems.\\nIn this work, we present MotionChain, a conversational human motion controller\\nto generate continuous and long-term human motion through multimodal prompts.\\nSpecifically, MotionChain consists of multi-modal tokenizers that transform\\nvarious data types such as text, image, and motion, into discrete tokens,\\ncoupled with a Vision-Motion-aware Language model. By leveraging large-scale\\nlanguage, vision-language, and vision-motion data to assist motion-related\\ngeneration tasks, MotionChain thus comprehends each instruction in multi-turn\\nconversation and generates human motions followed by these prompts. Extensive\\nexperiments validate the efficacy of MotionChain, demonstrating\\nstate-of-the-art performance in conversational motion generation, as well as\\nmore intuitive manners of controlling and interacting with virtual humans.","authors":["Biao Jiang","Xin Chen","Chi Zhang","Fukun Yin","Zhuoyuan Li","Gang YU","Jiayuan Fan"],"year":2024,"month":4,"url":"https://arxiv.org/abs/2404.01700","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionChain","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"LLM","approach_tags":""},{"arxiv_id":"2404.01284","title":"Large Motion Model for Unified Multi-Modal Motion Generation","abstract":"Human motion generation, a cornerstone technique in animation and video\\nproduction, has widespread applications in various tasks like text-to-motion\\nand music-to-dance. Previous works focus on developing specialist models\\ntailored for each task without scalability. In this work, we present Large\\nMotion Model (LMM), a motion-centric, multi-modal framework that unifies\\nmainstream motion generation tasks into a generalist model. A unified motion\\nmodel is appealing since it can leverage a wide range of motion data to achieve\\nbroad generalization beyond a single task. However, it is also challenging due\\nto the heterogeneous nature of substantially different motion data and tasks.\\nLMM tackles these challenges from three principled aspects: 1) Data: We\\nconsolidate datasets with different modalities, formats and tasks into a\\ncomprehensive yet unified motion generation dataset, MotionVerse, comprising 10\\ntasks, 16 datasets, a total of 320k sequences, and 100 million frames. 2)\\nArchitecture: We design an articulated attention mechanism ArtAttention that\\nincorporates body part-aware modeling into Diffusion Transformer backbone. 3)\\nPre-Training: We propose a novel pre-training strategy for LMM, which employs\\nvariable frame rates and masking forms, to better exploit knowledge from\\ndiverse training data. Extensive experiments demonstrate that our generalist\\nLMM achieves competitive performance across various standard motion generation\\ntasks over state-of-the-art specialist models. Notably, LMM exhibits strong\\ngeneralization capabilities and emerging properties across many unseen tasks.\\nAdditionally, our ablation studies reveal valuable insights about training and\\nscaling up large motion models for future research.","authors":["Mingyuan Zhang","Daisheng Jin","Chenyang Gu","Fangzhou Hong","Zhongang Cai","Jingfang Huang","Chongzhi Zhang","Xinying Guo","Lei Yang","Ying He","Ziwei Liu"],"year":2024,"month":4,"url":"https://arxiv.org/abs/2404.01284","survey":false,"survey_abbr":"","model":true,"model_abbr":"LMM","dataset":true,"dataset_abbr":"MotionVerse","submission":"","submission_year":"","page":"https://mingyuan-zhang.github.io/projects/LMM.html","repo":"https://github.com/mingyuan-zhang/LMM","backbone_tags":"Diffusion, Transformer","approach_tags":"Multi-Task"},{"arxiv_id":"2403.19435","title":"BAMM: Bidirectional Autoregressive Motion Model","abstract":"Generating human motion from text has been dominated by denoising motion\\nmodels either through diffusion or generative masking process. However, these\\nmodels face great limitations in usability by requiring prior knowledge of the\\nmotion length. Conversely, autoregressive motion models address this limitation\\nby adaptively predicting motion endpoints, at the cost of degraded generation\\nquality and editing capabilities. To address these challenges, we propose\\nBidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion\\ngeneration framework. BAMM consists of two key components: (1) a motion\\ntokenizer that transforms 3D human motion into discrete tokens in latent space,\\nand (2) a masked self-attention transformer that autoregressively predicts\\nrandomly masked tokens via a hybrid attention masking strategy. By unifying\\ngenerative masked modeling and autoregressive modeling, BAMM captures rich and\\nbidirectional dependencies among motion tokens, while learning the\\nprobabilistic mapping from textual inputs to motion outputs with\\ndynamically-adjusted motion sequence length. This feature enables BAMM to\\nsimultaneously achieving high-quality motion generation with enhanced usability\\nand built-in motion editability. Extensive experiments on HumanML3D and KIT-ML\\ndatasets demonstrate that BAMM surpasses current state-of-the-art methods in\\nboth qualitative and quantitative measures. Our project page is available at\\nhttps://exitudio.github.io/BAMM-page","authors":["Ekkasit Pinyoanuntapong","Muhammad Usama Saleem","Pu Wang","Minwoo Lee","Srijan Das","Chen Chen"],"year":2024,"month":3,"url":"https://arxiv.org/abs/2403.19435","survey":false,"survey_abbr":"","model":true,"model_abbr":"BAMM","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2024","page":"https://www.ekkasit.com/BAMM-page/","repo":"https://github.com/exitudio/BAMM/","backbone_tags":"VQ-VAE, CLIP, Transformer","approach_tags":""},{"arxiv_id":"2403.18512","title":"ParCo: Part-Coordinating Text-to-Motion Synthesis","abstract":"We study a challenging task: text-to-motion synthesis, aiming to generate\\nmotions that align with textual descriptions and exhibit coordinated movements.\\nCurrently, the part-based methods introduce part partition into the motion\\nsynthesis process to achieve finer-grained generation. However, these methods\\nencounter challenges such as the lack of coordination between different part\\nmotions and difficulties for networks to understand part concepts. Moreover,\\nintroducing finer-grained part concepts poses computational complexity\\nchallenges. In this paper, we propose Part-Coordinating Text-to-Motion\\nSynthesis (ParCo), endowed with enhanced capabilities for understanding part\\nmotions and communication among different part motion generators, ensuring a\\ncoordinated and fined-grained motion synthesis. Specifically, we discretize\\nwhole-body motion into multiple part motions to establish the prior concept of\\ndifferent parts. Afterward, we employ multiple lightweight generators designed\\nto synthesize different part motions and coordinate them through our part\\ncoordination module. Our approach demonstrates superior performance on common\\nbenchmarks with economic computations, including HumanML3D and KIT-ML,\\nproviding substantial evidence of its effectiveness. Code is available at\\nhttps://github.com/qrzou/ParCo .","authors":["Qiran Zou","Shangyuan Yuan","Shian Du","Yu Wang","Chang Liu","Yi Xu","Jie Chen","Xiangyang Ji"],"year":2024,"month":3,"url":"https://arxiv.org/abs/2403.18512","survey":false,"survey_abbr":"","model":true,"model_abbr":"ParCo","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2024","page":"","repo":"https://github.com/qrzou/ParCo","backbone_tags":"VQ-VAE, Transformer","approach_tags":"Finegrained"},{"arxiv_id":"2403.15709","title":"Contact-aware Human Motion Generation from Textual Descriptions","abstract":"This paper addresses the problem of generating 3D interactive human motion\\nfrom text. Given a textual description depicting the actions of different body\\nparts in contact with static objects, we synthesize sequences of 3D body poses\\nthat are visually natural and physically plausible. Yet, this task poses a\\nsignificant challenge due to the inadequate consideration of interactions by\\nphysical contacts in both motion and textual descriptions, leading to unnatural\\nand implausible sequences. To tackle this challenge, we create a novel dataset\\nnamed RICH-CAT, representing \\"Contact-Aware Texts\\" constructed from the RICH\\ndataset. RICH-CAT comprises high-quality motion, accurate human-object contact\\nlabels, and detailed textual descriptions, encompassing over 8,500 motion-text\\npairs across 26 indoor/outdoor actions. Leveraging RICH-CAT, we propose a novel\\napproach named CATMO for text-driven interactive human motion synthesis that\\nexplicitly integrates human body contacts as evidence. We employ two VQ-VAE\\nmodels to encode motion and body contact sequences into distinct yet\\ncomplementary latent spaces and an intertwined GPT for generating human motions\\nand contacts in a mutually conditioned manner. Additionally, we introduce a\\npre-trained text encoder to learn textual embeddings that better discriminate\\namong various contact types, allowing for more precise control over synthesized\\nmotions and contacts. Our experiments demonstrate the superior performance of\\nour approach compared to existing text-to-motion methods, producing stable,\\ncontact-aware motion sequences. Code and data will be available for research\\npurposes at https://xymsh.github.io/RICH-CAT/","authors":["Sihan Ma","Qiong Cao","Jing Zhang","Dacheng Tao"],"year":2024,"month":3,"url":"https://arxiv.org/abs/2403.15709","survey":false,"survey_abbr":"","model":true,"model_abbr":"CATMO","dataset":true,"dataset_abbr":"RICH-CAT","submission":"","submission_year":"","page":"https://xymsh.github.io/RICH-CAT/","repo":"","backbone_tags":"VQ-VAE, Transformer","approach_tags":""},{"arxiv_id":"2403.13900","title":"CoMo: Controllable Motion Generation through Language Guided Pose Code Editing","abstract":"Text-to-motion models excel at efficient human motion generation, but\\nexisting approaches lack fine-grained controllability over the generation\\nprocess. Consequently, modifying subtle postures within a motion or inserting\\nnew actions at specific moments remains a challenge, limiting the applicability\\nof these methods in diverse scenarios. In light of these challenges, we\\nintroduce CoMo, a Controllable Motion generation model, adept at accurately\\ngenerating and editing motions by leveraging the knowledge priors of large\\nlanguage models (LLMs). Specifically, CoMo decomposes motions into discrete and\\nsemantically meaningful pose codes, with each code encapsulating the semantics\\nof a body part, representing elementary information such as \\"left knee slightly\\nbent\\". Given textual inputs, CoMo autoregressively generates sequences of pose\\ncodes, which are then decoded into 3D motions. Leveraging pose codes as\\ninterpretable representations, an LLM can directly intervene in motion editing\\nby adjusting the pose codes according to editing instructions. Experiments\\ndemonstrate that CoMo achieves competitive performance in motion generation\\ncompared to state-of-the-art models while, in human studies, CoMo substantially\\nsurpasses previous work in motion editing abilities.","authors":["Yiming Huang","Weilin Wan","Yue Yang","Chris Callison-Burch","Mark Yatskar","Lingjie Liu"],"year":2024,"month":3,"url":"https://arxiv.org/abs/2403.13900","survey":false,"survey_abbr":"","model":true,"model_abbr":"CoMo","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2024","page":"https://yh2371.github.io/como/","repo":"https://github.com/yh2371/CoMo","backbone_tags":"LLM, CLIP, Transformer","approach_tags":"Finegrained, Editing"},{"arxiv_id":"2403.13518","title":"Motion Generation from Fine-grained Textual Descriptions","abstract":"The task of text2motion is to generate human motion sequences from given\\ntextual descriptions, where the model explores diverse mappings from natural\\nlanguage instructions to human body movements. While most existing works are\\nconfined to coarse-grained motion descriptions, e.g., \\"A man squats.\\",\\nfine-grained descriptions specifying movements of relevant body parts are\\nbarely explored. Models trained with coarse-grained texts may not be able to\\nlearn mappings from fine-grained motion-related words to motion primitives,\\nresulting in the failure to generate motions from unseen descriptions. In this\\npaper, we build a large-scale language-motion dataset specializing in\\nfine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with\\nstep-by-step instructions with pseudo-code compulsory checks. Accordingly, we\\ndesign a new text2motion model, FineMotionDiffuse, making full use of\\nfine-grained textual information. Our quantitative evaluation shows that\\nFineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of\\n0.38, compared with competitive baselines. According to the qualitative\\nevaluation and case study, our model outperforms MotionDiffuse in generating\\nspatially or chronologically composite motions, by learning the implicit\\nmappings from fine-grained descriptions to the corresponding basic motions. We\\nrelease our data at https://github.com/KunhangL/finemotiondiffuse.","authors":["Kunhang Li","Yansong Feng"],"year":2024,"month":3,"url":"https://arxiv.org/abs/2403.13518","survey":false,"survey_abbr":"","model":false,"model_abbr":"","dataset":true,"dataset_abbr":"FineHumanML3D","submission":"LREC-COLING","submission_year":"2024","page":"","repo":"https://github.com/KunhangL/finemotiondiffuse","backbone_tags":"","approach_tags":""},{"arxiv_id":"2403.12835","title":"AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents","abstract":"Traditional approaches in physics-based motion generation, centered around\\nimitation learning and reward shaping, often struggle to adapt to new\\nscenarios. To tackle this limitation, we propose AnySkill, a novel hierarchical\\nmethod that learns physically plausible interactions following open-vocabulary\\ninstructions. Our approach begins by developing a set of atomic actions via a\\nlow-level controller trained via imitation learning. Upon receiving an\\nopen-vocabulary textual instruction, AnySkill employs a high-level policy that\\nselects and integrates these atomic actions to maximize the CLIP similarity\\nbetween the agent\'s rendered images and the text. An important feature of our\\nmethod is the use of image-based rewards for the high-level policy, which\\nallows the agent to learn interactions with objects without manual reward\\nengineering. We demonstrate AnySkill\'s capability to generate realistic and\\nnatural motion sequences in response to unseen instructions of varying lengths,\\nmarking it the first method capable of open-vocabulary physical skill learning\\nfor interactive humanoid agents.","authors":["Jieming Cui","Tengyu Liu","Nian Liu","Yaodong Yang","Yixin Zhu","Siyuan Huang"],"year":2024,"month":3,"url":"https://arxiv.org/abs/2403.12835","survey":false,"survey_abbr":"","model":true,"model_abbr":"AnySkill","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2024","page":"https://anyskill.github.io/","repo":"https://github.com/jiemingcui/anyskill","backbone_tags":"CLIP","approach_tags":"Physical, Open-Vocabulary, Imitation Learning, Reward"},{"arxiv_id":"2403.07487","title":"Motion Mamba: Efficient and Long Sequence Motion Generation","abstract":"Human motion generation stands as a significant pursuit in generative\\ncomputer vision, while achieving long-sequence and efficient motion generation\\nremains challenging. Recent advancements in state space models (SSMs), notably\\nMamba, have showcased considerable promise in long sequence modeling with an\\nefficient hardware-aware design, which appears to be a promising direction to\\nbuild motion generation model upon it. Nevertheless, adapting SSMs to motion\\ngeneration faces hurdles since the lack of a specialized design architecture to\\nmodel motion sequence. To address these challenges, we propose Motion Mamba, a\\nsimple and efficient approach that presents the pioneering motion generation\\nmodel utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba\\n(HTM) block to process temporal data by ensemble varying numbers of isolated\\nSSM modules across a symmetric U-Net architecture aimed at preserving motion\\nconsistency between frames. We also design a Bidirectional Spatial Mamba (BSM)\\nblock to bidirectionally process latent poses, to enhance accurate motion\\ngeneration within a temporal frame. Our proposed method achieves up to 50% FID\\nimprovement and up to 4 times faster on the HumanML3D and KIT-ML datasets\\ncompared to the previous best diffusion-based method, which demonstrates strong\\ncapabilities of high-quality long sequence motion modeling and real-time human\\nmotion generation. See project website\\nhttps://steve-zeyu-zhang.github.io/MotionMamba/","authors":["Zeyu Zhang","Akide Liu","Ian Reid","Richard Hartley","Bohan Zhuang","Hao Tang"],"year":2024,"month":3,"url":"https://arxiv.org/abs/2403.07487","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionMamba","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2024","page":"https://steve-zeyu-zhang.github.io/MotionMamba/","repo":"","backbone_tags":"Mamba","approach_tags":""},{"arxiv_id":"2403.02905","title":"MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model","abstract":"The body movements accompanying speech aid speakers in expressing their\\nideas. Co-speech motion generation is one of the important approaches for\\nsynthesizing realistic avatars. Due to the intricate correspondence between\\nspeech and motion, generating realistic and diverse motion is a challenging\\ntask. In this paper, we propose MMoFusion, a Multi-modal co-speech Motion\\ngeneration framework based on the diffusion model to ensure both the\\nauthenticity and diversity of generated motion. We propose a progressive fusion\\nstrategy to enhance the interaction of inter-modal and intra-modal, efficiently\\nintegrating multi-modal information. Specifically, we employ a masked style\\nmatrix based on emotion and identity information to control the generation of\\ndifferent motion styles. Temporal modeling of speech and motion is partitioned\\ninto style-guided specific feature encoding and shared feature encoding, aiming\\nto learn both inter-modal and intra-modal features. Besides, we propose a\\ngeometric loss to enforce the joints\' velocity and acceleration coherence among\\nframes. Our framework generates vivid, diverse, and style-controllable motion\\nof arbitrary length through inputting speech and editing identity and emotion.\\nExtensive experiments demonstrate that our method outperforms current co-speech\\nmotion generation methods including upper body and challenging full body.","authors":["Sen Wang","Jiangning Zhang","Xin Tan","Zhifeng Xie","Chengjie Wang","Lizhuang Ma"],"year":2024,"month":3,"url":"https://arxiv.org/abs/2403.02905","survey":false,"survey_abbr":"","model":true,"model_abbr":"MMoFusion","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://mmofusion.github.io/","repo":"https://github.com/wangsen99/MMoFusion","backbone_tags":"Transformer","approach_tags":"Multi-Task"},{"arxiv_id":"2402.15509","title":"Seamless Human Motion Composition with Blended Positional Encodings","abstract":"Conditional human motion generation is an important topic with many\\napplications in virtual reality, gaming, and robotics. While prior works have\\nfocused on generating motion guided by text, music, or scenes, these typically\\nresult in isolated motions confined to short durations. Instead, we address the\\ngeneration of long, continuous sequences guided by a series of varying textual\\ndescriptions. In this context, we introduce FlowMDM, the first diffusion-based\\nmodel that generates seamless Human Motion Compositions (HMC) without any\\npostprocessing or redundant denoising steps. For this, we introduce the Blended\\nPositional Encodings, a technique that leverages both absolute and relative\\npositional encodings in the denoising chain. More specifically, global motion\\ncoherence is recovered at the absolute stage, whereas smooth and realistic\\ntransitions are built at the relative stage. As a result, we achieve\\nstate-of-the-art results in terms of accuracy, realism, and smoothness on the\\nBabel and HumanML3D datasets. FlowMDM excels when trained with only a single\\ndescription per motion sequence thanks to its Pose-Centric Cross-ATtention,\\nwhich makes it robust against varying text descriptions at inference time.\\nFinally, to address the limitations of existing HMC metrics, we propose two new\\nmetrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt\\ntransitions.","authors":["German Barquero","Sergio Escalera","Cristina Palmero"],"year":2024,"month":2,"url":"https://arxiv.org/abs/2402.15509","survey":false,"survey_abbr":"","model":true,"model_abbr":"FlowMDM","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2024","page":"https://barquerogerman.github.io/FlowMDM/","repo":"https://github.com/BarqueroGerman/FlowMDM","backbone_tags":"Transformer","approach_tags":"Jerk"},{"arxiv_id":"2401.11115","title":"MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation","abstract":"Controllable generation of 3D human motions becomes an important topic as the\\nworld embraces digital transformation. Existing works, though making promising\\nprogress with the advent of diffusion models, heavily rely on meticulously\\ncaptured and annotated (e.g., text) high-quality motion corpus, a\\nresource-intensive endeavor in the real world. This motivates our proposed\\nMotionMix, a simple yet effective weakly-supervised diffusion model that\\nleverages both noisy and unannotated motion sequences. Specifically, we\\nseparate the denoising objectives of a diffusion model into two stages:\\nobtaining conditional rough motion approximations in the initial $T-T^*$ steps\\nby learning the noisy annotated motions, followed by the unconditional\\nrefinement of these preliminary motions during the last $T^*$ steps using\\nunannotated motions. Notably, though learning from two sources of imperfect\\ndata, our model does not compromise motion generation quality compared to fully\\nsupervised approaches that access gold data. Extensive experiments on several\\nbenchmarks demonstrate that our MotionMix, as a versatile framework,\\nconsistently achieves state-of-the-art performances on text-to-motion,\\naction-to-motion, and music-to-dance tasks. Project page:\\nhttps://nhathoang2002.github.io/MotionMix-page/","authors":["Nhat M. Hoang","Kehong Gong","Chuan Guo","Michael Bi Mi"],"year":2024,"month":1,"url":"https://arxiv.org/abs/2401.11115","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionMix","dataset":false,"dataset_abbr":"","submission":"AAAI","submission_year":"2024","page":"https://nhathoang2002.github.io/MotionMix-page/","repo":"https://github.com/NhatHoang2002/MotionMix","backbone_tags":"Diffusion","approach_tags":""},{"arxiv_id":"2401.08559","title":"Multi-Track Timeline Control for Text-Driven 3D Human Motion Generation","abstract":"Recent advances in generative modeling have led to promising progress on\\nsynthesizing 3D human motion from text, with methods that can generate\\ncharacter animations from short prompts and specified durations. However, using\\na single text prompt as input lacks the fine-grained control needed by\\nanimators, such as composing multiple actions and defining precise durations\\nfor parts of the motion. To address this, we introduce the new problem of\\ntimeline control for text-driven motion synthesis, which provides an intuitive,\\nyet fine-grained, input interface for users. Instead of a single prompt, users\\ncan specify a multi-track timeline of multiple prompts organized in temporal\\nintervals that may overlap. This enables specifying the exact timings of each\\naction and composing multiple actions in sequence or at overlapping intervals.\\nTo generate composite animations from a multi-track timeline, we propose a new\\ntest-time denoising method. This method can be integrated with any pre-trained\\nmotion diffusion model to synthesize realistic motions that accurately reflect\\nthe timeline. At every step of denoising, our method processes each timeline\\ninterval (text prompt) individually, subsequently aggregating the predictions\\nwith consideration for the specific body parts engaged in each action.\\nExperimental comparisons and ablations validate that our method produces\\nrealistic motions that respect the semantics and timing of given text prompts.\\nOur code and models are publicly available at https://mathis.petrovich.fr/stmc.","authors":["Mathis Petrovich","Or Litany","Umar Iqbal","Michael J. Black","G\xfcl Varol","Xue Bin Peng","Davis Rempe"],"year":2024,"month":1,"url":"https://arxiv.org/abs/2401.08559","survey":false,"survey_abbr":"","model":true,"model_abbr":"STMC","dataset":false,"dataset_abbr":"","submission":"CVPR Workshop","submission_year":"2024","page":"https://mathis.petrovich.fr/stmc/","repo":"https://github.com/nv-tlabs/stmc","backbone_tags":"Diffusion","approach_tags":"Finegrained"},{"arxiv_id":"2401.02142","title":"GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion Generation","abstract":"In this paper, we propose a novel cascaded diffusion-based generative\\nframework for text-driven human motion synthesis, which exploits a strategy\\nnamed GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy\\nsets up generation objectives by grouping body joints of detailed skeletons in\\nclose semantic proximity together and then replacing each of such joint group\\nwith a single body-part node. Such an operation recursively abstracts a human\\npose to coarser and coarser skeletons at multiple granularity levels. With\\ngradually increasing the abstraction level, human motion becomes more and more\\nconcise and stable, significantly benefiting the cross-modal motion synthesis\\ntask. The whole text-driven human motion synthesis problem is then divided into\\nmultiple abstraction levels and solved with a multi-stage generation framework\\nwith a cascaded latent diffusion model: an initial generator first generates\\nthe coarsest human motion guess from a given text description; then, a series\\nof successive generators gradually enrich the motion details based on the\\ntextual description and the previous synthesized results. Notably, we further\\nintegrate GUESS with the proposed dynamic multi-condition fusion mechanism to\\ndynamically balance the cooperative effects of the given textual condition and\\nsynthesized coarse motion prompt in different generation stages. Extensive\\nexperiments on large-scale datasets verify that GUESS outperforms existing\\nstate-of-the-art methods by large margins in terms of accuracy, realisticness,\\nand diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.","authors":["Xuehao Gao","Yang Yang","Zhenyu Xie","Shaoyi Du","Zhongqian Sun","Yang Wu"],"year":2024,"month":1,"url":"https://arxiv.org/abs/2401.02142","survey":false,"survey_abbr":"","model":true,"model_abbr":"GUESS","dataset":false,"dataset_abbr":"","submission":"TVCG","submission_year":"2024","page":"","repo":"https://github.com/Xuehao-Gao/GUESS","backbone_tags":"CLIP, Transformer, Diffusion","approach_tags":"Diversity"},{"arxiv_id":"2312.17135","title":"InsActor: Instruction-driven Physics-based Characters","abstract":"Generating animation of physics-based characters with intuitive control has\\nlong been a desirable task with numerous applications. However, generating\\nphysically simulated animations that reflect high-level human instructions\\nremains a difficult problem due to the complexity of physical environments and\\nthe richness of human language. In this paper, we present InsActor, a\\nprincipled generative framework that leverages recent advancements in\\ndiffusion-based human motion models to produce instruction-driven animations of\\nphysics-based characters. Our framework empowers InsActor to capture complex\\nrelationships between high-level human instructions and character motions by\\nemploying diffusion policies for flexibly conditioned motion planning. To\\novercome invalid states and infeasible state transitions in planned motions,\\nInsActor discovers low-level skills and maps plans to latent skill sequences in\\na compact latent space. Extensive experiments demonstrate that InsActor\\nachieves state-of-the-art results on various tasks, including\\ninstruction-driven motion generation and instruction-driven waypoint heading.\\nNotably, the ability of InsActor to generate physically simulated animations\\nusing high-level human instructions makes it a valuable tool, particularly in\\nexecuting long-horizon tasks with a rich set of instructions.","authors":["Jiawei Ren","Mingyuan Zhang","Cunjun Yu","Xiao Ma","Liang Pan","Ziwei Liu"],"year":2023,"month":12,"url":"https://arxiv.org/abs/2312.17135","survey":false,"survey_abbr":"","model":true,"model_abbr":"InsActor","dataset":false,"dataset_abbr":"","submission":"NeurIPS","submission_year":"2023","page":"https://jiawei-ren.github.io/projects/insactor/","repo":"https://github.com/jiawei-ren/insactor","backbone_tags":"Diffusion","approach_tags":"Trajectory, Physical"},{"arxiv_id":"2312.15004","title":"FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing","abstract":"Text-driven motion generation has achieved substantial progress with the\\nemergence of diffusion models. However, existing methods still struggle to\\ngenerate complex motion sequences that correspond to fine-grained descriptions,\\ndepicting detailed and accurate spatio-temporal actions. This lack of fine\\ncontrollability limits the usage of motion generation to a larger audience. To\\ntackle these challenges, we present FineMoGen, a diffusion-based motion\\ngeneration and editing framework that can synthesize fine-grained motions, with\\nspatial-temporal composition to the user instructions. Specifically, FineMoGen\\nbuilds upon diffusion model with a novel transformer architecture dubbed\\nSpatio-Temporal Mixture Attention (SAMI). SAMI optimizes the generation of the\\nglobal attention template from two perspectives: 1) explicitly modeling the\\nconstraints of spatio-temporal composition; and 2) utilizing sparsely-activated\\nmixture-of-experts to adaptively extract fine-grained features. To facilitate a\\nlarge-scale study on this new fine-grained motion generation task, we\\ncontribute the HuMMan-MoGen dataset, which consists of 2,968 videos and 102,336\\nfine-grained spatio-temporal descriptions. Extensive experiments validate that\\nFineMoGen exhibits superior motion generation quality over state-of-the-art\\nmethods. Notably, FineMoGen further enables zero-shot motion editing\\ncapabilities with the aid of modern large language models (LLM), which\\nfaithfully manipulates motion sequences with fine-grained instructions. Project\\nPage: https://mingyuan-zhang.github.io/projects/FineMoGen.html","authors":["Mingyuan Zhang","Huirong Li","Zhongang Cai","Jiawei Ren","Lei Yang","Ziwei Liu"],"year":2023,"month":12,"url":"https://arxiv.org/abs/2312.15004","survey":false,"survey_abbr":"","model":true,"model_abbr":"FineMoGen","dataset":true,"dataset_abbr":"HuMMan-MoGen","submission":"NeurIPS","submission_year":"2023","page":"https://mingyuan-zhang.github.io/projects/FineMoGen.html","repo":"https://github.com/mingyuan-zhang/FineMoGen","backbone_tags":"LLM, Diffusion, Transformer","approach_tags":"Finegrained"},{"arxiv_id":"2312.14828","title":"Plan, Posture and Go: Towards Open-World Text-to-Motion Generation","abstract":"Conventional text-to-motion generation methods are usually trained on limited\\ntext-motion pairs, making them hard to generalize to open-world scenarios. Some\\nworks use the CLIP model to align the motion space and the text space, aiming\\nto enable motion generation from natural language motion descriptions. However,\\nthey are still constrained to generate limited and unrealistic in-place\\nmotions. To address these issues, we present a divide-and-conquer framework\\nnamed PRO-Motion, which consists of three modules as motion planner,\\nposture-diffuser and go-diffuser. The motion planner instructs Large Language\\nModels (LLMs) to generate a sequence of scripts describing the key postures in\\nthe target motion. Differing from natural languages, the scripts can describe\\nall possible postures following very simple text templates. This significantly\\nreduces the complexity of posture-diffuser, which transforms a script to a\\nposture, paving the way for open-world generation. Finally, go-diffuser,\\nimplemented as another diffusion model, estimates whole-body translations and\\nrotations for all postures, resulting in realistic motions. Experimental\\nresults have shown the superiority of our method with other counterparts, and\\ndemonstrated its capability of generating diverse and realistic motions from\\ncomplex open-world prompts such as \\"Experiencing a profound sense of joy\\". The\\nproject page is available at https://moonsliu.github.io/Pro-Motion.","authors":["Jinpeng Liu","Wenxun Dai","Chunyu Wang","Yiji Cheng","Yansong Tang","Xin Tong"],"year":2023,"month":12,"url":"https://arxiv.org/abs/2312.14828","survey":false,"survey_abbr":"","model":true,"model_abbr":"PRO-Motion","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2024","page":"https://moonsliu.github.io/Pro-Motion/","repo":"","backbone_tags":"CLIP, LLM, Diffusion","approach_tags":""},{"arxiv_id":"2312.11538","title":"Iterative Motion Editing with Natural Language","abstract":"Text-to-motion diffusion models can generate realistic animations from text\\nprompts, but do not support fine-grained motion editing controls. In this\\npaper, we present a method for using natural language to iteratively specify\\nlocal edits to existing character animations, a task that is common in most\\ncomputer animation workflows. Our key idea is to represent a space of motion\\nedits using a set of kinematic motion editing operators (MEOs) whose effects on\\nthe source motion is well-aligned with user expectations. We provide an\\nalgorithm that leverages pre-existing language models to translate textual\\ndescriptions of motion edits into source code for programs that define and\\nexecute sequences of MEOs on a source animation. We execute MEOs by first\\ntranslating them into keyframe constraints, and then use diffusion-based motion\\nmodels to generate output motions that respect these constraints. Through a\\nuser study and quantitative evaluation, we demonstrate that our system can\\nperform motion edits that respect the animator\'s editing intent, remain\\nfaithful to the original animation (it edits the original animation, but does\\nnot dramatically change it), and yield realistic character animation results.","authors":["Purvi Goel","Kuan-Chieh Wang","C. Karen Liu","Kayvon Fatahalian"],"year":2023,"month":12,"url":"https://arxiv.org/abs/2312.11538","survey":false,"survey_abbr":"","model":true,"model_abbr":"IterativeEditing","dataset":false,"dataset_abbr":"","submission":"SIGGRAPH","submission_year":"2024","page":"https://purvigoel.github.io/iterative-motion-editing/","repo":"https://github.com/purvigoel/iterative-editing-release","backbone_tags":"Transformer, Diffusion, LLM","approach_tags":"Editing"},{"arxiv_id":"2312.10993","title":"Realistic Human Motion Generation with Cross-Diffusion Models","abstract":"We introduce the Cross Human Motion Diffusion Model (CrossDiff), a novel\\napproach for generating high-quality human motion based on textual\\ndescriptions. Our method integrates 3D and 2D information using a shared\\ntransformer network within the training of the diffusion model, unifying motion\\nnoise into a single feature space. This enables cross-decoding of features into\\nboth 3D and 2D motion representations, regardless of their original dimension.\\nThe primary advantage of CrossDiff is its cross-diffusion mechanism, which\\nallows the model to reverse either 2D or 3D noise into clean motion during\\ntraining. This capability leverages the complementary information in both\\nmotion representations, capturing intricate human movement details often missed\\nby models relying solely on 3D information. Consequently, CrossDiff effectively\\ncombines the strengths of both representations to generate more realistic\\nmotion sequences. In our experiments, our model demonstrates competitive\\nstate-of-the-art performance on text-to-motion benchmarks. Moreover, our method\\nconsistently provides enhanced motion generation quality, capturing complex\\nfull-body movement intricacies. Additionally, with a pretrained model,our\\napproach accommodates using in the wild 2D motion data without 3D motion ground\\ntruth during training to generate 3D motion, highlighting its potential for\\nbroader applications and efficient use of available data resources. Project\\npage: https://wonderno.github.io/CrossDiff-webpage/.","authors":["Zeping Ren","Shaoli Huang","Xiu Li"],"year":2023,"month":12,"url":"https://arxiv.org/abs/2312.10993","survey":false,"survey_abbr":"","model":true,"model_abbr":"CrossDiff","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2024","page":"https://wonderno.github.io/CrossDiff-webpage/","repo":"https://github.com/wonderNo/crossdiff","backbone_tags":"Diffusion","approach_tags":""},{"arxiv_id":"2312.10960","title":"Towards Detailed Text-to-Motion Synthesis via Basic-to-Advanced Hierarchical Diffusion Model","abstract":"Text-guided motion synthesis aims to generate 3D human motion that not only\\nprecisely reflects the textual description but reveals the motion details as\\nmuch as possible. Pioneering methods explore the diffusion model for\\ntext-to-motion synthesis and obtain significant superiority. However, these\\nmethods conduct diffusion processes either on the raw data distribution or the\\nlow-dimensional latent space, which typically suffer from the problem of\\nmodality inconsistency or detail-scarce. To tackle this problem, we propose a\\nnovel Basic-to-Advanced Hierarchical Diffusion Model, named B2A-HDM, to\\ncollaboratively exploit low-dimensional and high-dimensional diffusion models\\nfor high quality detailed motion synthesis. Specifically, the basic diffusion\\nmodel in low-dimensional latent space provides the intermediate denoising\\nresult that to be consistent with the textual description, while the advanced\\ndiffusion model in high-dimensional latent space focuses on the following\\ndetail-enhancing denoising process. Besides, we introduce a multi-denoiser\\nframework for the advanced diffusion model to ease the learning of\\nhigh-dimensional model and fully explore the generative potential of the\\ndiffusion model. Quantitative and qualitative experiment results on two\\ntext-to-motion benchmarks (HumanML3D and KIT-ML) demonstrate that B2A-HDM can\\noutperform existing state-of-the-art methods in terms of fidelity, modality\\nconsistency, and diversity.","authors":["Zhenyu Xie","Yang Wu","Xuehao Gao","Zhongqian Sun","Wei Yang","Xiaodan Liang"],"year":2023,"month":12,"url":"https://arxiv.org/abs/2312.10960","survey":false,"survey_abbr":"","model":true,"model_abbr":"B2A-HDM","dataset":false,"dataset_abbr":"","submission":"AAAI","submission_year":"2024","page":"","repo":"","backbone_tags":"CLIP, Diffusion","approach_tags":""},{"arxiv_id":"2312.08985","title":"OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers","abstract":"We have recently seen tremendous progress in realistic text-to-motion\\ngeneration. Yet, the existing methods often fail or produce implausible motions\\nwith unseen text inputs, which limits the applications. In this paper, we\\npresent OMG, a novel framework, which enables compelling motion generation from\\nzero-shot open-vocabulary text prompts. Our key idea is to carefully tailor the\\npretrain-then-finetune paradigm into the text-to-motion generation. At the\\npre-training stage, our model improves the generation ability by learning the\\nrich out-of-domain inherent motion traits. To this end, we scale up a large\\nunconditional diffusion model up to 1B parameters, so as to utilize the massive\\nunlabeled motion data up to over 20M motion instances. At the subsequent\\nfine-tuning stage, we introduce motion ControlNet, which incorporates text\\nprompts as conditioning information, through a trainable copy of the\\npre-trained model and the proposed novel Mixture-of-Controllers (MoC) block.\\nMoC block adaptively recognizes various ranges of the sub-motions with a\\ncross-attention mechanism and processes them separately with the\\ntext-token-specific experts. Such a design effectively aligns the CLIP token\\nembeddings of text prompts to various ranges of compact and expressive motion\\nfeatures. Extensive experiments demonstrate that our OMG achieves significant\\nimprovements over the state-of-the-art methods on zero-shot text-to-motion\\ngeneration. Project page: https://tr3e.github.io/omg-page.","authors":["Han Liang","Jiacheng Bao","Ruichi Zhang","Sihan Ren","Yuecheng Xu","Sibei Yang","Xin Chen","Jingyi Yu","Lan Xu"],"year":2023,"month":12,"url":"https://arxiv.org/abs/2312.08985","survey":false,"survey_abbr":"","model":true,"model_abbr":"OMG","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2024","page":"https://tr3e.github.io/omg-page/","repo":"","backbone_tags":"Diffusion","approach_tags":"Pre-Train, Open-Vocabulary"},{"arxiv_id":"2312.03596","title":"MMM: Generative Masked Motion Model","abstract":"Recent advances in text-to-motion generation using diffusion and\\nautoregressive models have shown promising results. However, these models often\\nsuffer from a trade-off between real-time performance, high fidelity, and\\nmotion editability. To address this gap, we introduce MMM, a novel yet simple\\nmotion generation paradigm based on Masked Motion Model. MMM consists of two\\nkey components: (1) a motion tokenizer that transforms 3D human motion into a\\nsequence of discrete tokens in latent space, and (2) a conditional masked\\nmotion transformer that learns to predict randomly masked motion tokens,\\nconditioned on the pre-computed text tokens. By attending to motion and text\\ntokens in all directions, MMM explicitly captures inherent dependency among\\nmotion tokens and semantic mapping between motion and text tokens. During\\ninference, this allows parallel and iterative decoding of multiple motion\\ntokens that are highly consistent with fine-grained text descriptions,\\ntherefore simultaneously achieving high-fidelity and high-speed motion\\ngeneration. In addition, MMM has innate motion editability. By simply placing\\nmask tokens in the place that needs editing, MMM automatically fills the gaps\\nwhile guaranteeing smooth transitions between editing and non-editing parts.\\nExtensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM\\nsurpasses current leading methods in generating high-quality motion (evidenced\\nby superior FID scores of 0.08 and 0.429), while offering advanced editing\\nfeatures such as body-part modification, motion in-betweening, and the\\nsynthesis of long motion sequences. In addition, MMM is two orders of magnitude\\nfaster on a single mid-range GPU than editable motion diffusion models. Our\\nproject page is available at \\\\url{https://exitudio.github.io/MMM-page}.","authors":["Ekkasit Pinyoanuntapong","Pu Wang","Minwoo Lee","Chen Chen"],"year":2023,"month":12,"url":"https://arxiv.org/abs/2312.03596","survey":false,"survey_abbr":"","model":true,"model_abbr":"MMM","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2024","page":"https://www.ekkasit.com/MMM-page/","repo":"https://github.com/exitudio/MMM/","backbone_tags":"Transformer, Diffusion","approach_tags":"Editing"},{"arxiv_id":"2312.02256","title":"EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation","abstract":"We introduce Efficient Motion Diffusion Model (EMDM) for fast and\\nhigh-quality human motion generation. Current state-of-the-art generative\\ndiffusion models have produced impressive results but struggle to achieve fast\\ngeneration without sacrificing quality. On the one hand, previous works, like\\nmotion latent diffusion, conduct diffusion within a latent space for\\nefficiency, but learning such a latent space can be a non-trivial effort. On\\nthe other hand, accelerating generation by naively increasing the sampling step\\nsize, e.g., DDIM, often leads to quality degradation as it fails to approximate\\nthe complex denoising distribution. To address these issues, we propose EMDM,\\nwhich captures the complex distribution during multiple sampling steps in the\\ndiffusion model, allowing for much fewer sampling steps and significant\\nacceleration in generation. This is achieved by a conditional denoising\\ndiffusion GAN to capture multimodal data distributions among arbitrary (and\\npotentially larger) step sizes conditioned on control signals, enabling\\nfewer-step motion sampling with high fidelity and diversity. To minimize\\nundesired motion artifacts, geometric losses are imposed during network\\nlearning. As a result, EMDM achieves real-time motion generation and\\nsignificantly improves the efficiency of motion diffusion models compared to\\nexisting methods while achieving high-quality motion generation. Our code will\\nbe publicly available upon publication.","authors":["Wenyang Zhou","Zhiyang Dou","Zeyu Cao","Zhouyingcheng Liao","Jingbo Wang","Wenjia Wang","Yuan Liu","Taku Komura","Wenping Wang","Lingjie Liu"],"year":2023,"month":12,"url":"https://arxiv.org/abs/2312.02256","survey":false,"survey_abbr":"","model":true,"model_abbr":"EMDM","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2024","page":"https://frank-zy-dou.github.io/projects/EMDM/index.html","repo":"https://github.com/Frank-ZY-Dou/EMDM","backbone_tags":"Diffusion, GAN","approach_tags":"Efficient"},{"arxiv_id":"2312.00063","title":"MoMask: Generative Masked Modeling of 3D Human Motions","abstract":"We introduce MoMask, a novel masked modeling framework for text-driven 3D\\nhuman motion generation. In MoMask, a hierarchical quantization scheme is\\nemployed to represent human motion as multi-layer discrete motion tokens with\\nhigh-fidelity details. Starting at the base layer, with a sequence of motion\\ntokens obtained by vector quantization, the residual tokens of increasing\\norders are derived and stored at the subsequent layers of the hierarchy. This\\nis consequently followed by two distinct bidirectional transformers. For the\\nbase-layer motion tokens, a Masked Transformer is designated to predict\\nrandomly masked motion tokens conditioned on text input at training stage.\\nDuring generation (i.e. inference) stage, starting from an empty sequence, our\\nMasked Transformer iteratively fills up the missing tokens; Subsequently, a\\nResidual Transformer learns to progressively predict the next-layer tokens\\nbased on the results from current layer. Extensive experiments demonstrate that\\nMoMask outperforms the state-of-art methods on the text-to-motion generation\\ntask, with an FID of 0.045 (vs e.g. 0.141 of T2M-GPT) on the HumanML3D dataset,\\nand 0.228 (vs 0.514) on KIT-ML, respectively. MoMask can also be seamlessly\\napplied in related tasks without further model fine-tuning, such as text-guided\\ntemporal inpainting.","authors":["Chuan Guo","Yuxuan Mu","Muhammad Gohar Javed","Sen Wang","Li Cheng"],"year":2023,"month":11,"url":"https://arxiv.org/abs/2312.00063","survey":false,"survey_abbr":"","model":true,"model_abbr":"MoMask","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2024","page":"https://ericguo5513.github.io/momask/","repo":"https://github.com/EricGuo5513/momask-codes","backbone_tags":"VQ-VAE, Transformer","approach_tags":""},{"arxiv_id":"2311.17135","title":"TLControl: Trajectory and Language Control for Human Motion Synthesis","abstract":"Controllable human motion synthesis is essential for applications in AR/VR,\\ngaming and embodied AI. Existing methods often focus solely on either language\\nor full trajectory control, lacking precision in synthesizing motions aligned\\nwith user-specified trajectories, especially for multi-joint control. To\\naddress these issues, we present TLControl, a novel method for realistic human\\nmotion synthesis, incorporating both low-level Trajectory and high-level\\nLanguage semantics controls, through the integration of neural-based and\\noptimization-based techniques. Specifically, we begin with training a VQ-VAE\\nfor a compact and well-structured latent motion space organized by body parts.\\nWe then propose a Masked Trajectories Transformer (MTT) for predicting a motion\\ndistribution conditioned on language and trajectory. Once trained, we use MTT\\nto sample initial motion predictions given user-specified partial trajectories\\nand text descriptions as conditioning. Finally, we introduce a test-time\\noptimization to refine these coarse predictions for precise trajectory control,\\nwhich offers flexibility by allowing users to specify various optimization\\ngoals and ensures high runtime efficiency. Comprehensive experiments show that\\nTLControl significantly outperforms the state-of-the-art in trajectory accuracy\\nand time efficiency, making it practical for interactive and high-quality\\nanimation generation.","authors":["Weilin Wan","Zhiyang Dou","Taku Komura","Wenping Wang","Dinesh Jayaraman","Lingjie Liu"],"year":2023,"month":11,"url":"https://arxiv.org/abs/2311.17135","survey":false,"survey_abbr":"","model":true,"model_abbr":"TLControl","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"https://github.com/HiWilliamWWL/TLControl","backbone_tags":"VQ-VAE, Transformer","approach_tags":"Trajectory"},{"arxiv_id":"2311.16471","title":"A Unified Framework for Multimodal, Multi-Part Human Motion Synthesis","abstract":"The field has made significant progress in synthesizing realistic human\\nmotion driven by various modalities. Yet, the need for different methods to\\nanimate various body parts according to different control signals limits the\\nscalability of these techniques in practical scenarios. In this paper, we\\nintroduce a cohesive and scalable approach that consolidates multimodal (text,\\nmusic, speech) and multi-part (hand, torso) human motion generation. Our\\nmethodology unfolds in several steps: We begin by quantizing the motions of\\ndiverse body parts into separate codebooks tailored to their respective\\ndomains. Next, we harness the robust capabilities of pre-trained models to\\ntranscode multimodal signals into a shared latent space. We then translate\\nthese signals into discrete motion tokens by iteratively predicting subsequent\\ntokens to form a complete sequence. Finally, we reconstruct the continuous\\nactual motion from this tokenized sequence. Our method frames the multimodal\\nmotion generation challenge as a token prediction task, drawing from\\nspecialized codebooks based on the modality of the control signal. This\\napproach is inherently scalable, allowing for the easy integration of new\\nmodalities. Extensive experiments demonstrated the effectiveness of our design,\\nemphasizing its potential for broad application.","authors":["Zixiang Zhou","Yu Wan","Baoyuan Wang"],"year":2023,"month":11,"url":"https://arxiv.org/abs/2311.16471","survey":false,"survey_abbr":"","model":true,"model_abbr":"UDE-2","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"https://zixiangzhou916.github.io/UDE-2/","repo":"https://github.com/zixiangzhou916/UDE-2","backbone_tags":"VQ-VAE, Transformer","approach_tags":""},{"arxiv_id":"2311.01015","title":"Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs","abstract":"Most text-driven human motion generation methods employ sequential modeling\\napproaches, e.g., transformer, to extract sentence-level text representations\\nautomatically and implicitly for human motion synthesis. However, these compact\\ntext representations may overemphasize the action names at the expense of other\\nimportant properties and lack fine-grained details to guide the synthesis of\\nsubtly distinct motion. In this paper, we propose hierarchical semantic graphs\\nfor fine-grained control over motion generation. Specifically, we disentangle\\nmotion descriptions into hierarchical semantic graphs including three levels of\\nmotions, actions, and specifics. Such global-to-local structures facilitate a\\ncomprehensive understanding of motion description and fine-grained control of\\nmotion generation. Correspondingly, to leverage the coarse-to-fine topology of\\nhierarchical semantic graphs, we decompose the text-to-motion diffusion process\\ninto three semantic levels, which correspond to capturing the overall motion,\\nlocal actions, and action specifics. Extensive experiments on two benchmark\\nhuman motion datasets, including HumanML3D and KIT, with superior performances,\\njustify the efficacy of our method. More encouragingly, by modifying the edge\\nweights of hierarchical semantic graphs, our method can continuously refine the\\ngenerated motion, which may have a far-reaching impact on the community. Code\\nand pre-training weights are available at\\nhttps://github.com/jpthu17/GraphMotion.","authors":["Peng Jin","Yang Wu","Yanbo Fan","Zhongqian Sun","Yang Wei","Li Yuan"],"year":2023,"month":11,"url":"https://arxiv.org/abs/2311.01015","survey":false,"survey_abbr":"","model":true,"model_abbr":"GraphMotion","dataset":false,"dataset_abbr":"","submission":"NeurIPS","submission_year":"2023","page":"","repo":"https://github.com/jpthu17/GraphMotion","backbone_tags":"Transformer","approach_tags":"Finegrained"},{"arxiv_id":"2310.12978","title":"HumanTOMATO: Text-aligned Whole-body Motion Generation","abstract":"This work targets a novel text-driven whole-body motion generation task,\\nwhich takes a given textual description as input and aims at generating\\nhigh-quality, diverse, and coherent facial expressions, hand gestures, and body\\nmotions simultaneously. Previous works on text-driven motion generation tasks\\nmainly have two limitations: they ignore the key role of fine-grained hand and\\nface controlling in vivid whole-body motion generation, and lack a good\\nalignment between text and motion. To address such limitations, we propose a\\nText-aligned whOle-body Motion generATiOn framework, named HumanTOMATO, which\\nis the first attempt to our knowledge towards applicable holistic motion\\ngeneration in this research area. To tackle this challenging task, our solution\\nincludes two key designs: (1) a Holistic Hierarchical VQ-VAE (aka H$^2$VQ) and\\na Hierarchical-GPT for fine-grained body and hand motion reconstruction and\\ngeneration with two structured codebooks; and (2) a pre-trained\\ntext-motion-alignment model to help generated motion align with the input\\ntextual description explicitly. Comprehensive experiments verify that our model\\nhas significant advantages in both the quality of generated motions and their\\nalignment with text.","authors":["Shunlin Lu","Ling-Hao Chen","Ailing Zeng","Jing Lin","Ruimao Zhang","Lei Zhang","Heung-Yeung Shum"],"year":2023,"month":10,"url":"https://arxiv.org/abs/2310.12978","survey":false,"survey_abbr":"","model":true,"model_abbr":"HumanTOMATO","dataset":false,"dataset_abbr":"","submission":"ICML","submission_year":"2024","page":"https://lhchen.top/HumanTOMATO/","repo":"https://github.com/IDEA-Research/HumanTOMATO","backbone_tags":"VQ-VAE, LLM","approach_tags":"Finegrained"},{"arxiv_id":"2310.10198","title":"MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations","abstract":"In this work, we present MoConVQ, a novel unified framework for physics-based\\nmotion control leveraging scalable discrete representations. Building upon\\nvector quantized variational autoencoders (VQ-VAE) and model-based\\nreinforcement learning, our approach effectively learns motion embeddings from\\na large, unstructured dataset spanning tens of hours of motion examples. The\\nresultant motion representation not only captures diverse motion skills but\\nalso offers a robust and intuitive interface for various applications. We\\ndemonstrate the versatility of MoConVQ through several applications: universal\\ntracking control from various motion sources, interactive character control\\nwith latent motion representations using supervised learning, physics-based\\nmotion generation from natural language descriptions using the GPT framework,\\nand, most interestingly, seamless integration with large language models (LLMs)\\nwith in-context learning to tackle complex and abstract tasks.","authors":["Heyuan Yao","Zhenhua Song","Yuyang Zhou","Tenglong Ao","Baoquan Chen","Libin Liu"],"year":2023,"month":10,"url":"https://arxiv.org/abs/2310.10198","survey":false,"survey_abbr":"","model":true,"model_abbr":"MoConVQ","dataset":false,"dataset_abbr":"","submission":"SIGGRAPH","submission_year":"2024","page":"https://moconvq.github.io/","repo":"https://github.com/heyuanYao-pku/MoConVQ","backbone_tags":"LLM, Transformer","approach_tags":"Multi-Task, Physical"},{"arxiv_id":"2310.04189","title":"Bridging the Gap between Human Motion and Action Semantics via Kinematic Phrases","abstract":"Motion understanding aims to establish a reliable mapping between motion and\\naction semantics, while it is a challenging many-to-many problem. An abstract\\naction semantic (i.e., walk forwards) could be conveyed by perceptually diverse\\nmotions (walking with arms up or swinging). In contrast, a motion could carry\\ndifferent semantics w.r.t. its context and intention. This makes an elegant\\nmapping between them difficult. Previous attempts adopted direct-mapping\\nparadigms with limited reliability. Also, current automatic metrics fail to\\nprovide reliable assessments of the consistency between motions and action\\nsemantics. We identify the source of these problems as the significant gap\\nbetween the two modalities. To alleviate this gap, we propose Kinematic Phrases\\n(KP) that take the objective kinematic facts of human motion with proper\\nabstraction, interpretability, and generality. Based on KP, we can unify a\\nmotion knowledge base and build a motion understanding system. Meanwhile, KP\\ncan be automatically converted from motions to text descriptions with no\\nsubjective bias, inspiring Kinematic Prompt Generation (KPG) as a novel\\nwhite-box motion generation benchmark. In extensive experiments, our approach\\nshows superiority over other methods. Our project is available at\\nhttps://foruck.github.io/KP/.","authors":["Xinpeng Liu","Yong-Lu Li","Ailing Zeng","Zizheng Zhou","Yang You","Cewu Lu"],"year":2023,"month":10,"url":"https://arxiv.org/abs/2310.04189","survey":false,"survey_abbr":"","model":true,"model_abbr":"KP","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2024","page":"https://foruck.github.io/KP/","repo":"https://github.com/Foruck/Kinematic-Phrases","backbone_tags":"","approach_tags":"Finegrained"},{"arxiv_id":"2309.06284","title":"Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model","abstract":"Text-driven human motion generation in computer vision is both significant\\nand challenging. However, current methods are limited to producing either\\ndeterministic or imprecise motion sequences, failing to effectively control the\\ntemporal and spatial relationships required to conform to a given text\\ndescription. In this work, we propose a fine-grained method for generating\\nhigh-quality, conditional human motion sequences supporting precise text\\ndescription. Our approach consists of two key components: 1) a\\nlinguistics-structure assisted module that constructs accurate and complete\\nlanguage feature to fully utilize text information; and 2) a context-aware\\nprogressive reasoning module that learns neighborhood and overall semantic\\nlinguistics features from shallow and deep graph neural networks to achieve a\\nmulti-step inference. Experiments show that our approach outperforms\\ntext-driven motion generation methods on HumanML3D and KIT test sets and\\ngenerates better visually confirmed motion to the text conditions.","authors":["Yin Wang","Zhiying Leng","Frederick W. B. Li","Shun-Cheng Wu","Xiaohui Liang"],"year":2023,"month":9,"url":"https://arxiv.org/abs/2309.06284","survey":false,"survey_abbr":"","model":true,"model_abbr":"Fg-T2M","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2023","page":"","repo":"","backbone_tags":"CLIP, Diffusion","approach_tags":"Graph"},{"arxiv_id":"2309.00796","title":"AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism","abstract":"Generating 3D human motion based on textual descriptions has been a research\\nfocus in recent years. It requires the generated motion to be diverse, natural,\\nand conform to the textual description. Due to the complex spatio-temporal\\nnature of human motion and the difficulty in learning the cross-modal\\nrelationship between text and motion, text-driven motion generation is still a\\nchallenging problem. To address these issues, we propose \\\\textbf{AttT2M}, a\\ntwo-stage method with multi-perspective attention mechanism: \\\\textbf{body-part\\nattention} and \\\\textbf{global-local motion-text attention}. The former focuses\\non the motion embedding perspective, which means introducing a body-part\\nspatio-temporal encoder into VQ-VAE to learn a more expressive discrete latent\\nspace. The latter is from the cross-modal perspective, which is used to learn\\nthe sentence-level and word-level motion-text cross-modal relationship. The\\ntext-driven motion is finally generated with a generative transformer.\\nExtensive experiments conducted on HumanML3D and KIT-ML demonstrate that our\\nmethod outperforms the current state-of-the-art works in terms of qualitative\\nand quantitative evaluation, and achieve fine-grained synthesis and\\naction2motion. Our code is in https://github.com/ZcyMonkey/AttT2M","authors":["Chongyang Zhong","Lei Hu","Zihao Zhang","Shihong Xia"],"year":2023,"month":9,"url":"https://arxiv.org/abs/2309.00796","survey":false,"survey_abbr":"","model":true,"model_abbr":"AttT2M","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2023","page":"","repo":"https://github.com/ZcyMonkey/AttT2M","backbone_tags":"VQ-VAE, Transformer","approach_tags":"Finegrained"},{"arxiv_id":"2308.14480","title":"Priority-Centric Human Motion Generation in Discrete Latent Space","abstract":"Text-to-motion generation is a formidable task, aiming to produce human\\nmotions that align with the input text while also adhering to human\\ncapabilities and physical laws. While there have been advancements in diffusion\\nmodels, their application in discrete spaces remains underexplored. Current\\nmethods often overlook the varying significance of different motions, treating\\nthem uniformly. It is essential to recognize that not all motions hold the same\\nrelevance to a particular textual description. Some motions, being more salient\\nand informative, should be given precedence during generation. In response, we\\nintroduce a Priority-Centric Motion Discrete Diffusion Model (M2DM), which\\nutilizes a Transformer-based VQ-VAE to derive a concise, discrete motion\\nrepresentation, incorporating a global self-attention mechanism and a\\nregularization term to counteract code collapse. We also present a motion\\ndiscrete diffusion model that employs an innovative noise schedule, determined\\nby the significance of each motion token within the entire motion sequence.\\nThis approach retains the most salient motions during the reverse diffusion\\nprocess, leading to more semantically rich and varied motions. Additionally, we\\nformulate two strategies to gauge the importance of motion tokens, drawing from\\nboth textual and visual indicators. Comprehensive experiments on the HumanML3D\\nand KIT-ML datasets confirm that our model surpasses existing techniques in\\nfidelity and diversity, particularly for intricate textual descriptions.","authors":["Hanyang Kong","Kehong Gong","Dongze Lian","Michael Bi Mi","Xinchao Wang"],"year":2023,"month":8,"url":"https://arxiv.org/abs/2308.14480","survey":false,"survey_abbr":"","model":true,"model_abbr":"M2DM","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2023","page":"","repo":"","backbone_tags":"VQ-VAE, Diffusion, Transformer","approach_tags":""},{"arxiv_id":"2308.09611","title":"Language-guided Human Motion Synthesis with Atomic Actions","abstract":"Language-guided human motion synthesis has been a challenging task due to the\\ninherent complexity and diversity of human behaviors. Previous methods face\\nlimitations in generalization to novel actions, often resulting in unrealistic\\nor incoherent motion sequences. In this paper, we propose ATOM (ATomic mOtion\\nModeling) to mitigate this problem, by decomposing actions into atomic actions,\\nand employing a curriculum learning strategy to learn atomic action\\ncomposition. First, we disentangle complex human motions into a set of atomic\\nactions during learning, and then assemble novel actions using the learned\\natomic actions, which offers better adaptability to new actions. Moreover, we\\nintroduce a curriculum learning training strategy that leverages masked motion\\nmodeling with a gradual increase in the mask ratio, and thus facilitates atomic\\naction assembly. This approach mitigates the overfitting problem commonly\\nencountered in previous methods while enforcing the model to learn better\\nmotion representations. We demonstrate the effectiveness of ATOM through\\nextensive experiments, including text-to-motion and action-to-motion synthesis\\ntasks. We further illustrate its superiority in synthesizing plausible and\\ncoherent text-guided human motion sequences.","authors":["Yuanhao Zhai","Mingzhen Huang","Tianyu Luan","Lu Dong","Ifeoma Nwogu","Siwei Lyu","David Doermann","Junsong Yuan"],"year":2023,"month":8,"url":"https://arxiv.org/abs/2308.09611","survey":false,"survey_abbr":"","model":true,"model_abbr":"ATOM","dataset":false,"dataset_abbr":"","submission":"MM","submission_year":"2023","page":"","repo":"https://github.com/yhZhai/ATOM","backbone_tags":"CLIP, Transformer","approach_tags":""},{"arxiv_id":"2307.00818","title":"Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset","abstract":"In this paper, we present Motion-X, a large-scale 3D expressive whole-body\\nmotion dataset. Existing motion datasets predominantly contain body-only poses,\\nlacking facial expressions, hand gestures, and fine-grained pose descriptions.\\nMoreover, they are primarily collected from limited laboratory scenes with\\ntextual descriptions manually labeled, which greatly limits their scalability.\\nTo overcome these limitations, we develop a whole-body motion and text\\nannotation pipeline, which can automatically annotate motion from either\\nsingle- or multi-view videos and provide comprehensive semantic labels for each\\nvideo and fine-grained whole-body pose descriptions for each frame. This\\npipeline is of high precision, cost-effective, and scalable for further\\nresearch. Based on it, we construct Motion-X, which comprises 15.6M precise 3D\\nwhole-body pose annotations (i.e., SMPL-X) covering 81.1K motion sequences from\\nmassive scenes. Besides, Motion-X provides 15.6M frame-level whole-body pose\\ndescriptions and 81.1K sequence-level semantic labels. Comprehensive\\nexperiments demonstrate the accuracy of the annotation pipeline and the\\nsignificant benefit of Motion-X in enhancing expressive, diverse, and natural\\nmotion generation, as well as 3D whole-body human mesh recovery.","authors":["Jing Lin","Ailing Zeng","Shunlin Lu","Yuanhao Cai","Ruimao Zhang","Haoqian Wang","Lei Zhang"],"year":2023,"month":7,"url":"https://arxiv.org/abs/2307.00818","survey":false,"survey_abbr":"","model":false,"model_abbr":"","dataset":true,"dataset_abbr":"Motion-X","submission":"NeurIPS","submission_year":"2023","page":"https://motion-x-dataset.github.io/","repo":"https://github.com/IDEA-Research/Motion-X","backbone_tags":"","approach_tags":""},{"arxiv_id":"2306.14795","title":"MotionGPT: Human Motion as a Foreign Language","abstract":"Though the advancement of pre-trained large language models unfolds, the\\nexploration of building a unified model for language and other multi-modal\\ndata, such as motion, remains challenging and untouched so far. Fortunately,\\nhuman motion displays a semantic coupling akin to human language, often\\nperceived as a form of body language. By fusing language data with large-scale\\nmotion models, motion-language pre-training that can enhance the performance of\\nmotion-related tasks becomes feasible. Driven by this insight, we propose\\nMotionGPT, a unified, versatile, and user-friendly motion-language model to\\nhandle multiple motion-relevant tasks. Specifically, we employ the discrete\\nvector quantization for human motion and transfer 3D motion into motion tokens,\\nsimilar to the generation process of word tokens. Building upon this \\"motion\\nvocabulary\\", we perform language modeling on both motion and text in a unified\\nmanner, treating human motion as a specific language. Moreover, inspired by\\nprompt learning, we pre-train MotionGPT with a mixture of motion-language data\\nand fine-tune it on prompt-based question-and-answer tasks. Extensive\\nexperiments demonstrate that MotionGPT achieves state-of-the-art performances\\non multiple motion tasks including text-driven motion generation, motion\\ncaptioning, motion prediction, and motion in-between.","authors":["Biao Jiang","Xin Chen","Wen Liu","Jingyi Yu","Gang Yu","Tao Chen"],"year":2023,"month":6,"url":"https://arxiv.org/abs/2306.14795","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionGPT","dataset":false,"dataset_abbr":"","submission":"NeurIPS","submission_year":"2023","page":"https://motion-gpt.github.io/","repo":"https://github.com/OpenMotionLab/MotionGPT","backbone_tags":"LLM, VQ-VAE","approach_tags":""},{"arxiv_id":"2306.10900","title":"MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators","abstract":"Generating realistic human motion from given action descriptions has\\nexperienced significant advancements because of the emerging requirement of\\ndigital humans. While recent works have achieved impressive results in\\ngenerating motion directly from textual action descriptions, they often support\\nonly a single modality of the control signal, which limits their application in\\nthe real digital human industry. This paper presents a Motion General-Purpose\\ngeneraTor (MotionGPT) that can use multimodal control signals, e.g., text and\\nsingle-frame poses, for generating consecutive human motions by treating\\nmultimodal signals as special input tokens in large language models (LLMs).\\nSpecifically, we first quantize multimodal control signals into discrete codes\\nand then formulate them in a unified prompt instruction to ask the LLMs to\\ngenerate the motion answer. Our MotionGPT demonstrates a unified human motion\\ngeneration model with multimodal control signals by tuning a mere 0.4% of LLM\\nparameters. To the best of our knowledge, MotionGPT is the first method to\\ngenerate human motion by multimodal control signals, which we hope can shed\\nlight on this new direction. Visit our webpage at\\nhttps://qiqiapink.github.io/MotionGPT/.","authors":["Yaqi Zhang","Di Huang","Bin Liu","Shixiang Tang","Yan Lu","Lu Chen","Lei Bai","Qi Chu","Nenghai Yu","Wanli Ouyang"],"year":2023,"month":6,"url":"https://arxiv.org/abs/2306.10900","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionGPT","dataset":false,"dataset_abbr":"","submission":"AAAI","submission_year":"2024","page":"https://qiqiapink.github.io/MotionGPT/","repo":"https://github.com/qiqiApink/MotionGPT","backbone_tags":"LLM, VQ-VAE","approach_tags":"LoRA"},{"arxiv_id":"2305.13773","title":"Enhanced Fine-grained Motion Diffusion for Text-driven Human Motion Synthesis","abstract":"The emergence of text-driven motion synthesis technique provides animators\\nwith great potential to create efficiently. However, in most cases, textual\\nexpressions only contain general and qualitative motion descriptions, while\\nlack fine depiction and sufficient intensity, leading to the synthesized\\nmotions that either (a) semantically compliant but uncontrollable over specific\\npose details, or (b) even deviates from the provided descriptions, bringing\\nanimators with undesired cases. In this paper, we propose DiffKFC, a\\nconditional diffusion model for text-driven motion synthesis with KeyFrames\\nCollaborated, enabling realistic generation with collaborative and efficient\\ndual-level control: coarse guidance at semantic level, with only few keyframes\\nfor direct and fine-grained depiction down to body posture level. Unlike\\nexisting inference-editing diffusion models that incorporate conditions without\\ntraining, our conditional diffusion model is explicitly trained and can fully\\nexploit correlations among texts, keyframes and the diffused target frames. To\\npreserve the control capability of discrete and sparse keyframes, we customize\\ndilated mask attention modules where only partial valid tokens participate in\\nlocal-to-global attention, indicated by the dilated keyframe mask.\\nAdditionally, we develop a simple yet effective smoothness prior, which steers\\nthe generated frames towards seamless keyframe transitions at inference.\\nExtensive experiments show that our model not only achieves state-of-the-art\\nperformance in terms of semantic fidelity, but more importantly, is able to\\nsatisfy animator requirements through fine-grained guidance without tedious\\nlabor.","authors":["Dong Wei","Xiaoning Sun","Huaijiang Sun","Bin Li","Shengxiang Hu","Weiqing Li","Jianfeng Lu"],"year":2023,"month":5,"url":"https://arxiv.org/abs/2305.13773","survey":false,"survey_abbr":"","model":true,"model_abbr":"DiffKFC","dataset":false,"dataset_abbr":"","submission":"AAAI","submission_year":"2024","page":"","repo":"","backbone_tags":"CLIP, Transformer","approach_tags":""},{"arxiv_id":"2305.12577","title":"Guided Motion Diffusion for Controllable Human Motion Synthesis","abstract":"Denoising diffusion models have shown great promise in human motion synthesis\\nconditioned on natural language descriptions. However, integrating spatial\\nconstraints, such as pre-defined motion trajectories and obstacles, remains a\\nchallenge despite being essential for bridging the gap between isolated human\\nmotion and its surrounding environment. To address this issue, we propose\\nGuided Motion Diffusion (GMD), a method that incorporates spatial constraints\\ninto the motion generation process. Specifically, we propose an effective\\nfeature projection scheme that manipulates motion representation to enhance the\\ncoherency between spatial information and local poses. Together with a new\\nimputation formulation, the generated motion can reliably conform to spatial\\nconstraints such as global motion trajectories. Furthermore, given sparse\\nspatial constraints (e.g. sparse keyframes), we introduce a new dense guidance\\napproach to turn a sparse signal, which is susceptible to being ignored during\\nthe reverse steps, into denser signals to guide the generated motion to the\\ngiven constraints. Our extensive experiments justify the development of GMD,\\nwhich achieves a significant improvement over state-of-the-art methods in\\ntext-based motion generation while allowing control of the synthesized motions\\nwith spatial constraints.","authors":["Korrawe Karunratanakul","Konpat Preechakul","Supasorn Suwajanakorn","Siyu Tang"],"year":2023,"month":5,"url":"https://arxiv.org/abs/2305.12577","survey":false,"survey_abbr":"","model":true,"model_abbr":"GMD","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2023","page":"https://korrawe.github.io/gmd-project/","repo":"https://github.com/korrawe/guided-motion-diffusion","backbone_tags":"Diffusion","approach_tags":"Trajectory"},{"arxiv_id":"2305.09662","title":"Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation","abstract":"Text-guided human motion generation has drawn significant interest because of\\nits impactful applications spanning animation and robotics. Recently,\\napplication of diffusion models for motion generation has enabled improvements\\nin the quality of generated motions. However, existing approaches are limited\\nby their reliance on relatively small-scale motion capture data, leading to\\npoor performance on more diverse, in-the-wild prompts. In this paper, we\\nintroduce Make-An-Animation, a text-conditioned human motion generation model\\nwhich learns more diverse poses and prompts from large-scale image-text\\ndatasets, enabling significant improvement in performance over prior works.\\nMake-An-Animation is trained in two stages. First, we train on a curated\\nlarge-scale dataset of (text, static pseudo-pose) pairs extracted from\\nimage-text datasets. Second, we fine-tune on motion capture data, adding\\nadditional layers to model the temporal dimension. Unlike prior diffusion\\nmodels for motion generation, Make-An-Animation uses a U-Net architecture\\nsimilar to recent text-to-video generation models. Human evaluation of motion\\nrealism and alignment with input text shows that our model reaches\\nstate-of-the-art performance on text-to-motion generation.","authors":["Samaneh Azadi","Akbar Shah","Thomas Hayes","Devi Parikh","Sonal Gupta"],"year":2023,"month":5,"url":"https://arxiv.org/abs/2305.09662","survey":false,"survey_abbr":"","model":true,"model_abbr":"MAA","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2023","page":"https://azadis.github.io/make-an-animation/","repo":"","backbone_tags":"Transformer","approach_tags":""},{"arxiv_id":"2305.09381","title":"AMD: Autoregressive Motion Diffusion","abstract":"Human motion generation aims to produce plausible human motion sequences\\naccording to various conditional inputs, such as text or audio. Despite the\\nfeasibility of existing methods in generating motion based on short prompts and\\nsimple motion patterns, they encounter difficulties when dealing with long\\nprompts or complex motions. The challenges are two-fold: 1) the scarcity of\\nhuman motion-captured data for long prompts and complex motions. 2) the high\\ndiversity of human motions in the temporal domain and the substantial\\ndivergence of distributions from conditional modalities, leading to a\\nmany-to-many mapping problem when generating motion with complex and long\\ntexts. In this work, we address these gaps by 1) elaborating the first dataset\\npairing long textual descriptions and 3D complex motions (HumanLong3D), and 2)\\nproposing an autoregressive motion diffusion model (AMD). Specifically, AMD\\nintegrates the text prompt at the current timestep with the text prompt and\\naction sequences at the previous timestep as conditional information to predict\\nthe current action sequences in an iterative manner. Furthermore, we present\\nits generalization for X-to-Motion with \\"No Modality Left Behind\\", enabling the\\ngeneration of high-definition and high-fidelity human motions based on\\nuser-defined modality input.","authors":["Bo Han","Hao Peng","Minjing Dong","Yi Ren","Yixuan Shen","Chang Xu"],"year":2023,"month":5,"url":"https://arxiv.org/abs/2305.09381","survey":false,"survey_abbr":"","model":true,"model_abbr":"AMD","dataset":true,"dataset_abbr":"HumanLong3D","submission":"AAAI","submission_year":"2024","page":"","repo":"https://github.com/fluide1022/AMD","backbone_tags":"Transformer, Diffusion","approach_tags":""},{"arxiv_id":"2305.00976","title":"TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis","abstract":"In this paper, we present TMR, a simple yet effective approach for text to 3D\\nhuman motion retrieval. While previous work has only treated retrieval as a\\nproxy evaluation metric, we tackle it as a standalone task. Our method extends\\nthe state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a\\ncontrastive loss to better structure the cross-modal latent space. We show that\\nmaintaining the motion generation loss, along with the contrastive training, is\\ncrucial to obtain good performance. We introduce a benchmark for evaluation and\\nprovide an in-depth analysis by reporting results on several protocols. Our\\nextensive experiments on the KIT-ML and HumanML3D datasets show that TMR\\noutperforms the prior work by a significant margin, for example reducing the\\nmedian rank from 54 to 19. Finally, we showcase the potential of our approach\\non moment retrieval. Our code and models are publicly available at\\nhttps://mathis.petrovich.fr/tmr.","authors":["Mathis Petrovich","Michael J. Black","G\xfcl Varol"],"year":2023,"month":5,"url":"https://arxiv.org/abs/2305.00976","survey":false,"survey_abbr":"","model":true,"model_abbr":"TMR","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2023","page":"https://mathis.petrovich.fr/tmr/","repo":"https://github.com/Mathux/TMR","backbone_tags":"","approach_tags":"Retrieval"},{"arxiv_id":"2304.02419","title":"TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration","abstract":"We propose a novel task for generating 3D dance movements that simultaneously\\nincorporate both text and music modalities. Unlike existing works that generate\\ndance movements using a single modality such as music, our goal is to produce\\nricher dance movements guided by the instructive information provided by the\\ntext. However, the lack of paired motion data with both music and text\\nmodalities limits the ability to generate dance movements that integrate both.\\nTo alleviate this challenge, we propose to utilize a 3D human motion VQ-VAE to\\nproject the motions of the two datasets into a latent space consisting of\\nquantized vectors, which effectively mix the motion tokens from the two\\ndatasets with different distributions for training. Additionally, we propose a\\ncross-modal transformer to integrate text instructions into motion generation\\narchitecture for generating 3D dance movements without degrading the\\nperformance of music-conditioned dance generation. To better evaluate the\\nquality of the generated motion, we introduce two novel metrics, namely Motion\\nPrediction Distance (MPD) and Freezing Score (FS), to measure the coherence and\\nfreezing percentage of the generated motion. Extensive experiments show that\\nour approach can generate realistic and coherent dance movements conditioned on\\nboth text and music while maintaining comparable performance with the two\\nsingle modalities. Code is available at https://garfield-kh.github.io/TM2D/.","authors":["Kehong Gong","Dongze Lian","Heng Chang","Chuan Guo","Zihang Jiang","Xinxin Zuo","Michael Bi Mi","Xinchao Wang"],"year":2023,"month":4,"url":"https://arxiv.org/abs/2304.02419","survey":false,"survey_abbr":"","model":true,"model_abbr":"TM2D","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2023","page":"https://garfield-kh.github.io/TM2D/","repo":"https://github.com/Garfield-kh/TM2D","backbone_tags":"VQ-VAE","approach_tags":"Multi-Task"},{"arxiv_id":"2304.01116","title":"ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model","abstract":"3D human motion generation is crucial for creative industry. Recent advances\\nrely on generative models with domain knowledge for text-driven motion\\ngeneration, leading to substantial progress in capturing common motions.\\nHowever, the performance on more diverse motions remains unsatisfactory. In\\nthis work, we propose ReMoDiffuse, a diffusion-model-based motion generation\\nframework that integrates a retrieval mechanism to refine the denoising\\nprocess. ReMoDiffuse enhances the generalizability and diversity of text-driven\\nmotion generation with three key designs: 1) Hybrid Retrieval finds appropriate\\nreferences from the database in terms of both semantic and kinematic\\nsimilarities. 2) Semantic-Modulated Transformer selectively absorbs retrieval\\nknowledge, adapting to the difference between retrieved samples and the target\\nmotion sequence. 3) Condition Mixture better utilizes the retrieval database\\nduring inference, overcoming the scale sensitivity in classifier-free guidance.\\nExtensive experiments demonstrate that ReMoDiffuse outperforms state-of-the-art\\nmethods by balancing both text-motion consistency and motion quality,\\nespecially for more diverse motion generation.","authors":["Mingyuan Zhang","Xinying Guo","Liang Pan","Zhongang Cai","Fangzhou Hong","Huirong Li","Lei Yang","Ziwei Liu"],"year":2023,"month":4,"url":"https://arxiv.org/abs/2304.01116","survey":false,"survey_abbr":"","model":true,"model_abbr":"ReMoDiffuse","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2023","page":"https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html","repo":"https://github.com/mingyuan-zhang/ReMoDiffuse","backbone_tags":"Transformer","approach_tags":"Retrieval"},{"arxiv_id":"2303.01418","title":"Human Motion Diffusion as a Generative Prior","abstract":"Recent work has demonstrated the significant potential of denoising diffusion\\nmodels for generating human motion, including text-to-motion capabilities.\\nHowever, these methods are restricted by the paucity of annotated motion data,\\na focus on single-person motions, and a lack of detailed control. In this\\npaper, we introduce three forms of composition based on diffusion priors:\\nsequential, parallel, and model composition. Using sequential composition, we\\ntackle the challenge of long sequence generation. We introduce DoubleTake, an\\ninference-time method with which we generate long animations consisting of\\nsequences of prompted intervals and their transitions, using a prior trained\\nonly for short clips. Using parallel composition, we show promising steps\\ntoward two-person generation. Beginning with two fixed priors as well as a few\\ntwo-person training examples, we learn a slim communication block, ComMDM, to\\ncoordinate interaction between the two resulting motions. Lastly, using model\\ncomposition, we first train individual priors to complete motions that realize\\na prescribed motion for a given joint. We then introduce DiffusionBlending, an\\ninterpolation mechanism to effectively blend several such models to enable\\nflexible and efficient fine-grained joint and trajectory-level control and\\nediting. We evaluate the composition methods using an off-the-shelf motion\\ndiffusion model, and further compare the results to dedicated models trained\\nfor these specific tasks.","authors":["Yonatan Shafir","Guy Tevet","Roy Kapon","Amit H. Bermano"],"year":2023,"month":3,"url":"https://arxiv.org/abs/2303.01418","survey":false,"survey_abbr":"","model":true,"model_abbr":"priorMDM","dataset":false,"dataset_abbr":"","submission":"ICLR","submission_year":"2024","page":"https://priormdm.github.io/priorMDM-page/","repo":"https://github.com/priorMDM/priorMDM","backbone_tags":"Diffusion","approach_tags":"Finegrained, Trajectory"},{"arxiv_id":"2301.06052","title":"T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations","abstract":"In this work, we investigate a simple and must-known conditional generative\\nframework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) and\\nGenerative Pre-trained Transformer (GPT) for human motion generation from\\ntextural descriptions. We show that a simple CNN-based VQ-VAE with commonly\\nused training recipes (EMA and Code Reset) allows us to obtain high-quality\\ndiscrete representations. For GPT, we incorporate a simple corruption strategy\\nduring the training to alleviate training-testing discrepancy. Despite its\\nsimplicity, our T2M-GPT shows better performance than competitive approaches,\\nincluding recent diffusion-based approaches. For example, on HumanML3D, which\\nis currently the largest dataset, we achieve comparable performance on the\\nconsistency between text and generated motion (R-Precision), but with FID 0.116\\nlargely outperforming MotionDiffuse of 0.630. Additionally, we conduct analyses\\non HumanML3D and observe that the dataset size is a limitation of our approach.\\nOur work suggests that VQ-VAE still remains a competitive approach for human\\nmotion generation.","authors":["Jianrong Zhang","Yangsong Zhang","Xiaodong Cun","Shaoli Huang","Yong Zhang","Hongwei Zhao","Hongtao Lu","Xi Shen"],"year":2023,"month":1,"url":"https://arxiv.org/abs/2301.06052","survey":false,"survey_abbr":"","model":true,"model_abbr":"T2M-GPT","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2023","page":"https://mael-zys.github.io/T2M-GPT/","repo":"https://github.com/Mael-zys/T2M-GPT","backbone_tags":"VQ-VAE, Transformer","approach_tags":""},{"arxiv_id":"2301.03949","title":"Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models","abstract":"Diffusion-based generative models have recently emerged as powerful solutions\\nfor high-quality synthesis in multiple domains. Leveraging the bidirectional\\nMarkov chains, diffusion probabilistic models generate samples by inferring the\\nreversed Markov chain based on the learned distribution mapping at the forward\\ndiffusion process. In this work, we propose Modiff, a conditional paradigm that\\nbenefits from the denoising diffusion probabilistic model (DDPM) to tackle the\\nproblem of realistic and diverse action-conditioned 3D skeleton-based motion\\ngeneration. We are a pioneering attempt that uses DDPM to synthesize a variable\\nnumber of motion sequences conditioned on a categorical action. We evaluate our\\napproach on the large-scale NTU RGB+D dataset and show improvements over\\nstate-of-the-art motion generation methods.","authors":["Mengyi Zhao","Mengyuan Liu","Bin Ren","Shuling Dai","Nicu Sebe"],"year":2023,"month":1,"url":"https://arxiv.org/abs/2301.03949","survey":false,"survey_abbr":"","model":true,"model_abbr":"Modiff","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"Diffusion","approach_tags":""},{"arxiv_id":"2212.05897","title":"MultiAct: Long-Term 3D Human Motion Generation from Multiple Action Labels","abstract":"We tackle the problem of generating long-term 3D human motion from multiple\\naction labels. Two main previous approaches, such as action- and\\nmotion-conditioned methods, have limitations to solve this problem. The\\naction-conditioned methods generate a sequence of motion from a single action.\\nHence, it cannot generate long-term motions composed of multiple actions and\\ntransitions between actions. Meanwhile, the motion-conditioned methods generate\\nfuture motions from initial motion. The generated future motions only depend on\\nthe past, so they are not controllable by the user\'s desired actions. We\\npresent MultiAct, the first framework to generate long-term 3D human motion\\nfrom multiple action labels. MultiAct takes account of both action and motion\\nconditions with a unified recurrent generation system. It repetitively takes\\nthe previous motion and action label; then, it generates a smooth transition\\nand the motion of the given action. As a result, MultiAct produces realistic\\nlong-term motion controlled by the given sequence of multiple action labels.\\nCodes are available here at https://github.com/TaeryungLee/MultiAct_RELEASE.","authors":["Taeryung Lee","Gyeongsik Moon","Kyoung Mu Lee"],"year":2022,"month":12,"url":"https://arxiv.org/abs/2212.05897","survey":false,"survey_abbr":"","model":true,"model_abbr":"MultiAct","dataset":false,"dataset_abbr":"","submission":"AAAI","submission_year":"2023","page":"","repo":"https://github.com/TaeryungLee/MultiAct_RELEASE","backbone_tags":"VAE","approach_tags":""},{"arxiv_id":"2212.04495","title":"MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis","abstract":"Conventional methods for human motion synthesis are either deterministic or\\nstruggle with the trade-off between motion diversity and motion quality. In\\nresponse to these limitations, we introduce MoFusion, i.e., a new\\ndenoising-diffusion-based framework for high-quality conditional human motion\\nsynthesis that can generate long, temporally plausible, and semantically\\naccurate motions based on a range of conditioning contexts (such as music and\\ntext). We also present ways to introduce well-known kinematic losses for motion\\nplausibility within the motion diffusion framework through our scheduled\\nweighting strategy. The learned latent space can be used for several\\ninteractive motion editing applications -- like inbetweening, seed\\nconditioning, and text-based editing -- thus, providing crucial abilities for\\nvirtual character animation and robotics. Through comprehensive quantitative\\nevaluations and a perceptual user study, we demonstrate the effectiveness of\\nMoFusion compared to the state of the art on established benchmarks in the\\nliterature. We urge the reader to watch our supplementary video and visit\\nhttps://vcai.mpi-inf.mpg.de/projects/MoFusion.","authors":["Rishabh Dabral","Muhammad Hamza Mughal","Vladislav Golyanik","Christian Theobalt"],"year":2022,"month":12,"url":"https://arxiv.org/abs/2212.04495","survey":false,"survey_abbr":"","model":true,"model_abbr":"MoFusion","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2023","page":"https://vcai.mpi-inf.mpg.de/projects/MoFusion/","repo":"","backbone_tags":"UNet, Transformer, Diffusion","approach_tags":""},{"arxiv_id":"2212.04048","title":"Executing your Commands via Motion Diffusion in Latent Space","abstract":"We study a challenging task, conditional human motion generation, which\\nproduces plausible human motion sequences according to various conditional\\ninputs, such as action classes or textual descriptors. Since human motions are\\nhighly diverse and have a property of quite different distribution from\\nconditional modalities, such as textual descriptors in natural languages, it is\\nhard to learn a probabilistic mapping from the desired conditional modality to\\nthe human motion sequences. Besides, the raw motion data from the motion\\ncapture system might be redundant in sequences and contain noises; directly\\nmodeling the joint distribution over the raw motion sequences and conditional\\nmodalities would need a heavy computational overhead and might result in\\nartifacts introduced by the captured noises. To learn a better representation\\nof the various human motion sequences, we first design a powerful Variational\\nAutoEncoder (VAE) and arrive at a representative and low-dimensional latent\\ncode for a human motion sequence. Then, instead of using a diffusion model to\\nestablish the connections between the raw motion sequences and the conditional\\ninputs, we perform a diffusion process on the motion latent space. Our proposed\\nMotion Latent-based Diffusion model (MLD) could produce vivid motion sequences\\nconforming to the given conditional inputs and substantially reduce the\\ncomputational overhead in both the training and inference stages. Extensive\\nexperiments on various human motion generation tasks demonstrate that our MLD\\nachieves significant improvements over the state-of-the-art methods among\\nextensive human motion generation tasks, with two orders of magnitude faster\\nthan previous diffusion models on raw motion sequences.","authors":["Xin Chen","Biao Jiang","Wen Liu","Zilong Huang","Bin Fu","Tao Chen","Jingyi Yu","Gang Yu"],"year":2022,"month":12,"url":"https://arxiv.org/abs/2212.04048","survey":false,"survey_abbr":"","model":true,"model_abbr":"MLD","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2023","page":"https://chenxin.tech/mld/","repo":"https://github.com/chenfengye/motion-latent-diffusion","backbone_tags":"VAE, Diffusion","approach_tags":""},{"arxiv_id":"2212.02500","title":"PhysDiff: Physics-Guided Human Motion Diffusion Model","abstract":"Denoising diffusion models hold great promise for generating diverse and\\nrealistic human motions. However, existing motion diffusion models largely\\ndisregard the laws of physics in the diffusion process and often generate\\nphysically-implausible motions with pronounced artifacts such as floating, foot\\nsliding, and ground penetration. This seriously impacts the quality of\\ngenerated motions and limits their real-world application. To address this\\nissue, we present a novel physics-guided motion diffusion model (PhysDiff),\\nwhich incorporates physical constraints into the diffusion process.\\nSpecifically, we propose a physics-based motion projection module that uses\\nmotion imitation in a physics simulator to project the denoised motion of a\\ndiffusion step to a physically-plausible motion. The projected motion is\\nfurther used in the next diffusion step to guide the denoising diffusion\\nprocess. Intuitively, the use of physics in our model iteratively pulls the\\nmotion toward a physically-plausible space, which cannot be achieved by simple\\npost-processing. Experiments on large-scale human motion datasets show that our\\napproach achieves state-of-the-art motion quality and improves physical\\nplausibility drastically (>78% for all datasets).","authors":["Ye Yuan","Jiaming Song","Umar Iqbal","Arash Vahdat","Jan Kautz"],"year":2022,"month":12,"url":"https://arxiv.org/abs/2212.02500","survey":false,"survey_abbr":"","model":true,"model_abbr":"PhysDiff","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2023","page":"https://nvlabs.github.io/PhysDiff/","repo":"","backbone_tags":"Diffusion","approach_tags":"Physical, Component"},{"arxiv_id":"2211.16016","title":"UDE: A Unified Driving Engine for Human Motion Generation","abstract":"Generating controllable and editable human motion sequences is a key\\nchallenge in 3D Avatar generation. It has been labor-intensive to generate and\\nanimate human motion for a long time until learning-based approaches have been\\ndeveloped and applied recently. However, these approaches are still\\ntask-specific or modality-specific\\\\cite\\n{ahuja2019language2pose}\\\\cite{ghosh2021synthesis}\\\\cite{ferreira2021learning}\\\\cite{li2021ai}.\\nIn this paper, we propose ``UDE\\", the first unified driving engine that enables\\ngenerating human motion sequences from natural language or audio sequences (see\\nFig.~\\\\ref{fig:teaser}). Specifically, UDE consists of the following key\\ncomponents: 1) a motion quantization module based on VQVAE that represents\\ncontinuous motion sequence as discrete latent code\\\\cite{van2017neural}, 2) a\\nmodality-agnostic transformer encoder\\\\cite{vaswani2017attention} that learns to\\nmap modality-aware driving signals to a joint space, and 3) a unified token\\ntransformer (GPT-like\\\\cite{radford2019language}) network to predict the\\nquantized latent code index in an auto-regressive manner. 4) a diffusion motion\\ndecoder that takes as input the motion tokens and decodes them into motion\\nsequences with high diversity. We evaluate our method on\\nHumanML3D\\\\cite{Guo_2022_CVPR} and AIST++\\\\cite{li2021learn} benchmarks, and the\\nexperiment results demonstrate our method achieves state-of-the-art\\nperformance. Project website: \\\\url{https://github.com/zixiangzhou916/UDE/","authors":["Zixiang Zhou","Baoyuan Wang"],"year":2022,"month":11,"url":"https://arxiv.org/abs/2211.16016","survey":false,"survey_abbr":"","model":true,"model_abbr":"UDE","dataset":false,"dataset_abbr":"","submission":"CVPR","submission_year":"2023","page":"https://zixiangzhou916.github.io/UDE/","repo":"https://github.com/zixiangzhou916/UDE/","backbone_tags":"VQ-VAE, Transformer","approach_tags":""},{"arxiv_id":"2211.15603","title":"Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation","abstract":"We introduce Action-GPT, a plug-and-play framework for incorporating Large\\nLanguage Models (LLMs) into text-based action generation models. Action phrases\\nin current motion capture datasets contain minimal and to-the-point\\ninformation. By carefully crafting prompts for LLMs, we generate richer and\\nfine-grained descriptions of the action. We show that utilizing these detailed\\ndescriptions instead of the original action phrases leads to better alignment\\nof text and motion spaces. We introduce a generic approach compatible with\\nstochastic (e.g. VAE-based) and deterministic (e.g. MotionCLIP) text-to-motion\\nmodels. In addition, the approach enables multiple text descriptions to be\\nutilized. Our experiments show (i) noticeable qualitative and quantitative\\nimprovement in the quality of synthesized motions, (ii) benefits of utilizing\\nmultiple LLM-generated descriptions, (iii) suitability of the prompt function,\\nand (iv) zero-shot generation capabilities of the proposed approach. Project\\npage: https://actiongpt.github.io","authors":["Sai Shashank Kalakonda","Shubh Maheshwari","Ravi Kiran Sarvadevabhatla"],"year":2022,"month":11,"url":"https://arxiv.org/abs/2211.15603","survey":false,"survey_abbr":"","model":true,"model_abbr":"Action-GPT","dataset":false,"dataset_abbr":"","submission":"ICME","submission_year":"2023","page":"https://actiongpt.github.io/","repo":"https://github.com/actiongpt/actiongpt","backbone_tags":"LLM, VAE","approach_tags":""},{"arxiv_id":"2210.15929","title":"Being Comes from Not-being: Open-vocabulary Text-to-Motion Generation with Wordless Training","abstract":"Text-to-motion generation is an emerging and challenging problem, which aims\\nto synthesize motion with the same semantics as the input text. However, due to\\nthe lack of diverse labeled training data, most approaches either limit to\\nspecific types of text annotations or require online optimizations to cater to\\nthe texts during inference at the cost of efficiency and stability. In this\\npaper, we investigate offline open-vocabulary text-to-motion generation in a\\nzero-shot learning manner that neither requires paired training data nor extra\\nonline optimization to adapt for unseen texts. Inspired by the prompt learning\\nin NLP, we pretrain a motion generator that learns to reconstruct the full\\nmotion from the masked motion. During inference, instead of changing the motion\\ngenerator, our method reformulates the input text into a masked motion as the\\nprompt for the motion generator to ``reconstruct\'\' the motion. In constructing\\nthe prompt, the unmasked poses of the prompt are synthesized by a text-to-pose\\ngenerator. To supervise the optimization of the text-to-pose generator, we\\npropose the first text-pose alignment model for measuring the alignment between\\ntexts and 3D poses. And to prevent the pose generator from overfitting to\\nlimited training texts, we further propose a novel wordless training mechanism\\nthat optimizes the text-to-pose generator without any training texts. The\\ncomprehensive experimental results show that our method obtains a significant\\nimprovement against the baseline methods. The code is available at\\nhttps://github.com/junfanlin/oohmg.","authors":["Junfan Lin","Jianlong Chang","Lingbo Liu","Guanbin Li","Liang Lin","Qi Tian","Chang Wen Chen"],"year":2022,"month":10,"url":"https://arxiv.org/abs/2210.15929","survey":false,"survey_abbr":"","model":true,"model_abbr":"OOHMG","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"https://github.com/junfanlin/oohmg","backbone_tags":"CLIP, Transformer","approach_tags":"Open-Vocabulary"},{"arxiv_id":"2209.14916","title":"Human Motion Diffusion Model","abstract":"Natural and expressive human motion generation is the holy grail of computer\\nanimation. It is a challenging task, due to the diversity of possible motion,\\nhuman perceptual sensitivity to it, and the difficulty of accurately describing\\nit. Therefore, current generative solutions are either low-quality or limited\\nin expressiveness. Diffusion models, which have already shown remarkable\\ngenerative capabilities in other domains, are promising candidates for human\\nmotion due to their many-to-many nature, but they tend to be resource hungry\\nand hard to control. In this paper, we introduce Motion Diffusion Model (MDM),\\na carefully adapted classifier-free diffusion-based generative model for the\\nhuman motion domain. MDM is transformer-based, combining insights from motion\\ngeneration literature. A notable design-choice is the prediction of the sample,\\nrather than the noise, in each diffusion step. This facilitates the use of\\nestablished geometric losses on the locations and velocities of the motion,\\nsuch as the foot contact loss. As we demonstrate, MDM is a generic approach,\\nenabling different modes of conditioning, and different generation tasks. We\\nshow that our model is trained with lightweight resources and yet achieves\\nstate-of-the-art results on leading benchmarks for text-to-motion and\\naction-to-motion. https://guytevet.github.io/mdm-page/ .","authors":["Guy Tevet","Sigal Raab","Brian Gordon","Yonatan Shafir","Daniel Cohen-Or","Amit H. Bermano"],"year":2022,"month":9,"url":"https://arxiv.org/abs/2209.14916","survey":false,"survey_abbr":"","model":true,"model_abbr":"MDM","dataset":false,"dataset_abbr":"","submission":"ICLR","submission_year":"2023","page":"https://guytevet.github.io/mdm-page/","repo":"https://github.com/GuyTevet/motion-diffusion-model","backbone_tags":"CLIP, Transformer, Diffusion","approach_tags":""},{"arxiv_id":"2209.04066","title":"TEACH: Temporal Action Composition for 3D Humans","abstract":"Given a series of natural language descriptions, our task is to generate 3D\\nhuman motions that correspond semantically to the text, and follow the temporal\\norder of the instructions. In particular, our goal is to enable the synthesis\\nof a series of actions, which we refer to as temporal action composition. The\\ncurrent state of the art in text-conditioned motion synthesis only takes a\\nsingle action or a single sentence as input. This is partially due to lack of\\nsuitable training data containing action sequences, but also due to the\\ncomputational complexity of their non-autoregressive model formulation, which\\ndoes not scale well to long sequences. In this work, we address both issues.\\nFirst, we exploit the recent BABEL motion-text collection, which has a wide\\nrange of labeled actions, many of which occur in a sequence with transitions\\nbetween them. Next, we design a Transformer-based approach that operates\\nnon-autoregressively within an action, but autoregressively within the sequence\\nof actions. This hierarchical formulation proves effective in our experiments\\nwhen compared with multiple baselines. Our approach, called TEACH for \\"TEmporal\\nAction Compositions for Human motions\\", produces realistic human motions for a\\nwide variety of actions and temporal compositions from language descriptions.\\nTo encourage work on this new task, we make our code available for research\\npurposes at our $\\\\href{teach.is.tue.mpg.de}{\\\\text{website}}$.","authors":["Nikos Athanasiou","Mathis Petrovich","Michael J. Black","G\xfcl Varol"],"year":2022,"month":9,"url":"https://arxiv.org/abs/2209.04066","survey":false,"survey_abbr":"","model":true,"model_abbr":"TEACH","dataset":false,"dataset_abbr":"","submission":"3DV","submission_year":"2022","page":"https://teach.is.tue.mpg.de/","repo":"https://github.com/atnikos/teach","backbone_tags":"Transformer","approach_tags":""},{"arxiv_id":"2209.00349","title":"FLAME: Free-form Language-based Motion Synthesis & Editing","abstract":"Text-based motion generation models are drawing a surge of interest for their\\npotential for automating the motion-making process in the game, animation, or\\nrobot industries. In this paper, we propose a diffusion-based motion synthesis\\nand editing model named FLAME. Inspired by the recent successes in diffusion\\nmodels, we integrate diffusion-based generative models into the motion domain.\\nFLAME can generate high-fidelity motions well aligned with the given text.\\nAlso, it can edit the parts of the motion, both frame-wise and joint-wise,\\nwithout any fine-tuning. FLAME involves a new transformer-based architecture we\\ndevise to better handle motion data, which is found to be crucial to manage\\nvariable-length motions and well attend to free-form text. In experiments, we\\nshow that FLAME achieves state-of-the-art generation performances on three\\ntext-motion datasets: HumanML3D, BABEL, and KIT. We also demonstrate that\\nediting capability of FLAME can be extended to other tasks such as motion\\nprediction or motion in-betweening, which have been previously covered by\\ndedicated models.","authors":["Jihoon Kim","Jiseob Kim","Sungjoon Choi"],"year":2022,"month":9,"url":"https://arxiv.org/abs/2209.00349","survey":false,"survey_abbr":"","model":true,"model_abbr":"FLAME","dataset":false,"dataset_abbr":"","submission":"AAAI","submission_year":"2023","page":"https://kakaobrain.github.io/flame/","repo":"https://github.com/kakaobrain/flame","backbone_tags":"Transformer","approach_tags":"Editing"},{"arxiv_id":"2208.15001","title":"MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model","abstract":"Human motion modeling is important for many modern graphics applications,\\nwhich typically require professional skills. In order to remove the skill\\nbarriers for laymen, recent motion generation methods can directly generate\\nhuman motions conditioned on natural languages. However, it remains challenging\\nto achieve diverse and fine-grained motion generation with various text inputs.\\nTo address this problem, we propose MotionDiffuse, the first diffusion\\nmodel-based text-driven motion generation framework, which demonstrates several\\ndesired properties over existing methods. 1) Probabilistic Mapping. Instead of\\na deterministic language-motion mapping, MotionDiffuse generates motions\\nthrough a series of denoising steps in which variations are injected. 2)\\nRealistic Synthesis. MotionDiffuse excels at modeling complicated data\\ndistribution and generating vivid motion sequences. 3) Multi-Level\\nManipulation. MotionDiffuse responds to fine-grained instructions on body\\nparts, and arbitrary-length motion synthesis with time-varied text prompts. Our\\nexperiments show MotionDiffuse outperforms existing SoTA methods by convincing\\nmargins on text-driven motion generation and action-conditioned motion\\ngeneration. A qualitative analysis further demonstrates MotionDiffuse\'s\\ncontrollability for comprehensive motion generation. Homepage:\\nhttps://mingyuan-zhang.github.io/projects/MotionDiffuse.html","authors":["Mingyuan Zhang","Zhongang Cai","Liang Pan","Fangzhou Hong","Xinying Guo","Lei Yang","Ziwei Liu"],"year":2022,"month":8,"url":"https://arxiv.org/abs/2208.15001","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionDiffuse","dataset":false,"dataset_abbr":"","submission":"TPAMI","submission_year":"2024","page":"https://mingyuan-zhang.github.io/projects/MotionDiffuse.html","repo":"https://github.com/mingyuan-zhang/MotionDiffuse","backbone_tags":"Diffusion, Transformer","approach_tags":""},{"arxiv_id":"2207.01696","title":"TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts","abstract":"Inspired by the strong ties between vision and language, the two intimate\\nhuman sensing and communication modalities, our paper aims to explore the\\ngeneration of 3D human full-body motions from texts, as well as its reciprocal\\ntask, shorthanded for text2motion and motion2text, respectively. To tackle the\\nexisting challenges, especially to enable the generation of multiple distinct\\nmotions from the same text, and to avoid the undesirable production of trivial\\nmotionless pose sequences, we propose the use of motion token, a discrete and\\ncompact motion representation. This provides one level playing ground when\\nconsidering both motions and text signals, as the motion and text tokens,\\nrespectively. Moreover, our motion2text module is integrated into the inverse\\nalignment process of our text2motion training pipeline, where a significant\\ndeviation of synthesized text from the input text would be penalized by a large\\ntraining loss; empirically this is shown to effectively improve performance.\\nFinally, the mappings in-between the two modalities of motions and texts are\\nfacilitated by adapting the neural model for machine translation (NMT) to our\\ncontext. This autoregressive modeling of the distribution over discrete motion\\ntokens further enables non-deterministic production of pose sequences, of\\nvariable lengths, from an input text. Our approach is flexible, could be used\\nfor both text2motion and motion2text tasks. Empirical evaluations on two\\nbenchmark datasets demonstrate the superior performance of our approach on both\\ntasks over a variety of state-of-the-art methods. Project page:\\nhttps://ericguo5513.github.io/TM2T/","authors":["Chuan Guo","Xinxin Zuo","Sen Wang","Li Cheng"],"year":2022,"month":7,"url":"https://arxiv.org/abs/2207.01696","survey":false,"survey_abbr":"","model":true,"model_abbr":"TM2T","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2022","page":"https://ericguo5513.github.io/TM2T/","repo":"https://github.com/EricGuo5513/TM2T","backbone_tags":"GRU","approach_tags":"Multi-Task"},{"arxiv_id":"2205.08535","title":"AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars","abstract":"3D avatar creation plays a crucial role in the digital age. However, the\\nwhole production process is prohibitively time-consuming and labor-intensive.\\nTo democratize this technology to a larger audience, we propose AvatarCLIP, a\\nzero-shot text-driven framework for 3D avatar generation and animation. Unlike\\nprofessional software that requires expert knowledge, AvatarCLIP empowers\\nlayman users to customize a 3D avatar with the desired shape and texture, and\\ndrive the avatar with the described motions using solely natural languages. Our\\nkey insight is to take advantage of the powerful vision-language model CLIP for\\nsupervising neural human generation, in terms of 3D geometry, texture and\\nanimation. Specifically, driven by natural language descriptions, we initialize\\n3D human geometry generation with a shape VAE network. Based on the generated\\n3D human shapes, a volume rendering model is utilized to further facilitate\\ngeometry sculpting and texture generation. Moreover, by leveraging the priors\\nlearned in the motion VAE, a CLIP-guided reference-based motion synthesis\\nmethod is proposed for the animation of the generated 3D avatar. Extensive\\nqualitative and quantitative experiments validate the effectiveness and\\ngeneralizability of AvatarCLIP on a wide range of avatars. Remarkably,\\nAvatarCLIP can generate unseen 3D avatars with novel animations, achieving\\nsuperior zero-shot capability.","authors":["Fangzhou Hong","Mingyuan Zhang","Liang Pan","Zhongang Cai","Lei Yang","Ziwei Liu"],"year":2022,"month":5,"url":"https://arxiv.org/abs/2205.08535","survey":false,"survey_abbr":"","model":true,"model_abbr":"AvatarCLIP","dataset":false,"dataset_abbr":"","submission":"SIGGRAPH","submission_year":"2022","page":"https://hongfz16.github.io/projects/AvatarCLIP.html","repo":"https://github.com/hongfz16/AvatarCLIP","backbone_tags":"CLIP, VAE","approach_tags":""},{"arxiv_id":"2204.14109","title":"TEMOS: Generating diverse human motions from textual descriptions","abstract":"We address the problem of generating diverse 3D human motions from textual\\ndescriptions. This challenging task requires joint modeling of both modalities:\\nunderstanding and extracting useful human-centric information from the text,\\nand then generating plausible and realistic sequences of human poses. In\\ncontrast to most previous work which focuses on generating a single,\\ndeterministic, motion from a textual description, we design a variational\\napproach that can produce multiple diverse human motions. We propose TEMOS, a\\ntext-conditioned generative model leveraging variational autoencoder (VAE)\\ntraining with human motion data, in combination with a text encoder that\\nproduces distribution parameters compatible with the VAE latent space. We show\\nthe TEMOS framework can produce both skeleton-based animations as in prior\\nwork, as well more expressive SMPL body motions. We evaluate our approach on\\nthe KIT Motion-Language benchmark and, despite being relatively\\nstraightforward, demonstrate significant improvements over the state of the\\nart. Code and models are available on our webpage.","authors":["Mathis Petrovich","Michael J. Black","G\xfcl Varol"],"year":2022,"month":4,"url":"https://arxiv.org/abs/2204.14109","survey":false,"survey_abbr":"","model":true,"model_abbr":"TEMOS","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2022","page":"https://mathis.petrovich.fr/temos/","repo":"https://github.com/Mathux/TEMOS","backbone_tags":"VAE, Transformer","approach_tags":""},{"arxiv_id":"2203.13694","title":"Implicit Neural Representations for Variable Length Human Motion Generation","abstract":"We propose an action-conditional human motion generation method using\\nvariational implicit neural representations (INR). The variational formalism\\nenables action-conditional distributions of INRs, from which one can easily\\nsample representations to generate novel human motion sequences. Our method\\noffers variable-length sequence generation by construction because a part of\\nINR is optimized for a whole sequence of arbitrary length with temporal\\nembeddings. In contrast, previous works reported difficulties with modeling\\nvariable-length sequences. We confirm that our method with a Transformer\\ndecoder outperforms all relevant methods on HumanAct12, NTU-RGBD, and UESTC\\ndatasets in terms of realism and diversity of generated motions. Surprisingly,\\neven our method with an MLP decoder consistently outperforms the\\nstate-of-the-art Transformer-based auto-encoder. In particular, we show that\\nvariable-length motions generated by our method are better than fixed-length\\nmotions generated by the state-of-the-art method in terms of realism and\\ndiversity. Code at https://github.com/PACerv/ImplicitMotion.","authors":["Pablo Cervantes","Yusuke Sekikawa","Ikuro Sato","Koichi Shinoda"],"year":2022,"month":3,"url":"https://arxiv.org/abs/2203.13694","survey":false,"survey_abbr":"","model":true,"model_abbr":"ImplicitMotion","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2022","page":"","repo":"https://github.com/PACerv/ImplicitMotion","backbone_tags":"VAE","approach_tags":""},{"arxiv_id":"2203.08063","title":"MotionCLIP: Exposing Human Motion Generation to CLIP Space","abstract":"We introduce MotionCLIP, a 3D human motion auto-encoder featuring a latent\\nembedding that is disentangled, well behaved, and supports highly semantic\\ntextual descriptions. MotionCLIP gains its unique power by aligning its latent\\nspace with that of the Contrastive Language-Image Pre-training (CLIP) model.\\nAligning the human motion manifold to CLIP space implicitly infuses the\\nextremely rich semantic knowledge of CLIP into the manifold. In particular, it\\nhelps continuity by placing semantically similar motions close to one another,\\nand disentanglement, which is inherited from the CLIP-space structure.\\nMotionCLIP comprises a transformer-based motion auto-encoder, trained to\\nreconstruct motion while being aligned to its text label\'s position in\\nCLIP-space. We further leverage CLIP\'s unique visual understanding and inject\\nan even stronger signal through aligning motion to rendered frames in a\\nself-supervised manner. We show that although CLIP has never seen the motion\\ndomain, MotionCLIP offers unprecedented text-to-motion abilities, allowing\\nout-of-domain actions, disentangled editing, and abstract language\\nspecification. For example, the text prompt \\"couch\\" is decoded into a sitting\\ndown motion, due to lingual similarity, and the prompt \\"Spiderman\\" results in a\\nweb-swinging-like solution that is far from seen during training. In addition,\\nwe show how the introduced latent space can be leveraged for motion\\ninterpolation, editing and recognition.","authors":["Guy Tevet","Brian Gordon","Amir Hertz","Amit H. Bermano","Daniel Cohen-Or"],"year":2022,"month":3,"url":"https://arxiv.org/abs/2203.08063","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionCLIP","dataset":false,"dataset_abbr":"","submission":"ECCV","submission_year":"2022","page":"https://guytevet.github.io/motionclip-page/","repo":"https://github.com/GuyTevet/MotionCLIP","backbone_tags":"CLIP, Transformer","approach_tags":""},{"arxiv_id":"2104.05670","title":"Action-Conditioned 3D Human Motion Synthesis with Transformer VAE","abstract":"We tackle the problem of action-conditioned generation of realistic and\\ndiverse human motion sequences. In contrast to methods that complete, or\\nextend, motion sequences, this task does not require an initial pose or\\nsequence. Here we learn an action-aware latent representation for human motions\\nby training a generative variational autoencoder (VAE). By sampling from this\\nlatent space and querying a certain duration through a series of positional\\nencodings, we synthesize variable-length motion sequences conditioned on a\\ncategorical action. Specifically, we design a Transformer-based architecture,\\nACTOR, for encoding and decoding a sequence of parametric SMPL human body\\nmodels estimated from action recognition datasets. We evaluate our approach on\\nthe NTU RGB+D, HumanAct12 and UESTC datasets and show improvements over the\\nstate of the art. Furthermore, we present two use cases: improving action\\nrecognition through adding our synthesized data to training, and motion\\ndenoising. Code and models are available on our project page.","authors":["Mathis Petrovich","Michael J. Black","G\xfcl Varol"],"year":2021,"month":4,"url":"https://arxiv.org/abs/2104.05670","survey":false,"survey_abbr":"","model":true,"model_abbr":"ACTOR","dataset":false,"dataset_abbr":"","submission":"ICCV","submission_year":"2021","page":"https://mathis.petrovich.fr/actor/","repo":"https://github.com/Mathux/ACTOR","backbone_tags":"Transformer","approach_tags":""},{"arxiv_id":"2007.15240","title":"Action2Motion: Conditioned Generation of 3D Human Motions","abstract":"Action recognition is a relatively established task, where givenan input\\nsequence of human motion, the goal is to predict its ac-tion category. This\\npaper, on the other hand, considers a relativelynew problem, which could be\\nthought of as an inverse of actionrecognition: given a prescribed action type,\\nwe aim to generateplausible human motion sequences in 3D. Importantly, the set\\nofgenerated motions are expected to maintain itsdiversityto be ableto explore\\nthe entire action-conditioned motion space; meanwhile,each sampled sequence\\nfaithfully resembles anaturalhuman bodyarticulation dynamics. Motivated by\\nthese objectives, we followthe physics law of human kinematics by adopting the\\nLie Algebratheory to represent thenaturalhuman motions; we also propose\\natemporal Variational Auto-Encoder (VAE) that encourages adiversesampling of\\nthe motion space. A new 3D human motion dataset, HumanAct12, is also\\nconstructed. Empirical experiments overthree distinct human motion datasets\\n(including ours) demonstratethe effectiveness of our approach.","authors":["Chuan Guo","Xinxin Zuo","Sen Wang","Shihao Zou","Qingyao Sun","Annan Deng","Minglun Gong","Li Cheng"],"year":2020,"month":7,"url":"https://arxiv.org/abs/2007.15240","survey":false,"survey_abbr":"","model":true,"model_abbr":"Action2Motion","dataset":false,"dataset_abbr":"","submission":"MM","submission_year":"2020","page":"https://ericguo5513.github.io/action-to-motion/","repo":"https://github.com/EricGuo5513/action-to-motion","backbone_tags":"GRU","approach_tags":""},{"arxiv_id":"1607.03827","title":"The KIT Motion-Language Dataset","abstract":"Linking human motion and natural language is of great interest for the\\ngeneration of semantic representations of human activities as well as for the\\ngeneration of robot activities based on natural language input. However, while\\nthere have been years of research in this area, no standardized and openly\\navailable dataset exists to support the development and evaluation of such\\nsystems. We therefore propose the KIT Motion-Language Dataset, which is large,\\nopen, and extensible. We aggregate data from multiple motion capture databases\\nand include them in our dataset using a unified representation that is\\nindependent of the capture system or marker set, making it easy to work with\\nthe data regardless of its origin. To obtain motion annotations in natural\\nlanguage, we apply a crowd-sourcing approach and a web-based tool that was\\nspecifically build for this purpose, the Motion Annotation Tool. We thoroughly\\ndocument the annotation process itself and discuss gamification methods that we\\nused to keep annotators motivated. We further propose a novel method,\\nperplexity-based selection, which systematically selects motions for further\\nannotation that are either under-represented in our dataset or that have\\nerroneous annotations. We show that our method mitigates the two aforementioned\\nproblems and ensures a systematic annotation process. We provide an in-depth\\nanalysis of the structure and contents of our resulting dataset, which, as of\\nOctober 10, 2016, contains 3911 motions with a total duration of 11.23 hours\\nand 6278 annotations in natural language that contain 52,903 words. We believe\\nthis makes our dataset an excellent choice that enables more transparent and\\ncomparable research in this important area.","authors":["Matthias Plappert","Christian Mandery","Tamim Asfour"],"year":2016,"month":7,"url":"https://arxiv.org/abs/1607.03827","survey":false,"survey_abbr":"","model":false,"model_abbr":"","dataset":true,"dataset_abbr":"KIT","submission":"Big Data","submission_year":"2016","page":"https://motion-annotation.humanoids.kit.edu/dataset/","repo":"","backbone_tags":"","approach_tags":""},{"arxiv_id":"2307.10894","title":"Human Motion Generation: A Survey","abstract":"Human motion generation aims to generate natural human pose sequences and\\nshows immense potential for real-world applications. Substantial progress has\\nbeen made recently in motion data collection technologies and generation\\nmethods, laying the foundation for increasing interest in human motion\\ngeneration. Most research within this field focuses on generating human motions\\nbased on conditional signals, such as text, audio, and scene contexts. While\\nsignificant advancements have been made in recent years, the task continues to\\npose challenges due to the intricate nature of human motion and its implicit\\nrelationship with conditional signals. In this survey, we present a\\ncomprehensive literature review of human motion generation, which, to the best\\nof our knowledge, is the first of its kind in this field. We begin by\\nintroducing the background of human motion and generative models, followed by\\nan examination of representative methods for three mainstream sub-tasks:\\ntext-conditioned, audio-conditioned, and scene-conditioned human motion\\ngeneration. Additionally, we provide an overview of common datasets and\\nevaluation metrics. Lastly, we discuss open problems and outline potential\\nfuture research directions. We hope that this survey could provide the\\ncommunity with a comprehensive glimpse of this rapidly evolving field and\\ninspire novel ideas that address the outstanding challenges.","authors":["Wentao Zhu","Xiaoxuan Ma","Dongwoo Ro","Hai Ci","Jinlu Zhang","Jiaxin Shi","Feng Gao","Qi Tian","Yizhou Wang"],"year":2023,"month":7,"url":"https://arxiv.org/abs/2307.10894","survey":true,"survey_abbr":"Survey2307","model":false,"model_abbr":"","dataset":false,"dataset_abbr":"","submission":"TPAMI","submission_year":"2023","page":"","repo":"","backbone_tags":"","approach_tags":""},{"arxiv_id":"2506.03191","title":"Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward","abstract":"This paper presents an in-depth survey on the use of multimodal Generative\\nArtificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs)\\nfor human motion understanding and generation, offering insights into emerging\\nmethods, architectures, and their potential to advance realistic and versatile\\nmotion synthesis. Focusing exclusively on text and motion modalities, this\\nresearch investigates how textual descriptions can guide the generation of\\ncomplex, human-like motion sequences. The paper explores various generative\\napproaches, including autoregressive models, diffusion models, Generative\\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), and\\ntransformer-based models, by analyzing their strengths and limitations in terms\\nof motion quality, computational efficiency, and adaptability. It highlights\\nrecent advances in text-conditioned motion generation, where textual inputs are\\nused to control and refine motion outputs with greater precision. The\\nintegration of LLMs further enhances these models by enabling semantic\\nalignment between instructions and motion, improving coherence and contextual\\nrelevance. This systematic survey underscores the transformative potential of\\ntext-to-motion GenAI and LLM architectures in applications such as healthcare,\\nhumanoids, gaming, animation, and assistive technologies, while addressing\\nongoing challenges in generating efficient and realistic human motion.","authors":["Muhammad Islam","Tao Huang","Euijoon Ahn","Usman Naseem"],"year":2025,"month":5,"url":"https://arxiv.org/abs/2506.03191","survey":true,"survey_abbr":"Survey2506","model":false,"model_abbr":"","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"","approach_tags":""},{"arxiv_id":"2507.05419","title":"Motion Generation: A Survey of Generative Approaches and Benchmarks","abstract":"Motion generation, the task of synthesizing realistic motion sequences from\\nvarious conditioning inputs, has become a central problem in computer vision,\\ncomputer graphics, and robotics, with applications ranging from animation and\\nvirtual agents to human-robot interaction. As the field has rapidly progressed\\nwith the introduction of diverse modeling paradigms including GANs,\\nautoencoders, autoregressive models, and diffusion-based techniques, each\\napproach brings its own advantages and limitations. This growing diversity has\\ncreated a need for a comprehensive and structured review that specifically\\nexamines recent developments from the perspective of the generative approach\\nemployed.\\n  In this survey, we provide an in-depth categorization of motion generation\\nmethods based on their underlying generative strategies. Our main focus is on\\npapers published in top-tier venues since 2023, reflecting the most recent\\nadvancements in the field. In addition, we analyze architectural principles,\\nconditioning mechanisms, and generation settings, and compile a detailed\\noverview of the evaluation metrics and datasets used across the literature. Our\\nobjective is to enable clearer comparisons and identify open challenges,\\nthereby offering a timely and foundational reference for researchers and\\npractitioners navigating the rapidly evolving landscape of motion generation.","authors":["Aliasghar Khani","Arianna Rampini","Bruno Roy","Larasika Nadela","Noa Kaplan","Evan Atherton","Derek Cheung","Jacky Bibliowicz"],"year":2025,"month":7,"url":"https://arxiv.org/abs/2507.05419","survey":true,"survey_abbr":"Survey2507","model":false,"model_abbr":"","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"","approach_tags":""},{"arxiv_id":"2505.09379","title":"Text-driven Motion Generation: Overview, Challenges and Directions","abstract":"Text-driven motion generation offers a powerful and intuitive way to create\\nhuman movements directly from natural language. By removing the need for\\npredefined motion inputs, it provides a flexible and accessible approach to\\ncontrolling animated characters. This makes it especially useful in areas like\\nvirtual reality, gaming, human-computer interaction, and robotics. In this\\nreview, we first revisit the traditional perspective on motion synthesis, where\\nmodels focused on predicting future poses from observed initial sequences,\\noften conditioned on action labels. We then provide a comprehensive and\\nstructured survey of modern text-to-motion generation approaches, categorizing\\nthem from two complementary perspectives: (i) architectural, dividing methods\\ninto VAE-based, diffusion-based, and hybrid models; and (ii) motion\\nrepresentation, distinguishing between discrete and continuous motion\\ngeneration strategies. In addition, we explore the most widely used datasets,\\nevaluation methods, and recent benchmarks that have shaped progress in this\\narea. With this survey, we aim to capture where the field currently stands,\\nbring attention to its key challenges and limitations, and highlight promising\\ndirections for future exploration. We hope this work offers a valuable starting\\npoint for researchers and practitioners working to push the boundaries of\\nlanguage-driven human motion synthesis.","authors":["Ali Rida Sahili","Najett Neji","Hedi Tabia"],"year":2025,"month":5,"url":"https://arxiv.org/abs/2505.09379","survey":true,"survey_abbr":"Survey2505","model":false,"model_abbr":"","dataset":false,"dataset_abbr":"","submission":"","submission_year":"","page":"","repo":"","backbone_tags":"","approach_tags":""},{"arxiv_id":"non-arxiv1","title":"Generating Diverse and Natural 3D Human Motions from Text","abstract":"Automated generation of 3D human motions from text is a challenging problem. The generated motions are expected to be sufficiently diverse to explore the text-grounded motion space, and more importantly, accurately depicting the content in prescribed text descriptions. Here we tackle this problem with a two-stage approach: text2length sampling and text2motion generation. Text2length involves sampling from the learned distribution function of motion lengths conditioned on the input text. This is followed by our text2motion module using temporal variational autoencoder to synthesize a diverse set of human motions of the sampled lengths. Instead of directly engaging with pose sequences, we propose motion snippet code as our internal motion representation, which captures local semantic motion contexts and is empirically shown to facilitate the generation of plausible motions faithful to the input text. Moreover, a large-scale dataset of scripted 3D Human motions, HumanML3D, is constructed, consisting of 14,616 motion clips and 44,970 text descriptions.\\n\\n","authors":["Chuan Guo","Shihao Zou","Xinxin Zuo","Sen Wang","Wei Ji","Xingyu Li","Li Cheng"],"year":2022,"month":6,"url":"https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf","survey":false,"survey_abbr":"","model":true,"model_abbr":"T2M","dataset":true,"dataset_abbr":"HumanML3D","submission":"CVPR","submission_year":"2022","page":"https://ericguo5513.github.io/text-to-motion/","repo":"https://github.com/EricGuo5513/text-to-motion","backbone_tags":"GRU","approach_tags":""},{"arxiv_id":"non-arxiv2","title":"CLaM: An Open-Source Library for Performance Evaluation of Text-driven Human Motion Generation","abstract":"Text-driven human motion generation, which creates motion sequences based on textual descriptions, has attracted great attention in the communities of multimedia and artificial intelligence. By parsing and comprehending textual information and converting it into specific human movements, it realizes a direct transformation from human semantics to motion sequences. New text-driven human motion generators are springing up to achieve better performance. However, the absence of well-trained evaluators that can effectively estimate the consistency between the text prompts and motions generated by existing generators remains a challenge. To address the above issues, we propose an open-source library with a powerful Contrastive Language-and-Motion (CLaM) pre-training evaluator, which can be employed for evaluating a variety of text-driven human motion generation algorithms. We perform a thorough performance evaluation of the existing algorithms on various metrics, such as R-Precision. As a by-product, we build a large-scale HumanML3D-synthesis dataset, which consists of 14,616 motion sequences and 547,102 textual descriptions, which is ten times larger than the widely-used HumanML3D dataset. The source codes and models for CLaM are available at~https://github.com/SheldongChen/CLaM/.","authors":["Xiaodong Chen","Kunlang He","Wu Liu","Xinchen Liu","Zheng-Jun Zha","Tao Mei"],"year":2024,"month":10,"url":"https://dl.acm.org/doi/10.1145/3664647.3685523","survey":false,"survey_abbr":"","model":true,"model_abbr":"CLaM","dataset":true,"dataset_abbr":"HumanML3D-synthesis","submission":"MM","submission_year":"24","page":"","repo":"https://github.com/SheldongChen/CLaM/","backbone_tags":"Transformer","approach_tags":""},{"arxiv_id":"non-arxiv3","title":"Text Motion Translator: A Bi-Directional Model for Enhanced 3D Human Motion Generation from Open-Vocabulary Descriptions","abstract":"The field of 3D human motion generation from natural language descriptions, known as Text2Motion, has gained significant attention for its potential application in industries such as film, gaming, and AR/VR. To tackle a key challenge in Text2Motion, the deficiency of 3D human motions and their corresponding textual descriptions, we built a novel large-scale 3D human motion dataset, LaViMo, extracted from in-the-wild web videos and action recognition datasets. LaViMo is approximately 3.3 times larger and encompasses a much broader range of actions than the largest available 3D motion dataset. We then introduce a novel multi-task framework TMT (Text Motion Translator), aimed at generating faithful 3D human motions from natural language descriptions, especially focusing on complicated actions and those not existing in the training set. In contrast to prior works, TMT is uniquely regularized by multiple tasks, including Text2Motion, Motion2Text, Text2Text, and Motion2Motion. This multi-task regularization significantly bolsters the models robustness and enhances its ability of motion modeling and semantic understanding. Additionally, we devised an augmentation method for the textual descriptions using Large Language Models. This augmentation significantly enhances the models capability to interpret open-vocabulary descriptions while generating motions. The results demonstrate substantial improvements over existing state-of-the-art methods, particularly in handling diverse and novel motion descriptions, laying a strong foundation for future research in the field.","authors":["Yijun Qian","Jack Urbanek","Alexander Hauptmann","Jungdam Won"],"year":2024,"month":11,"url":"https://eccv.ecva.net/virtual/2024/poster/266","survey":false,"survey_abbr":"","model":true,"model_abbr":"TMT","dataset":true,"dataset_abbr":"LaViMo","submission":"ECCV","submission_year":"2024","page":"","repo":"","backbone_tags":"LLM, VQ-VAE","approach_tags":"Multi-Task"},{"arxiv_id":"non-arxiv4","title":"Towards Emotion-enriched Text-to-Motion Generation via LLM-guided Limb-level Emotion Manipulating\\n","abstract":"In the literature, existing studies on text-to-motion generation (TMG) routinely focus on exploring the objective alignment of text and motion, which largely ignore the subjective emotion information, especially the limb-level emotion information. With this in mind, this paper proposes a new Emotion-enriched Text-to-Motion Generation (ETMG) task, aiming to generate motions with the subjective emotion information. Further, this paper believes that injecting emotions into limbs (named intra-limb emotion injection) and ensuring the coordination and coherence of emotional motions after injecting emotion information (named inter-limb emotion disturbance) is rather important and challenging in this ETMG task. To this end, this paper proposes an LL M-guided Limb-level Emotion Manipulating ( L3 EM) approach to ETMG. Specifically, this approach designs an LLM-guided intra-limb emotion modeling block to inject emotion into limbs, followed by a graph-structured inter-limb relation modeling block to ensure the coordination and coherence of emotional motions. Particularly, this paper constructs a coarse-grained Emotional Text-to-Motion (EmotionalT2M) dataset and a fine-grained Limb -level Emotional Text-to-Motion (Limb-ET2M) dataset to justify the effectiveness of the proposed L3EM approach. Detailed evaluation demonstrates the significant advantage of our L3EM approach to ETMG over the state-of-the-art baselines. This justifies the importance of the limb-level emotion information for ETMG and the effectiveness of our L3EM approach in coherently manipulating such information.","authors":["Tan Yu","Jingjing Wang","Jiawen Wang","Jiamin Luo","Guodong Zhou"],"year":2024,"month":10,"url":"https://dl.acm.org/doi/10.1145/3664647.3681487","survey":false,"survey_abbr":"","model":true,"model_abbr":"L3EM","dataset":true,"dataset_abbr":"Limb-ET2M","submission":"MM","submission_year":"2024","page":"","repo":"https://github.com/aekx/L3EM","backbone_tags":"CLIP, LLM, Diffusion","approach_tags":"Graph"},{"arxiv_id":"non-arxiv5","title":"M-Adaptor: Text-driven Whole-body Human Motion Generation","abstract":"Text-driven whole-body human motion generation, which involves the creation of motion sequences based on textual descriptions, has attracted much attention in the communities of computer vision and artificial intelligence. It aims to extend text-driven motion generation tasks to accommodate complex whole-body human motions, encompassing facial expressions and hand gestures. Researchers have recently developed large-scale 3D expressive whole-body motion datasets enriched with semantic labels and pose descriptions. Nonetheless, there remains a considerable demand within the community for a straightforward and effective framework for generating and evaluating whole-body human motion based on textual descriptions. To address the above issues, we introduce M-Adaptor, a two-stage Low-Rank Adaptation (LoRA)-based generator for whole-body motion generation tasks, to improve the quality and diversity of body motions, facial expressions, and hand gestures. In particular, it first generates initial coarse-grained body motion tokens from textual prompts to enhance the stability of generated motions, then iterates fine-grained facial expressions with the LoRA-based adaptor to enhance motion expressiveness. Furthermore, we extend the existing state-of-the-art CLaM model to CLaM-H and CLaM-X for evaluation of SMPL-H and SMPL-X based motion generation. Extensive qualitative and quantitative evaluations demonstrate our framework\'s superior performance, with a significant R-Precision improvement for text-driven whole-body motion generation.","authors":["Alicia Li","Xiaodong Chen","Bohao Liang","Qian Bao","Wu Liu"],"year":2024,"month":11,"url":"https://openaccess.thecvf.com/content/CVPR2025W/PVUW/html/Li_M-Adaptor_Text-driven_Whole-body_Human_Motion_Generation_CVPRW_2025_paper.html","survey":false,"survey_abbr":"","model":true,"model_abbr":"M-Adaptor","dataset":false,"dataset_abbr":"","submission":"CVPR Workshop","submission_year":"2025","page":"","repo":"","backbone_tags":"Transformer","approach_tags":"LoRA"},{"arxiv_id":"non-arxiv6","title":"MotionGPT: Human Motion Synthesis with Improved Diversity and Realism via GPT-3 Prompting","abstract":"There are numerous applications for human motion synthesis, including animation, gaming, robotics, or sports science. In recent years, human motion generation from natural language has emerged as a promising alternative to costly and labor-intensive data collection methods relying on motion capture or wearable sensors (e.g., suits). Despite this, generating human motion from textual descriptions remains a challenging and intricate task, primarily due to the scarcity of large-scale supervised datasets capable of capturing the full diversity of human activity.This study proposes a new approach, called MotionGPT, to address the limitations of previous text-based human motion generation methods by utilizing the extensive semantic information available in large language models (LLMs). We first pretrain a doubly text-conditional motion diffusion model on both coarse (\\"high-level\\") and detailed (\\"low-level\\") ground truth text data. Then during inference, we improve motion diversity and alignment with the training set, by zero-shot prompting GPT-3 for additional \\"low-level\\" details. Our method achieves new state-of-the-art quantitative results in terms of Fr\xe9chet Inception Distance (FID) and motion diversity metrics, and improves all considered metrics. Furthermore, it has strong qualitative performance, producing natural results. Code is available at https://github.com/humansensinglab/MotionGPT","authors":["Jose Ribeiro-Gomes","Tianhui Cai","Zolt\xe1n \xc1. Milacski","Chen Wu","Aayush Prakash","Shingo Takagi","Amaury Aubel","Daeil Kim","Alexandre Bernardino","Fernando De La Torre"],"year":2024,"month":7,"url":"https://ieeexplore.ieee.org/document/10484383","survey":false,"survey_abbr":"","model":true,"model_abbr":"MotionGPT","dataset":false,"dataset_abbr":"","submission":"WACV","submission_year":"2024","page":"","repo":"https://github.com/humansensinglab/MotionGPT","backbone_tags":"LLM, CLIP, Transformer, Diffusion","approach_tags":""},{"arxiv_id":"non-arxiv7","title":"ReMoGPT: Part-Level Retrieval-Augmented Motion-Language Models","abstract":"Generation of 3D human motion holds significant importance in the creative industry. While recent notable advances have been made in generating common motions, existing methods struggle to generate diverse and rare motions due to the complexity of motions and limited training data. This work introduces ReMoGPT, a unified motion-language generative model that solves a wide range of motion-related tasks by incorporating a multi-modal retrieval mechanism into the generation process to address the limitations of existing models, namely diversity and generalizability. We propose to focus on body-part-level motion features to enable fine-grained text-motion retrieval and locate suitable references from the database to conduct generation. Then, the motion-language generative model is trained with prompt-based question-and-answer tasks designed for different motion-relevant problems. We incorporate the retrieved samples into the prompt, and then perform instruction tuning of the motion-language model, to learn from task feedback and produce promising results with the help of fine-grained multi-modal retrieval. Extensive experiments validate the efficacy of ReMoGPT, showcasing its superiority over existing state-of-the-art methods. The framework performs well on multiple motion tasks, including motion retrieval, generation, and captioning.","authors":["Qing Yu","Mikihiro Tanaka","Kent Fujiwara"],"year":2025,"month":4,"url":"https://ojs.aaai.org/index.php/AAAI/article/view/33044","survey":false,"survey_abbr":"","model":true,"model_abbr":"ReMoGPT","dataset":false,"dataset_abbr":"","submission":"AAAI","submission_year":"2025","page":"","repo":"","backbone_tags":"VQ-VAE","approach_tags":"Multi-Task, Retrieval, Fine-grained"},{"arxiv_id":"non-arxiv8","title":"Semantically Consistent Text-to-Motion with Unsupervised Styles","abstract":"Text-to-stylized human motion generation leverages text descriptions for motion generation with fine-grained style control with respect to a reference motion. However, existing approaches typically rely on supervised style learning with labeled datasets, constraining their adaptability and generalization for effective diverse style control. Additionally, they have not fully explored the temporal correlations between motion, textual descriptions, and style, making it challenging to generate semantically consistent motion with precise style alignment. To address these limitations, we introduce a novel method that integrates unsupervised style from arbitrary references into a text-driven diffusion model to generate semantically consistent stylized human motion. The core innovation lies in leveraging text as a mediator to capture the temporal correspondences between motion and style, enabling the seamless integration of temporally dynamic style into motion features. Specifically, we first train a diffusion model on a text-motion dataset to capture the correlation between motion and text semantics. A style adapter then extracts temporally dynamic style features from reference motions and integrates a novel Semantic-Aware Style Injection (SASI) module to infuse these features into the diffusion model. The SASI module computes the semantic correlation between motion and style features based on texts, selectively incorporating style features that align with motion content, ensuring semantic consistency and precise style alignment. Our style adapter does not require a labeled style dataset for training, enhancing adaptability and generalization of style control. Extensive evaluations show that our method outperforms previous approaches in terms of semantic consistency and style expressivity.","authors":["Linjun Wu","Xiangjun Tang","Jingyuan Cong","He Wang","Bo Hu","Xu Gong","Songnan Li","Yuchen Liao","Yiqian Wu","Chen Liu","Xiaogang Jin"],"year":2025,"month":8,"url":"https://dl.acm.org/doi/10.1145/3721238.3730641","survey":false,"survey_abbr":"","model":true,"model_abbr":"SASI","dataset":false,"dataset_abbr":"","submission":"SIGGRAPH","submission_year":"2025","page":"https://fivezerojun.github.io/stylization.github.io/","repo":"","backbone_tags":"UNet, CNN, Transformer, Diffusion","approach_tags":""},{"arxiv_id":"non-arxiv9","title":"SPORT: From Zero-shot Prompts to Real-time Motion Generation","abstract":"Real-time motion generation has garnered significant attention within the fields of computer animation and gaming. Existing methods typically realize motion control via isolated style or content labels, resulting in short, simply motion clips. In this paper, we propose a motion generation framework, called SPORT (from zero-Shot Prompt tO Real-Time motion generation), for generating real-time and ever-changing motions using zero-shot prompts. SPORT consists of three primary components: (1) a body-part phase autoencoder that ensures smooth transitions between diverse motions; (2) a body-part content encoder that mitigates semantic gap between texts and motions; (3) a diffusion-based decoder that accelerates the denoising process while enhancing the diversity and realism of motions. Moreover, we develop a prototype for real-time application in Unity, demonstrating that our approach effectively considering the semantic gap caused by abstract style texts and rapidly changing terrains. Through qualitative and quantitative comparisons, we show that SPORT outperforms other approaches in terms of motion quality, style diversity and inference speed.","authors":["Bin Ji","Ye Pan","Zhimeng Liu","Shuai Tan","Xiaokang Yang"],"year":2025,"month":2,"url":"https://ieeexplore.ieee.org/document/10891181/","survey":false,"survey_abbr":"","model":true,"model_abbr":"SPORT","dataset":false,"dataset_abbr":"","submission":"TVCG","submission_year":"2025","page":"","repo":"","backbone_tags":"CLIP, Transformer, Diffusion","approach_tags":"MoE, Phase"},{"arxiv_id":"non-arxiv10","title":"UniTMGE: Uniform Text-Motion Generation and Editing Model via Diffusion","abstract":"Current methods have shown promising results in applying diffusion models to motion generation given text input. However, these methods are limited to unimodal inputs and outputs, restricted to motion generation alone, and lacking multimodal control capabilities. To address these issues, we introduce UniTMGE, a text-motion multimodal generation and editing framework based on diffusion. UniTMGE overcomes single-modality limitations, enabling exceptional performance and strong generalization across multiple tasks like text-driven motion generation, motion captioning, motion completion, and multimodal motion editing. UniTMGE comprises three components: UTMV for mapping text and motion into a shared latent space using contrastive learning, a controllable diffusion model customized for the UTMV space, and MCRE for unifying multimodal conditions into CLIP representations, enabling precise multimodal control and flexible motion editing through simple linear operations. We conducted both closed-world experiments and open-world experiments using the Motion-X dataset with detailed text descriptions, with results demonstrating our model\'s effectiveness and generalizability across multiple tasks.","authors":["Ruoyu Wang","Yangfan He","Tengjiao Sun","Xiang Li","Tianyu Shi"],"year":2025,"month":4,"url":"https://ieeexplore.ieee.org/document/10943808","survey":false,"survey_abbr":"","model":true,"model_abbr":"UniTMGE","dataset":false,"dataset_abbr":"","submission":"WACV","submission_year":"2025","page":"","repo":"","backbone_tags":"CLIP, Transformer, Diffusion","approach_tags":"Multi-Task, Editing"}]');var k=function(a){return a[a.Survey=0]="Survey",a[a.Dataset=1]="Dataset",a[a.Model=2]="Model",a}({});let l=a=>a&&"string"==typeof a?a.split(",").map(a=>a.trim()):[],m=(a,b)=>{let c,d,e="";switch(b){case k.Survey:e=a.survey_abbr;break;case k.Dataset:e=a.dataset_abbr;break;case k.Model:e=a.model_abbr}return{arxiv_id:a.arxiv_id,publication:(c=a.year,d=a.month,`${c}-${d.toString().padStart(2,"0")}`),authors:a.authors,abstract:a.abstract,abbr:e,title:a.title,conference:((a,b)=>a&&""!==a.trim()?`${a} ${b}`:"-")(a.submission,a.submission_year),link:a.url,page:a.page,code:a.repo,backbone_tags:l(a.backbone_tags),approach_tags:l(a.approach_tags)}};var n=c(1215);function o(a,[b,c]){return Math.min(c,Math.max(b,a))}function p(a,b,{checkForDefaultPrevented:c=!0}={}){return function(d){if(a?.(d),!1===c||!d.defaultPrevented)return b?.(d)}}function q(a,b=[]){let c=[],d=()=>{let b=c.map(a=>h.createContext(a));return function(c){let d=c?.[a]||b;return h.useMemo(()=>({[`__scope${a}`]:{...c,[a]:d}}),[c,d])}};return d.scopeName=a,[function(b,d){let e=h.createContext(d),f=c.length;c=[...c,d];let i=b=>{let{scope:c,children:d,...i}=b,j=c?.[a]?.[f]||e,k=h.useMemo(()=>i,Object.values(i));return(0,g.jsx)(j.Provider,{value:k,children:d})};return i.displayName=b+"Provider",[i,function(c,g){let i=g?.[a]?.[f]||e,j=h.useContext(i);if(j)return j;if(void 0!==d)return d;throw Error(`\`${c}\` must be used within \`${b}\``)}]},function(...a){let b=a[0];if(1===a.length)return b;let c=()=>{let c=a.map(a=>({useScope:a(),scopeName:a.scopeName}));return function(a){let d=c.reduce((b,{useScope:c,scopeName:d})=>{let e=c(a)[`__scope${d}`];return{...b,...e}},{});return h.useMemo(()=>({[`__scope${b.scopeName}`]:d}),[d])}};return c.scopeName=b.scopeName,c}(d,...b)]}function r(a,b){if("function"==typeof a)return a(b);null!=a&&(a.current=b)}function s(...a){return b=>{let c=!1,d=a.map(a=>{let d=r(a,b);return c||"function"!=typeof d||(c=!0),d});if(c)return()=>{for(let b=0;b<d.length;b++){let c=d[b];"function"==typeof c?c():r(a[b],null)}}}}function t(...a){return h.useCallback(s(...a),a)}function u(a){let b=function(a){let b=h.forwardRef((a,b)=>{let{children:c,...d}=a;if(h.isValidElement(c)){var e;let a,f,g=(e=c,(f=(a=Object.getOwnPropertyDescriptor(e.props,"ref")?.get)&&"isReactWarning"in a&&a.isReactWarning)?e.ref:(f=(a=Object.getOwnPropertyDescriptor(e,"ref")?.get)&&"isReactWarning"in a&&a.isReactWarning)?e.props.ref:e.props.ref||e.ref),i=function(a,b){let c={...b};for(let d in b){let e=a[d],f=b[d];/^on[A-Z]/.test(d)?e&&f?c[d]=(...a)=>{let b=f(...a);return e(...a),b}:e&&(c[d]=e):"style"===d?c[d]={...e,...f}:"className"===d&&(c[d]=[e,f].filter(Boolean).join(" "))}return{...a,...c}}(d,c.props);return c.type!==h.Fragment&&(i.ref=b?s(b,g):g),h.cloneElement(c,i)}return h.Children.count(c)>1?h.Children.only(null):null});return b.displayName=`${a}.SlotClone`,b}(a),c=h.forwardRef((a,c)=>{let{children:d,...e}=a,f=h.Children.toArray(d),i=f.find(x);if(i){let a=i.props.children,d=f.map(b=>b!==i?b:h.Children.count(a)>1?h.Children.only(null):h.isValidElement(a)?a.props.children:null);return(0,g.jsx)(b,{...e,ref:c,children:h.isValidElement(a)?h.cloneElement(a,void 0,d):null})}return(0,g.jsx)(b,{...e,ref:c,children:d})});return c.displayName=`${a}.Slot`,c}"undefined"!=typeof window&&window.document&&window.document.createElement;var v=u("Slot"),w=Symbol("radix.slottable");function x(a){return h.isValidElement(a)&&"function"==typeof a.type&&"__radixId"in a.type&&a.type.__radixId===w}var y=new WeakMap;function z(a,b){if("at"in Array.prototype)return Array.prototype.at.call(a,b);let c=function(a,b){let c=a.length,d=A(b),e=d>=0?d:c+d;return e<0||e>=c?-1:e}(a,b);return -1===c?void 0:a[c]}function A(a){return a!=a||0===a?0:Math.trunc(a)}(class a extends Map{#a;constructor(a){super(a),this.#a=[...super.keys()],y.set(this,!0)}set(a,b){return y.get(this)&&(this.has(a)?this.#a[this.#a.indexOf(a)]=a:this.#a.push(a)),super.set(a,b),this}insert(a,b,c){let d,e=this.has(b),f=this.#a.length,g=A(a),h=g>=0?g:f+g,i=h<0||h>=f?-1:h;if(i===this.size||e&&i===this.size-1||-1===i)return this.set(b,c),this;let j=this.size+ +!e;g<0&&h++;let k=[...this.#a],l=!1;for(let a=h;a<j;a++)if(h===a){let f=k[a];k[a]===b&&(f=k[a+1]),e&&this.delete(b),d=this.get(f),this.set(b,c)}else{l||k[a-1]!==b||(l=!0);let c=k[l?a:a-1],e=d;d=this.get(c),this.delete(c),this.set(c,e)}return this}with(b,c,d){let e=new a(this);return e.insert(b,c,d),e}before(a){let b=this.#a.indexOf(a)-1;if(!(b<0))return this.entryAt(b)}setBefore(a,b,c){let d=this.#a.indexOf(a);return -1===d?this:this.insert(d,b,c)}after(a){let b=this.#a.indexOf(a);if(-1!==(b=-1===b||b===this.size-1?-1:b+1))return this.entryAt(b)}setAfter(a,b,c){let d=this.#a.indexOf(a);return -1===d?this:this.insert(d+1,b,c)}first(){return this.entryAt(0)}last(){return this.entryAt(-1)}clear(){return this.#a=[],super.clear()}delete(a){let b=super.delete(a);return b&&this.#a.splice(this.#a.indexOf(a),1),b}deleteAt(a){let b=this.keyAt(a);return void 0!==b&&this.delete(b)}at(a){let b=z(this.#a,a);if(void 0!==b)return this.get(b)}entryAt(a){let b=z(this.#a,a);if(void 0!==b)return[b,this.get(b)]}indexOf(a){return this.#a.indexOf(a)}keyAt(a){return z(this.#a,a)}from(a,b){let c=this.indexOf(a);if(-1===c)return;let d=c+b;return d<0&&(d=0),d>=this.size&&(d=this.size-1),this.at(d)}keyFrom(a,b){let c=this.indexOf(a);if(-1===c)return;let d=c+b;return d<0&&(d=0),d>=this.size&&(d=this.size-1),this.keyAt(d)}find(a,b){let c=0;for(let d of this){if(Reflect.apply(a,b,[d,c,this]))return d;c++}}findIndex(a,b){let c=0;for(let d of this){if(Reflect.apply(a,b,[d,c,this]))return c;c++}return -1}filter(b,c){let d=[],e=0;for(let a of this)Reflect.apply(b,c,[a,e,this])&&d.push(a),e++;return new a(d)}map(b,c){let d=[],e=0;for(let a of this)d.push([a[0],Reflect.apply(b,c,[a,e,this])]),e++;return new a(d)}reduce(...a){let[b,c]=a,d=0,e=c??this.at(0);for(let c of this)e=0===d&&1===a.length?c:Reflect.apply(b,this,[e,c,d,this]),d++;return e}reduceRight(...a){let[b,c]=a,d=c??this.at(-1);for(let c=this.size-1;c>=0;c--){let e=this.at(c);d=c===this.size-1&&1===a.length?e:Reflect.apply(b,this,[d,e,c,this])}return d}toSorted(b){return new a([...this.entries()].sort(b))}toReversed(){let b=new a;for(let a=this.size-1;a>=0;a--){let c=this.keyAt(a),d=this.get(c);b.set(c,d)}return b}toSpliced(...b){let c=[...this.entries()];return c.splice(...b),new a(c)}slice(b,c){let d=new a,e=this.size-1;if(void 0===b)return d;b<0&&(b+=this.size),void 0!==c&&c>0&&(e=c-1);for(let a=b;a<=e;a++){let b=this.keyAt(a),c=this.get(b);d.set(b,c)}return d}every(a,b){let c=0;for(let d of this){if(!Reflect.apply(a,b,[d,c,this]))return!1;c++}return!0}some(a,b){let c=0;for(let d of this){if(Reflect.apply(a,b,[d,c,this]))return!0;c++}return!1}});var B=h.createContext(void 0),C=["a","button","div","form","h2","h3","img","input","label","li","nav","ol","p","select","span","svg","ul"].reduce((a,b)=>{let c=u(`Primitive.${b}`),d=h.forwardRef((a,d)=>{let{asChild:e,...f}=a;return"undefined"!=typeof window&&(window[Symbol.for("radix-ui")]=!0),(0,g.jsx)(e?c:b,{...f,ref:d})});return d.displayName=`Primitive.${b}`,{...a,[b]:d}},{});function D(a){let b=h.useRef(a);return h.useEffect(()=>{b.current=a}),h.useMemo(()=>(...a)=>b.current?.(...a),[])}var E="dismissableLayer.update",F=h.createContext({layers:new Set,layersWithOutsidePointerEventsDisabled:new Set,branches:new Set}),G=h.forwardRef((a,b)=>{let{disableOutsidePointerEvents:c=!1,onEscapeKeyDown:d,onPointerDownOutside:f,onFocusOutside:i,onInteractOutside:j,onDismiss:k,...l}=a,m=h.useContext(F),[n,o]=h.useState(null),q=n?.ownerDocument??globalThis?.document,[,r]=h.useState({}),s=t(b,a=>o(a)),u=Array.from(m.layers),[v]=[...m.layersWithOutsidePointerEventsDisabled].slice(-1),w=u.indexOf(v),x=n?u.indexOf(n):-1,y=m.layersWithOutsidePointerEventsDisabled.size>0,z=x>=w,A=function(a,b=globalThis?.document){let c=D(a),d=h.useRef(!1),e=h.useRef(()=>{});return h.useEffect(()=>{let a=a=>{if(a.target&&!d.current){let d=function(){I("dismissableLayer.pointerDownOutside",c,f,{discrete:!0})},f={originalEvent:a};"touch"===a.pointerType?(b.removeEventListener("click",e.current),e.current=d,b.addEventListener("click",e.current,{once:!0})):d()}else b.removeEventListener("click",e.current);d.current=!1},f=window.setTimeout(()=>{b.addEventListener("pointerdown",a)},0);return()=>{window.clearTimeout(f),b.removeEventListener("pointerdown",a),b.removeEventListener("click",e.current)}},[b,c]),{onPointerDownCapture:()=>d.current=!0}}(a=>{let b=a.target,c=[...m.branches].some(a=>a.contains(b));z&&!c&&(f?.(a),j?.(a),a.defaultPrevented||k?.())},q),B=function(a,b=globalThis?.document){let c=D(a),d=h.useRef(!1);return h.useEffect(()=>{let a=a=>{a.target&&!d.current&&I("dismissableLayer.focusOutside",c,{originalEvent:a},{discrete:!1})};return b.addEventListener("focusin",a),()=>b.removeEventListener("focusin",a)},[b,c]),{onFocusCapture:()=>d.current=!0,onBlurCapture:()=>d.current=!1}}(a=>{let b=a.target;![...m.branches].some(a=>a.contains(b))&&(i?.(a),j?.(a),a.defaultPrevented||k?.())},q);return!function(a,b=globalThis?.document){let c=D(a);h.useEffect(()=>{let a=a=>{"Escape"===a.key&&c(a)};return b.addEventListener("keydown",a,{capture:!0}),()=>b.removeEventListener("keydown",a,{capture:!0})},[c,b])}(a=>{x===m.layers.size-1&&(d?.(a),!a.defaultPrevented&&k&&(a.preventDefault(),k()))},q),h.useEffect(()=>{if(n)return c&&(0===m.layersWithOutsidePointerEventsDisabled.size&&(e=q.body.style.pointerEvents,q.body.style.pointerEvents="none"),m.layersWithOutsidePointerEventsDisabled.add(n)),m.layers.add(n),H(),()=>{c&&1===m.layersWithOutsidePointerEventsDisabled.size&&(q.body.style.pointerEvents=e)}},[n,q,c,m]),h.useEffect(()=>()=>{n&&(m.layers.delete(n),m.layersWithOutsidePointerEventsDisabled.delete(n),H())},[n,m]),h.useEffect(()=>{let a=()=>r({});return document.addEventListener(E,a),()=>document.removeEventListener(E,a)},[]),(0,g.jsx)(C.div,{...l,ref:s,style:{pointerEvents:y?z?"auto":"none":void 0,...a.style},onFocusCapture:p(a.onFocusCapture,B.onFocusCapture),onBlurCapture:p(a.onBlurCapture,B.onBlurCapture),onPointerDownCapture:p(a.onPointerDownCapture,A.onPointerDownCapture)})});function H(){let a=new CustomEvent(E);document.dispatchEvent(a)}function I(a,b,c,{discrete:d}){let e=c.originalEvent.target,f=new CustomEvent(a,{bubbles:!1,cancelable:!0,detail:c});if(b&&e.addEventListener(a,b,{once:!0}),d)e&&n.flushSync(()=>e.dispatchEvent(f));else e.dispatchEvent(f)}G.displayName="DismissableLayer",h.forwardRef((a,b)=>{let c=h.useContext(F),d=h.useRef(null),e=t(b,d);return h.useEffect(()=>{let a=d.current;if(a)return c.branches.add(a),()=>{c.branches.delete(a)}},[c.branches]),(0,g.jsx)(C.div,{...a,ref:e})}).displayName="DismissableLayerBranch";var J=0;function K(){h.useEffect(()=>{let a=document.querySelectorAll("[data-radix-focus-guard]");return document.body.insertAdjacentElement("afterbegin",a[0]??L()),document.body.insertAdjacentElement("beforeend",a[1]??L()),J++,()=>{1===J&&document.querySelectorAll("[data-radix-focus-guard]").forEach(a=>a.remove()),J--}},[])}function L(){let a=document.createElement("span");return a.setAttribute("data-radix-focus-guard",""),a.tabIndex=0,a.style.outline="none",a.style.opacity="0",a.style.position="fixed",a.style.pointerEvents="none",a}var M="focusScope.autoFocusOnMount",N="focusScope.autoFocusOnUnmount",O={bubbles:!1,cancelable:!0},P=h.forwardRef((a,b)=>{let{loop:c=!1,trapped:d=!1,onMountAutoFocus:e,onUnmountAutoFocus:f,...i}=a,[j,k]=h.useState(null),l=D(e),m=D(f),n=h.useRef(null),o=t(b,a=>k(a)),p=h.useRef({paused:!1,pause(){this.paused=!0},resume(){this.paused=!1}}).current;h.useEffect(()=>{if(d){let a=function(a){if(p.paused||!j)return;let b=a.target;j.contains(b)?n.current=b:S(n.current,{select:!0})},b=function(a){if(p.paused||!j)return;let b=a.relatedTarget;null!==b&&(j.contains(b)||S(n.current,{select:!0}))};document.addEventListener("focusin",a),document.addEventListener("focusout",b);let c=new MutationObserver(function(a){if(document.activeElement===document.body)for(let b of a)b.removedNodes.length>0&&S(j)});return j&&c.observe(j,{childList:!0,subtree:!0}),()=>{document.removeEventListener("focusin",a),document.removeEventListener("focusout",b),c.disconnect()}}},[d,j,p.paused]),h.useEffect(()=>{if(j){T.add(p);let a=document.activeElement;if(!j.contains(a)){let b=new CustomEvent(M,O);j.addEventListener(M,l),j.dispatchEvent(b),b.defaultPrevented||(function(a,{select:b=!1}={}){let c=document.activeElement;for(let d of a)if(S(d,{select:b}),document.activeElement!==c)return}(Q(j).filter(a=>"A"!==a.tagName),{select:!0}),document.activeElement===a&&S(j))}return()=>{j.removeEventListener(M,l),setTimeout(()=>{let b=new CustomEvent(N,O);j.addEventListener(N,m),j.dispatchEvent(b),b.defaultPrevented||S(a??document.body,{select:!0}),j.removeEventListener(N,m),T.remove(p)},0)}}},[j,l,m,p]);let q=h.useCallback(a=>{if(!c&&!d||p.paused)return;let b="Tab"===a.key&&!a.altKey&&!a.ctrlKey&&!a.metaKey,e=document.activeElement;if(b&&e){let b=a.currentTarget,[d,f]=function(a){let b=Q(a);return[R(b,a),R(b.reverse(),a)]}(b);d&&f?a.shiftKey||e!==f?a.shiftKey&&e===d&&(a.preventDefault(),c&&S(f,{select:!0})):(a.preventDefault(),c&&S(d,{select:!0})):e===b&&a.preventDefault()}},[c,d,p.paused]);return(0,g.jsx)(C.div,{tabIndex:-1,...i,ref:o,onKeyDown:q})});function Q(a){let b=[],c=document.createTreeWalker(a,NodeFilter.SHOW_ELEMENT,{acceptNode:a=>{let b="INPUT"===a.tagName&&"hidden"===a.type;return a.disabled||a.hidden||b?NodeFilter.FILTER_SKIP:a.tabIndex>=0?NodeFilter.FILTER_ACCEPT:NodeFilter.FILTER_SKIP}});for(;c.nextNode();)b.push(c.currentNode);return b}function R(a,b){for(let c of a)if(!function(a,{upTo:b}){if("hidden"===getComputedStyle(a).visibility)return!0;for(;a&&(void 0===b||a!==b);){if("none"===getComputedStyle(a).display)return!0;a=a.parentElement}return!1}(c,{upTo:b}))return c}function S(a,{select:b=!1}={}){if(a&&a.focus){var c;let d=document.activeElement;a.focus({preventScroll:!0}),a!==d&&(c=a)instanceof HTMLInputElement&&"select"in c&&b&&a.select()}}P.displayName="FocusScope";var T=function(){let a=[];return{add(b){let c=a[0];b!==c&&c?.pause(),(a=U(a,b)).unshift(b)},remove(b){a=U(a,b),a[0]?.resume()}}}();function U(a,b){let c=[...a],d=c.indexOf(b);return -1!==d&&c.splice(d,1),c}var V=globalThis?.document?h.useLayoutEffect:()=>{},W=i[" useId ".trim().toString()]||(()=>void 0),X=0;function Y(a){let[b,c]=h.useState(W());return V(()=>{a||c(a=>a??String(X++))},[a]),a||(b?`radix-${b}`:"")}let Z=["top","right","bottom","left"],$=Math.min,_=Math.max,aa=Math.round,ab=Math.floor,ac=a=>({x:a,y:a}),ad={left:"right",right:"left",bottom:"top",top:"bottom"},ae={start:"end",end:"start"};function af(a,b){return"function"==typeof a?a(b):a}function ag(a){return a.split("-")[0]}function ah(a){return a.split("-")[1]}function ai(a){return"x"===a?"y":"x"}function aj(a){return"y"===a?"height":"width"}let ak=new Set(["top","bottom"]);function al(a){return ak.has(ag(a))?"y":"x"}function am(a){return a.replace(/start|end/g,a=>ae[a])}let an=["left","right"],ao=["right","left"],ap=["top","bottom"],aq=["bottom","top"];function ar(a){return a.replace(/left|right|bottom|top/g,a=>ad[a])}function as(a){return"number"!=typeof a?{top:0,right:0,bottom:0,left:0,...a}:{top:a,right:a,bottom:a,left:a}}function at(a){let{x:b,y:c,width:d,height:e}=a;return{width:d,height:e,top:c,left:b,right:b+d,bottom:c+e,x:b,y:c}}function au(a,b,c){let d,{reference:e,floating:f}=a,g=al(b),h=ai(al(b)),i=aj(h),j=ag(b),k="y"===g,l=e.x+e.width/2-f.width/2,m=e.y+e.height/2-f.height/2,n=e[i]/2-f[i]/2;switch(j){case"top":d={x:l,y:e.y-f.height};break;case"bottom":d={x:l,y:e.y+e.height};break;case"right":d={x:e.x+e.width,y:m};break;case"left":d={x:e.x-f.width,y:m};break;default:d={x:e.x,y:e.y}}switch(ah(b)){case"start":d[h]-=n*(c&&k?-1:1);break;case"end":d[h]+=n*(c&&k?-1:1)}return d}let av=async(a,b,c)=>{let{placement:d="bottom",strategy:e="absolute",middleware:f=[],platform:g}=c,h=f.filter(Boolean),i=await (null==g.isRTL?void 0:g.isRTL(b)),j=await g.getElementRects({reference:a,floating:b,strategy:e}),{x:k,y:l}=au(j,d,i),m=d,n={},o=0;for(let c=0;c<h.length;c++){let{name:f,fn:p}=h[c],{x:q,y:r,data:s,reset:t}=await p({x:k,y:l,initialPlacement:d,placement:m,strategy:e,middlewareData:n,rects:j,platform:g,elements:{reference:a,floating:b}});k=null!=q?q:k,l=null!=r?r:l,n={...n,[f]:{...n[f],...s}},t&&o<=50&&(o++,"object"==typeof t&&(t.placement&&(m=t.placement),t.rects&&(j=!0===t.rects?await g.getElementRects({reference:a,floating:b,strategy:e}):t.rects),{x:k,y:l}=au(j,m,i)),c=-1)}return{x:k,y:l,placement:m,strategy:e,middlewareData:n}};async function aw(a,b){var c;void 0===b&&(b={});let{x:d,y:e,platform:f,rects:g,elements:h,strategy:i}=a,{boundary:j="clippingAncestors",rootBoundary:k="viewport",elementContext:l="floating",altBoundary:m=!1,padding:n=0}=af(b,a),o=as(n),p=h[m?"floating"===l?"reference":"floating":l],q=at(await f.getClippingRect({element:null==(c=await (null==f.isElement?void 0:f.isElement(p)))||c?p:p.contextElement||await (null==f.getDocumentElement?void 0:f.getDocumentElement(h.floating)),boundary:j,rootBoundary:k,strategy:i})),r="floating"===l?{x:d,y:e,width:g.floating.width,height:g.floating.height}:g.reference,s=await (null==f.getOffsetParent?void 0:f.getOffsetParent(h.floating)),t=await (null==f.isElement?void 0:f.isElement(s))&&await (null==f.getScale?void 0:f.getScale(s))||{x:1,y:1},u=at(f.convertOffsetParentRelativeRectToViewportRelativeRect?await f.convertOffsetParentRelativeRectToViewportRelativeRect({elements:h,rect:r,offsetParent:s,strategy:i}):r);return{top:(q.top-u.top+o.top)/t.y,bottom:(u.bottom-q.bottom+o.bottom)/t.y,left:(q.left-u.left+o.left)/t.x,right:(u.right-q.right+o.right)/t.x}}function ax(a,b){return{top:a.top-b.height,right:a.right-b.width,bottom:a.bottom-b.height,left:a.left-b.width}}function ay(a){return Z.some(b=>a[b]>=0)}let az=new Set(["left","top"]);async function aA(a,b){let{placement:c,platform:d,elements:e}=a,f=await (null==d.isRTL?void 0:d.isRTL(e.floating)),g=ag(c),h=ah(c),i="y"===al(c),j=az.has(g)?-1:1,k=f&&i?-1:1,l=af(b,a),{mainAxis:m,crossAxis:n,alignmentAxis:o}="number"==typeof l?{mainAxis:l,crossAxis:0,alignmentAxis:null}:{mainAxis:l.mainAxis||0,crossAxis:l.crossAxis||0,alignmentAxis:l.alignmentAxis};return h&&"number"==typeof o&&(n="end"===h?-1*o:o),i?{x:n*k,y:m*j}:{x:m*j,y:n*k}}function aB(){return"undefined"!=typeof window}function aC(a){return aF(a)?(a.nodeName||"").toLowerCase():"#document"}function aD(a){var b;return(null==a||null==(b=a.ownerDocument)?void 0:b.defaultView)||window}function aE(a){var b;return null==(b=(aF(a)?a.ownerDocument:a.document)||window.document)?void 0:b.documentElement}function aF(a){return!!aB()&&(a instanceof Node||a instanceof aD(a).Node)}function aG(a){return!!aB()&&(a instanceof Element||a instanceof aD(a).Element)}function aH(a){return!!aB()&&(a instanceof HTMLElement||a instanceof aD(a).HTMLElement)}function aI(a){return!!aB()&&"undefined"!=typeof ShadowRoot&&(a instanceof ShadowRoot||a instanceof aD(a).ShadowRoot)}let aJ=new Set(["inline","contents"]);function aK(a){let{overflow:b,overflowX:c,overflowY:d,display:e}=aV(a);return/auto|scroll|overlay|hidden|clip/.test(b+d+c)&&!aJ.has(e)}let aL=new Set(["table","td","th"]),aM=[":popover-open",":modal"];function aN(a){return aM.some(b=>{try{return a.matches(b)}catch(a){return!1}})}let aO=["transform","translate","scale","rotate","perspective"],aP=["transform","translate","scale","rotate","perspective","filter"],aQ=["paint","layout","strict","content"];function aR(a){let b=aS(),c=aG(a)?aV(a):a;return aO.some(a=>!!c[a]&&"none"!==c[a])||!!c.containerType&&"normal"!==c.containerType||!b&&!!c.backdropFilter&&"none"!==c.backdropFilter||!b&&!!c.filter&&"none"!==c.filter||aP.some(a=>(c.willChange||"").includes(a))||aQ.some(a=>(c.contain||"").includes(a))}function aS(){return"undefined"!=typeof CSS&&!!CSS.supports&&CSS.supports("-webkit-backdrop-filter","none")}let aT=new Set(["html","body","#document"]);function aU(a){return aT.has(aC(a))}function aV(a){return aD(a).getComputedStyle(a)}function aW(a){return aG(a)?{scrollLeft:a.scrollLeft,scrollTop:a.scrollTop}:{scrollLeft:a.scrollX,scrollTop:a.scrollY}}function aX(a){if("html"===aC(a))return a;let b=a.assignedSlot||a.parentNode||aI(a)&&a.host||aE(a);return aI(b)?b.host:b}function aY(a,b,c){var d;void 0===b&&(b=[]),void 0===c&&(c=!0);let e=function a(b){let c=aX(b);return aU(c)?b.ownerDocument?b.ownerDocument.body:b.body:aH(c)&&aK(c)?c:a(c)}(a),f=e===(null==(d=a.ownerDocument)?void 0:d.body),g=aD(e);if(f){let a=aZ(g);return b.concat(g,g.visualViewport||[],aK(e)?e:[],a&&c?aY(a):[])}return b.concat(e,aY(e,[],c))}function aZ(a){return a.parent&&Object.getPrototypeOf(a.parent)?a.frameElement:null}function a$(a){let b=aV(a),c=parseFloat(b.width)||0,d=parseFloat(b.height)||0,e=aH(a),f=e?a.offsetWidth:c,g=e?a.offsetHeight:d,h=aa(c)!==f||aa(d)!==g;return h&&(c=f,d=g),{width:c,height:d,$:h}}function a_(a){return aG(a)?a:a.contextElement}function a0(a){let b=a_(a);if(!aH(b))return ac(1);let c=b.getBoundingClientRect(),{width:d,height:e,$:f}=a$(b),g=(f?aa(c.width):c.width)/d,h=(f?aa(c.height):c.height)/e;return g&&Number.isFinite(g)||(g=1),h&&Number.isFinite(h)||(h=1),{x:g,y:h}}let a1=ac(0);function a2(a){let b=aD(a);return aS()&&b.visualViewport?{x:b.visualViewport.offsetLeft,y:b.visualViewport.offsetTop}:a1}function a3(a,b,c,d){var e;void 0===b&&(b=!1),void 0===c&&(c=!1);let f=a.getBoundingClientRect(),g=a_(a),h=ac(1);b&&(d?aG(d)&&(h=a0(d)):h=a0(a));let i=(void 0===(e=c)&&(e=!1),d&&(!e||d===aD(g))&&e)?a2(g):ac(0),j=(f.left+i.x)/h.x,k=(f.top+i.y)/h.y,l=f.width/h.x,m=f.height/h.y;if(g){let a=aD(g),b=d&&aG(d)?aD(d):d,c=a,e=aZ(c);for(;e&&d&&b!==c;){let a=a0(e),b=e.getBoundingClientRect(),d=aV(e),f=b.left+(e.clientLeft+parseFloat(d.paddingLeft))*a.x,g=b.top+(e.clientTop+parseFloat(d.paddingTop))*a.y;j*=a.x,k*=a.y,l*=a.x,m*=a.y,j+=f,k+=g,e=aZ(c=aD(e))}}return at({width:l,height:m,x:j,y:k})}function a4(a,b){let c=aW(a).scrollLeft;return b?b.left+c:a3(aE(a)).left+c}function a5(a,b,c){void 0===c&&(c=!1);let d=a.getBoundingClientRect();return{x:d.left+b.scrollLeft-(c?0:a4(a,d)),y:d.top+b.scrollTop}}let a6=new Set(["absolute","fixed"]);function a7(a,b,c){let d;if("viewport"===b)d=function(a,b){let c=aD(a),d=aE(a),e=c.visualViewport,f=d.clientWidth,g=d.clientHeight,h=0,i=0;if(e){f=e.width,g=e.height;let a=aS();(!a||a&&"fixed"===b)&&(h=e.offsetLeft,i=e.offsetTop)}return{width:f,height:g,x:h,y:i}}(a,c);else if("document"===b)d=function(a){let b=aE(a),c=aW(a),d=a.ownerDocument.body,e=_(b.scrollWidth,b.clientWidth,d.scrollWidth,d.clientWidth),f=_(b.scrollHeight,b.clientHeight,d.scrollHeight,d.clientHeight),g=-c.scrollLeft+a4(a),h=-c.scrollTop;return"rtl"===aV(d).direction&&(g+=_(b.clientWidth,d.clientWidth)-e),{width:e,height:f,x:g,y:h}}(aE(a));else if(aG(b))d=function(a,b){let c=a3(a,!0,"fixed"===b),d=c.top+a.clientTop,e=c.left+a.clientLeft,f=aH(a)?a0(a):ac(1),g=a.clientWidth*f.x,h=a.clientHeight*f.y;return{width:g,height:h,x:e*f.x,y:d*f.y}}(b,c);else{let c=a2(a);d={x:b.x-c.x,y:b.y-c.y,width:b.width,height:b.height}}return at(d)}function a8(a){return"static"===aV(a).position}function a9(a,b){if(!aH(a)||"fixed"===aV(a).position)return null;if(b)return b(a);let c=a.offsetParent;return aE(a)===c&&(c=c.ownerDocument.body),c}function ba(a,b){var c;let d=aD(a);if(aN(a))return d;if(!aH(a)){let b=aX(a);for(;b&&!aU(b);){if(aG(b)&&!a8(b))return b;b=aX(b)}return d}let e=a9(a,b);for(;e&&(c=e,aL.has(aC(c)))&&a8(e);)e=a9(e,b);return e&&aU(e)&&a8(e)&&!aR(e)?d:e||function(a){let b=aX(a);for(;aH(b)&&!aU(b);){if(aR(b))return b;if(aN(b))break;b=aX(b)}return null}(a)||d}let bb=async function(a){let b=this.getOffsetParent||ba,c=this.getDimensions,d=await c(a.floating);return{reference:function(a,b,c){let d=aH(b),e=aE(b),f="fixed"===c,g=a3(a,!0,f,b),h={scrollLeft:0,scrollTop:0},i=ac(0);if(d||!d&&!f)if(("body"!==aC(b)||aK(e))&&(h=aW(b)),d){let a=a3(b,!0,f,b);i.x=a.x+b.clientLeft,i.y=a.y+b.clientTop}else e&&(i.x=a4(e));f&&!d&&e&&(i.x=a4(e));let j=!e||d||f?ac(0):a5(e,h);return{x:g.left+h.scrollLeft-i.x-j.x,y:g.top+h.scrollTop-i.y-j.y,width:g.width,height:g.height}}(a.reference,await b(a.floating),a.strategy),floating:{x:0,y:0,width:d.width,height:d.height}}},bc={convertOffsetParentRelativeRectToViewportRelativeRect:function(a){let{elements:b,rect:c,offsetParent:d,strategy:e}=a,f="fixed"===e,g=aE(d),h=!!b&&aN(b.floating);if(d===g||h&&f)return c;let i={scrollLeft:0,scrollTop:0},j=ac(1),k=ac(0),l=aH(d);if((l||!l&&!f)&&(("body"!==aC(d)||aK(g))&&(i=aW(d)),aH(d))){let a=a3(d);j=a0(d),k.x=a.x+d.clientLeft,k.y=a.y+d.clientTop}let m=!g||l||f?ac(0):a5(g,i,!0);return{width:c.width*j.x,height:c.height*j.y,x:c.x*j.x-i.scrollLeft*j.x+k.x+m.x,y:c.y*j.y-i.scrollTop*j.y+k.y+m.y}},getDocumentElement:aE,getClippingRect:function(a){let{element:b,boundary:c,rootBoundary:d,strategy:e}=a,f=[..."clippingAncestors"===c?aN(b)?[]:function(a,b){let c=b.get(a);if(c)return c;let d=aY(a,[],!1).filter(a=>aG(a)&&"body"!==aC(a)),e=null,f="fixed"===aV(a).position,g=f?aX(a):a;for(;aG(g)&&!aU(g);){let b=aV(g),c=aR(g);c||"fixed"!==b.position||(e=null),(f?!c&&!e:!c&&"static"===b.position&&!!e&&a6.has(e.position)||aK(g)&&!c&&function a(b,c){let d=aX(b);return!(d===c||!aG(d)||aU(d))&&("fixed"===aV(d).position||a(d,c))}(a,g))?d=d.filter(a=>a!==g):e=b,g=aX(g)}return b.set(a,d),d}(b,this._c):[].concat(c),d],g=f[0],h=f.reduce((a,c)=>{let d=a7(b,c,e);return a.top=_(d.top,a.top),a.right=$(d.right,a.right),a.bottom=$(d.bottom,a.bottom),a.left=_(d.left,a.left),a},a7(b,g,e));return{width:h.right-h.left,height:h.bottom-h.top,x:h.left,y:h.top}},getOffsetParent:ba,getElementRects:bb,getClientRects:function(a){return Array.from(a.getClientRects())},getDimensions:function(a){let{width:b,height:c}=a$(a);return{width:b,height:c}},getScale:a0,isElement:aG,isRTL:function(a){return"rtl"===aV(a).direction}};function bd(a,b){return a.x===b.x&&a.y===b.y&&a.width===b.width&&a.height===b.height}let be=a=>({name:"arrow",options:a,async fn(b){let{x:c,y:d,placement:e,rects:f,platform:g,elements:h,middlewareData:i}=b,{element:j,padding:k=0}=af(a,b)||{};if(null==j)return{};let l=as(k),m={x:c,y:d},n=ai(al(e)),o=aj(n),p=await g.getDimensions(j),q="y"===n,r=q?"clientHeight":"clientWidth",s=f.reference[o]+f.reference[n]-m[n]-f.floating[o],t=m[n]-f.reference[n],u=await (null==g.getOffsetParent?void 0:g.getOffsetParent(j)),v=u?u[r]:0;v&&await (null==g.isElement?void 0:g.isElement(u))||(v=h.floating[r]||f.floating[o]);let w=v/2-p[o]/2-1,x=$(l[q?"top":"left"],w),y=$(l[q?"bottom":"right"],w),z=v-p[o]-y,A=v/2-p[o]/2+(s/2-t/2),B=_(x,$(A,z)),C=!i.arrow&&null!=ah(e)&&A!==B&&f.reference[o]/2-(A<x?x:y)-p[o]/2<0,D=C?A<x?A-x:A-z:0;return{[n]:m[n]+D,data:{[n]:B,centerOffset:A-B-D,...C&&{alignmentOffset:D}},reset:C}}});var bf="undefined"!=typeof document?h.useLayoutEffect:function(){};function bg(a,b){let c,d,e;if(a===b)return!0;if(typeof a!=typeof b)return!1;if("function"==typeof a&&a.toString()===b.toString())return!0;if(a&&b&&"object"==typeof a){if(Array.isArray(a)){if((c=a.length)!==b.length)return!1;for(d=c;0!=d--;)if(!bg(a[d],b[d]))return!1;return!0}if((c=(e=Object.keys(a)).length)!==Object.keys(b).length)return!1;for(d=c;0!=d--;)if(!({}).hasOwnProperty.call(b,e[d]))return!1;for(d=c;0!=d--;){let c=e[d];if(("_owner"!==c||!a.$$typeof)&&!bg(a[c],b[c]))return!1}return!0}return a!=a&&b!=b}function bh(a){return"undefined"==typeof window?1:(a.ownerDocument.defaultView||window).devicePixelRatio||1}function bi(a,b){let c=bh(a);return Math.round(b*c)/c}function bj(a){let b=h.useRef(a);return bf(()=>{b.current=a}),b}var bk=h.forwardRef((a,b)=>{let{children:c,width:d=10,height:e=5,...f}=a;return(0,g.jsx)(C.svg,{...f,ref:b,width:d,height:e,viewBox:"0 0 30 10",preserveAspectRatio:"none",children:a.asChild?c:(0,g.jsx)("polygon",{points:"0,0 30,0 15,10"})})});bk.displayName="Arrow";var bl="Popper",[bm,bn]=q(bl),[bo,bp]=bm(bl),bq=a=>{let{__scopePopper:b,children:c}=a,[d,e]=h.useState(null);return(0,g.jsx)(bo,{scope:b,anchor:d,onAnchorChange:e,children:c})};bq.displayName=bl;var br="PopperAnchor",bs=h.forwardRef((a,b)=>{let{__scopePopper:c,virtualRef:d,...e}=a,f=bp(br,c),i=h.useRef(null),j=t(b,i),k=h.useRef(null);return h.useEffect(()=>{let a=k.current;k.current=d?.current||i.current,a!==k.current&&f.onAnchorChange(k.current)}),d?null:(0,g.jsx)(C.div,{...e,ref:j})});bs.displayName=br;var bt="PopperContent",[bu,bv]=bm(bt),bw=h.forwardRef((a,b)=>{let{__scopePopper:c,side:d="bottom",sideOffset:e=0,align:f="center",alignOffset:i=0,arrowPadding:j=0,avoidCollisions:k=!0,collisionBoundary:l=[],collisionPadding:m=0,sticky:o="partial",hideWhenDetached:p=!1,updatePositionStrategy:q="optimized",onPlaced:r,...s}=a,u=bp(bt,c),[v,w]=h.useState(null),x=t(b,a=>w(a)),[y,z]=h.useState(null),A=function(a){let[b,c]=h.useState(void 0);return V(()=>{if(a){c({width:a.offsetWidth,height:a.offsetHeight});let b=new ResizeObserver(b=>{let d,e;if(!Array.isArray(b)||!b.length)return;let f=b[0];if("borderBoxSize"in f){let a=f.borderBoxSize,b=Array.isArray(a)?a[0]:a;d=b.inlineSize,e=b.blockSize}else d=a.offsetWidth,e=a.offsetHeight;c({width:d,height:e})});return b.observe(a,{box:"border-box"}),()=>b.unobserve(a)}c(void 0)},[a]),b}(y),B=A?.width??0,E=A?.height??0,F="number"==typeof m?m:{top:0,right:0,bottom:0,left:0,...m},G=Array.isArray(l)?l:[l],H=G.length>0,I={padding:F,boundary:G.filter(bA),altBoundary:H},{refs:J,floatingStyles:K,placement:L,isPositioned:M,middlewareData:N}=function(a){void 0===a&&(a={});let{placement:b="bottom",strategy:c="absolute",middleware:d=[],platform:e,elements:{reference:f,floating:g}={},transform:i=!0,whileElementsMounted:j,open:k}=a,[l,m]=h.useState({x:0,y:0,strategy:c,placement:b,middlewareData:{},isPositioned:!1}),[o,p]=h.useState(d);bg(o,d)||p(d);let[q,r]=h.useState(null),[s,t]=h.useState(null),u=h.useCallback(a=>{a!==y.current&&(y.current=a,r(a))},[]),v=h.useCallback(a=>{a!==z.current&&(z.current=a,t(a))},[]),w=f||q,x=g||s,y=h.useRef(null),z=h.useRef(null),A=h.useRef(l),B=null!=j,C=bj(j),D=bj(e),E=bj(k),F=h.useCallback(()=>{if(!y.current||!z.current)return;let a={placement:b,strategy:c,middleware:o};D.current&&(a.platform=D.current),((a,b,c)=>{let d=new Map,e={platform:bc,...c},f={...e.platform,_c:d};return av(a,b,{...e,platform:f})})(y.current,z.current,a).then(a=>{let b={...a,isPositioned:!1!==E.current};G.current&&!bg(A.current,b)&&(A.current=b,n.flushSync(()=>{m(b)}))})},[o,b,c,D,E]);bf(()=>{!1===k&&A.current.isPositioned&&(A.current.isPositioned=!1,m(a=>({...a,isPositioned:!1})))},[k]);let G=h.useRef(!1);bf(()=>(G.current=!0,()=>{G.current=!1}),[]),bf(()=>{if(w&&(y.current=w),x&&(z.current=x),w&&x){if(C.current)return C.current(w,x,F);F()}},[w,x,F,C,B]);let H=h.useMemo(()=>({reference:y,floating:z,setReference:u,setFloating:v}),[u,v]),I=h.useMemo(()=>({reference:w,floating:x}),[w,x]),J=h.useMemo(()=>{let a={position:c,left:0,top:0};if(!I.floating)return a;let b=bi(I.floating,l.x),d=bi(I.floating,l.y);return i?{...a,transform:"translate("+b+"px, "+d+"px)",...bh(I.floating)>=1.5&&{willChange:"transform"}}:{position:c,left:b,top:d}},[c,i,I.floating,l.x,l.y]);return h.useMemo(()=>({...l,update:F,refs:H,elements:I,floatingStyles:J}),[l,F,H,I,J])}({strategy:"fixed",placement:d+("center"!==f?"-"+f:""),whileElementsMounted:(...a)=>(function(a,b,c,d){let e;void 0===d&&(d={});let{ancestorScroll:f=!0,ancestorResize:g=!0,elementResize:h="function"==typeof ResizeObserver,layoutShift:i="function"==typeof IntersectionObserver,animationFrame:j=!1}=d,k=a_(a),l=f||g?[...k?aY(k):[],...aY(b)]:[];l.forEach(a=>{f&&a.addEventListener("scroll",c,{passive:!0}),g&&a.addEventListener("resize",c)});let m=k&&i?function(a,b){let c,d=null,e=aE(a);function f(){var a;clearTimeout(c),null==(a=d)||a.disconnect(),d=null}return!function g(h,i){void 0===h&&(h=!1),void 0===i&&(i=1),f();let j=a.getBoundingClientRect(),{left:k,top:l,width:m,height:n}=j;if(h||b(),!m||!n)return;let o=ab(l),p=ab(e.clientWidth-(k+m)),q={rootMargin:-o+"px "+-p+"px "+-ab(e.clientHeight-(l+n))+"px "+-ab(k)+"px",threshold:_(0,$(1,i))||1},r=!0;function s(b){let d=b[0].intersectionRatio;if(d!==i){if(!r)return g();d?g(!1,d):c=setTimeout(()=>{g(!1,1e-7)},1e3)}1!==d||bd(j,a.getBoundingClientRect())||g(),r=!1}try{d=new IntersectionObserver(s,{...q,root:e.ownerDocument})}catch(a){d=new IntersectionObserver(s,q)}d.observe(a)}(!0),f}(k,c):null,n=-1,o=null;h&&(o=new ResizeObserver(a=>{let[d]=a;d&&d.target===k&&o&&(o.unobserve(b),cancelAnimationFrame(n),n=requestAnimationFrame(()=>{var a;null==(a=o)||a.observe(b)})),c()}),k&&!j&&o.observe(k),o.observe(b));let p=j?a3(a):null;return j&&function b(){let d=a3(a);p&&!bd(p,d)&&c(),p=d,e=requestAnimationFrame(b)}(),c(),()=>{var a;l.forEach(a=>{f&&a.removeEventListener("scroll",c),g&&a.removeEventListener("resize",c)}),null==m||m(),null==(a=o)||a.disconnect(),o=null,j&&cancelAnimationFrame(e)}})(...a,{animationFrame:"always"===q}),elements:{reference:u.anchor},middleware:[((a,b)=>({...function(a){return void 0===a&&(a=0),{name:"offset",options:a,async fn(b){var c,d;let{x:e,y:f,placement:g,middlewareData:h}=b,i=await aA(b,a);return g===(null==(c=h.offset)?void 0:c.placement)&&null!=(d=h.arrow)&&d.alignmentOffset?{}:{x:e+i.x,y:f+i.y,data:{...i,placement:g}}}}}(a),options:[a,b]}))({mainAxis:e+E,alignmentAxis:i}),k&&((a,b)=>({...function(a){return void 0===a&&(a={}),{name:"shift",options:a,async fn(b){let{x:c,y:d,placement:e}=b,{mainAxis:f=!0,crossAxis:g=!1,limiter:h={fn:a=>{let{x:b,y:c}=a;return{x:b,y:c}}},...i}=af(a,b),j={x:c,y:d},k=await aw(b,i),l=al(ag(e)),m=ai(l),n=j[m],o=j[l];if(f){let a="y"===m?"top":"left",b="y"===m?"bottom":"right",c=n+k[a],d=n-k[b];n=_(c,$(n,d))}if(g){let a="y"===l?"top":"left",b="y"===l?"bottom":"right",c=o+k[a],d=o-k[b];o=_(c,$(o,d))}let p=h.fn({...b,[m]:n,[l]:o});return{...p,data:{x:p.x-c,y:p.y-d,enabled:{[m]:f,[l]:g}}}}}}(a),options:[a,b]}))({mainAxis:!0,crossAxis:!1,limiter:"partial"===o?((a,b)=>({...function(a){return void 0===a&&(a={}),{options:a,fn(b){let{x:c,y:d,placement:e,rects:f,middlewareData:g}=b,{offset:h=0,mainAxis:i=!0,crossAxis:j=!0}=af(a,b),k={x:c,y:d},l=al(e),m=ai(l),n=k[m],o=k[l],p=af(h,b),q="number"==typeof p?{mainAxis:p,crossAxis:0}:{mainAxis:0,crossAxis:0,...p};if(i){let a="y"===m?"height":"width",b=f.reference[m]-f.floating[a]+q.mainAxis,c=f.reference[m]+f.reference[a]-q.mainAxis;n<b?n=b:n>c&&(n=c)}if(j){var r,s;let a="y"===m?"width":"height",b=az.has(ag(e)),c=f.reference[l]-f.floating[a]+(b&&(null==(r=g.offset)?void 0:r[l])||0)+(b?0:q.crossAxis),d=f.reference[l]+f.reference[a]+(b?0:(null==(s=g.offset)?void 0:s[l])||0)-(b?q.crossAxis:0);o<c?o=c:o>d&&(o=d)}return{[m]:n,[l]:o}}}}(a),options:[a,b]}))():void 0,...I}),k&&((a,b)=>({...function(a){return void 0===a&&(a={}),{name:"flip",options:a,async fn(b){var c,d,e,f,g;let{placement:h,middlewareData:i,rects:j,initialPlacement:k,platform:l,elements:m}=b,{mainAxis:n=!0,crossAxis:o=!0,fallbackPlacements:p,fallbackStrategy:q="bestFit",fallbackAxisSideDirection:r="none",flipAlignment:s=!0,...t}=af(a,b);if(null!=(c=i.arrow)&&c.alignmentOffset)return{};let u=ag(h),v=al(k),w=ag(k)===k,x=await (null==l.isRTL?void 0:l.isRTL(m.floating)),y=p||(w||!s?[ar(k)]:function(a){let b=ar(a);return[am(a),b,am(b)]}(k)),z="none"!==r;!p&&z&&y.push(...function(a,b,c,d){let e=ah(a),f=function(a,b,c){switch(a){case"top":case"bottom":if(c)return b?ao:an;return b?an:ao;case"left":case"right":return b?ap:aq;default:return[]}}(ag(a),"start"===c,d);return e&&(f=f.map(a=>a+"-"+e),b&&(f=f.concat(f.map(am)))),f}(k,s,r,x));let A=[k,...y],B=await aw(b,t),C=[],D=(null==(d=i.flip)?void 0:d.overflows)||[];if(n&&C.push(B[u]),o){let a=function(a,b,c){void 0===c&&(c=!1);let d=ah(a),e=ai(al(a)),f=aj(e),g="x"===e?d===(c?"end":"start")?"right":"left":"start"===d?"bottom":"top";return b.reference[f]>b.floating[f]&&(g=ar(g)),[g,ar(g)]}(h,j,x);C.push(B[a[0]],B[a[1]])}if(D=[...D,{placement:h,overflows:C}],!C.every(a=>a<=0)){let a=((null==(e=i.flip)?void 0:e.index)||0)+1,b=A[a];if(b&&("alignment"!==o||v===al(b)||D.every(a=>al(a.placement)!==v||a.overflows[0]>0)))return{data:{index:a,overflows:D},reset:{placement:b}};let c=null==(f=D.filter(a=>a.overflows[0]<=0).sort((a,b)=>a.overflows[1]-b.overflows[1])[0])?void 0:f.placement;if(!c)switch(q){case"bestFit":{let a=null==(g=D.filter(a=>{if(z){let b=al(a.placement);return b===v||"y"===b}return!0}).map(a=>[a.placement,a.overflows.filter(a=>a>0).reduce((a,b)=>a+b,0)]).sort((a,b)=>a[1]-b[1])[0])?void 0:g[0];a&&(c=a);break}case"initialPlacement":c=k}if(h!==c)return{reset:{placement:c}}}return{}}}}(a),options:[a,b]}))({...I}),((a,b)=>({...function(a){return void 0===a&&(a={}),{name:"size",options:a,async fn(b){var c,d;let e,f,{placement:g,rects:h,platform:i,elements:j}=b,{apply:k=()=>{},...l}=af(a,b),m=await aw(b,l),n=ag(g),o=ah(g),p="y"===al(g),{width:q,height:r}=h.floating;"top"===n||"bottom"===n?(e=n,f=o===(await (null==i.isRTL?void 0:i.isRTL(j.floating))?"start":"end")?"left":"right"):(f=n,e="end"===o?"top":"bottom");let s=r-m.top-m.bottom,t=q-m.left-m.right,u=$(r-m[e],s),v=$(q-m[f],t),w=!b.middlewareData.shift,x=u,y=v;if(null!=(c=b.middlewareData.shift)&&c.enabled.x&&(y=t),null!=(d=b.middlewareData.shift)&&d.enabled.y&&(x=s),w&&!o){let a=_(m.left,0),b=_(m.right,0),c=_(m.top,0),d=_(m.bottom,0);p?y=q-2*(0!==a||0!==b?a+b:_(m.left,m.right)):x=r-2*(0!==c||0!==d?c+d:_(m.top,m.bottom))}await k({...b,availableWidth:y,availableHeight:x});let z=await i.getDimensions(j.floating);return q!==z.width||r!==z.height?{reset:{rects:!0}}:{}}}}(a),options:[a,b]}))({...I,apply:({elements:a,rects:b,availableWidth:c,availableHeight:d})=>{let{width:e,height:f}=b.reference,g=a.floating.style;g.setProperty("--radix-popper-available-width",`${c}px`),g.setProperty("--radix-popper-available-height",`${d}px`),g.setProperty("--radix-popper-anchor-width",`${e}px`),g.setProperty("--radix-popper-anchor-height",`${f}px`)}}),y&&((a,b)=>({...(a=>({name:"arrow",options:a,fn(b){let{element:c,padding:d}="function"==typeof a?a(b):a;return c&&({}).hasOwnProperty.call(c,"current")?null!=c.current?be({element:c.current,padding:d}).fn(b):{}:c?be({element:c,padding:d}).fn(b):{}}}))(a),options:[a,b]}))({element:y,padding:j}),bB({arrowWidth:B,arrowHeight:E}),p&&((a,b)=>({...function(a){return void 0===a&&(a={}),{name:"hide",options:a,async fn(b){let{rects:c}=b,{strategy:d="referenceHidden",...e}=af(a,b);switch(d){case"referenceHidden":{let a=ax(await aw(b,{...e,elementContext:"reference"}),c.reference);return{data:{referenceHiddenOffsets:a,referenceHidden:ay(a)}}}case"escaped":{let a=ax(await aw(b,{...e,altBoundary:!0}),c.floating);return{data:{escapedOffsets:a,escaped:ay(a)}}}default:return{}}}}}(a),options:[a,b]}))({strategy:"referenceHidden",...I})]}),[O,P]=bC(L),Q=D(r);V(()=>{M&&Q?.()},[M,Q]);let R=N.arrow?.x,S=N.arrow?.y,T=N.arrow?.centerOffset!==0,[U,W]=h.useState();return V(()=>{v&&W(window.getComputedStyle(v).zIndex)},[v]),(0,g.jsx)("div",{ref:J.setFloating,"data-radix-popper-content-wrapper":"",style:{...K,transform:M?K.transform:"translate(0, -200%)",minWidth:"max-content",zIndex:U,"--radix-popper-transform-origin":[N.transformOrigin?.x,N.transformOrigin?.y].join(" "),...N.hide?.referenceHidden&&{visibility:"hidden",pointerEvents:"none"}},dir:a.dir,children:(0,g.jsx)(bu,{scope:c,placedSide:O,onArrowChange:z,arrowX:R,arrowY:S,shouldHideArrow:T,children:(0,g.jsx)(C.div,{"data-side":O,"data-align":P,...s,ref:x,style:{...s.style,animation:M?void 0:"none"}})})})});bw.displayName=bt;var bx="PopperArrow",by={top:"bottom",right:"left",bottom:"top",left:"right"},bz=h.forwardRef(function(a,b){let{__scopePopper:c,...d}=a,e=bv(bx,c),f=by[e.placedSide];return(0,g.jsx)("span",{ref:e.onArrowChange,style:{position:"absolute",left:e.arrowX,top:e.arrowY,[f]:0,transformOrigin:{top:"",right:"0 0",bottom:"center 0",left:"100% 0"}[e.placedSide],transform:{top:"translateY(100%)",right:"translateY(50%) rotate(90deg) translateX(-50%)",bottom:"rotate(180deg)",left:"translateY(50%) rotate(-90deg) translateX(50%)"}[e.placedSide],visibility:e.shouldHideArrow?"hidden":void 0},children:(0,g.jsx)(bk,{...d,ref:b,style:{...d.style,display:"block"}})})});function bA(a){return null!==a}bz.displayName=bx;var bB=a=>({name:"transformOrigin",options:a,fn(b){let{placement:c,rects:d,middlewareData:e}=b,f=e.arrow?.centerOffset!==0,g=f?0:a.arrowWidth,h=f?0:a.arrowHeight,[i,j]=bC(c),k={start:"0%",center:"50%",end:"100%"}[j],l=(e.arrow?.x??0)+g/2,m=(e.arrow?.y??0)+h/2,n="",o="";return"bottom"===i?(n=f?k:`${l}px`,o=`${-h}px`):"top"===i?(n=f?k:`${l}px`,o=`${d.floating.height+h}px`):"right"===i?(n=`${-h}px`,o=f?k:`${m}px`):"left"===i&&(n=`${d.floating.width+h}px`,o=f?k:`${m}px`),{data:{x:n,y:o}}}});function bC(a){let[b,c="center"]=a.split("-");return[b,c]}var bD=h.forwardRef((a,b)=>{let{container:c,...d}=a,[e,f]=h.useState(!1);V(()=>f(!0),[]);let i=c||e&&globalThis?.document?.body;return i?n.createPortal((0,g.jsx)(C.div,{...d,ref:b}),i):null});bD.displayName="Portal";var bE=i[" useInsertionEffect ".trim().toString()]||V;function bF({prop:a,defaultProp:b,onChange:c=()=>{},caller:d}){let[e,f,g]=function({defaultProp:a,onChange:b}){let[c,d]=h.useState(a),e=h.useRef(c),f=h.useRef(b);return bE(()=>{f.current=b},[b]),h.useEffect(()=>{e.current!==c&&(f.current?.(c),e.current=c)},[c,e]),[c,d,f]}({defaultProp:b,onChange:c}),i=void 0!==a,j=i?a:e;{let b=h.useRef(void 0!==a);h.useEffect(()=>{let a=b.current;if(a!==i){let b=i?"controlled":"uncontrolled";console.warn(`${d} is changing from ${a?"controlled":"uncontrolled"} to ${b}. Components should not switch from controlled to uncontrolled (or vice versa). Decide between using a controlled or uncontrolled value for the lifetime of the component.`)}b.current=i},[i,d])}return[j,h.useCallback(b=>{if(i){let c="function"==typeof b?b(a):b;c!==a&&g.current?.(c)}else f(b)},[i,a,f,g])]}Symbol("RADIX:SYNC_STATE");var bG=Object.freeze({position:"absolute",border:0,width:1,height:1,padding:0,margin:-1,overflow:"hidden",clip:"rect(0, 0, 0, 0)",whiteSpace:"nowrap",wordWrap:"normal"});h.forwardRef((a,b)=>(0,g.jsx)(C.span,{...a,ref:b,style:{...bG,...a.style}})).displayName="VisuallyHidden";var bH=new WeakMap,bI=new WeakMap,bJ={},bK=0,bL=function(a){return a&&(a.host||bL(a.parentNode))},bM=function(a,b,c,d){var e=(Array.isArray(a)?a:[a]).map(function(a){if(b.contains(a))return a;var c=bL(a);return c&&b.contains(c)?c:(console.error("aria-hidden",a,"in not contained inside",b,". Doing nothing"),null)}).filter(function(a){return!!a});bJ[c]||(bJ[c]=new WeakMap);var f=bJ[c],g=[],h=new Set,i=new Set(e),j=function(a){!a||h.has(a)||(h.add(a),j(a.parentNode))};e.forEach(j);var k=function(a){!a||i.has(a)||Array.prototype.forEach.call(a.children,function(a){if(h.has(a))k(a);else try{var b=a.getAttribute(d),e=null!==b&&"false"!==b,i=(bH.get(a)||0)+1,j=(f.get(a)||0)+1;bH.set(a,i),f.set(a,j),g.push(a),1===i&&e&&bI.set(a,!0),1===j&&a.setAttribute(c,"true"),e||a.setAttribute(d,"true")}catch(b){console.error("aria-hidden: cannot operate on ",a,b)}})};return k(b),h.clear(),bK++,function(){g.forEach(function(a){var b=bH.get(a)-1,e=f.get(a)-1;bH.set(a,b),f.set(a,e),b||(bI.has(a)||a.removeAttribute(d),bI.delete(a)),e||a.removeAttribute(c)}),--bK||(bH=new WeakMap,bH=new WeakMap,bI=new WeakMap,bJ={})}},bN=function(a,b,c){void 0===c&&(c="data-aria-hidden");var d=Array.from(Array.isArray(a)?a:[a]),e=b||("undefined"==typeof document?null:(Array.isArray(a)?a[0]:a).ownerDocument.body);return e?(d.push.apply(d,Array.from(e.querySelectorAll("[aria-live], script"))),bM(d,e,c,"aria-hidden")):function(){return null}},bO=function(){return(bO=Object.assign||function(a){for(var b,c=1,d=arguments.length;c<d;c++)for(var e in b=arguments[c])Object.prototype.hasOwnProperty.call(b,e)&&(a[e]=b[e]);return a}).apply(this,arguments)};function bP(a,b){var c={};for(var d in a)Object.prototype.hasOwnProperty.call(a,d)&&0>b.indexOf(d)&&(c[d]=a[d]);if(null!=a&&"function"==typeof Object.getOwnPropertySymbols)for(var e=0,d=Object.getOwnPropertySymbols(a);e<d.length;e++)0>b.indexOf(d[e])&&Object.prototype.propertyIsEnumerable.call(a,d[e])&&(c[d[e]]=a[d[e]]);return c}Object.create;Object.create;var bQ=("function"==typeof SuppressedError&&SuppressedError,"right-scroll-bar-position"),bR="width-before-scroll-bar";function bS(a,b){return"function"==typeof a?a(b):a&&(a.current=b),a}var bT="undefined"!=typeof window?h.useLayoutEffect:h.useEffect,bU=new WeakMap;function bV(a){return a}var bW=function(a){void 0===a&&(a={});var b,c,d,e=(void 0===b&&(b=bV),c=[],d=!1,{read:function(){if(d)throw Error("Sidecar: could not `read` from an `assigned` medium. `read` could be used only with `useMedium`.");return c.length?c[c.length-1]:null},useMedium:function(a){var e=b(a,d);return c.push(e),function(){c=c.filter(function(a){return a!==e})}},assignSyncMedium:function(a){for(d=!0;c.length;){var b=c;c=[],b.forEach(a)}c={push:function(b){return a(b)},filter:function(){return c}}},assignMedium:function(a){d=!0;var b=[];if(c.length){var e=c;c=[],e.forEach(a),b=c}var f=function(){var c=b;b=[],c.forEach(a)},g=function(){return Promise.resolve().then(f)};g(),c={push:function(a){b.push(a),g()},filter:function(a){return b=b.filter(a),c}}}});return e.options=bO({async:!0,ssr:!1},a),e}(),bX=function(){},bY=h.forwardRef(function(a,b){var c,d,e,f,g=h.useRef(null),i=h.useState({onScrollCapture:bX,onWheelCapture:bX,onTouchMoveCapture:bX}),j=i[0],k=i[1],l=a.forwardProps,m=a.children,n=a.className,o=a.removeScrollBar,p=a.enabled,q=a.shards,r=a.sideCar,s=a.noRelative,t=a.noIsolation,u=a.inert,v=a.allowPinchZoom,w=a.as,x=a.gapMode,y=bP(a,["forwardProps","children","className","removeScrollBar","enabled","shards","sideCar","noRelative","noIsolation","inert","allowPinchZoom","as","gapMode"]),z=(c=[g,b],d=function(a){return c.forEach(function(b){return bS(b,a)})},(e=(0,h.useState)(function(){return{value:null,callback:d,facade:{get current(){return e.value},set current(value){var a=e.value;a!==value&&(e.value=value,e.callback(value,a))}}}})[0]).callback=d,f=e.facade,bT(function(){var a=bU.get(f);if(a){var b=new Set(a),d=new Set(c),e=f.current;b.forEach(function(a){d.has(a)||bS(a,null)}),d.forEach(function(a){b.has(a)||bS(a,e)})}bU.set(f,c)},[c]),f),A=bO(bO({},y),j);return h.createElement(h.Fragment,null,p&&h.createElement(r,{sideCar:bW,removeScrollBar:o,shards:q,noRelative:s,noIsolation:t,inert:u,setCallbacks:k,allowPinchZoom:!!v,lockRef:g,gapMode:x}),l?h.cloneElement(h.Children.only(m),bO(bO({},A),{ref:z})):h.createElement(void 0===w?"div":w,bO({},A,{className:n,ref:z}),m))});bY.defaultProps={enabled:!0,removeScrollBar:!0,inert:!1},bY.classNames={fullWidth:bR,zeroRight:bQ};var bZ=function(a){var b=a.sideCar,c=bP(a,["sideCar"]);if(!b)throw Error("Sidecar: please provide `sideCar` property to import the right car");var d=b.read();if(!d)throw Error("Sidecar medium not found");return h.createElement(d,bO({},c))};bZ.isSideCarExport=!0;var b$=function(){var a=0,b=null;return{add:function(d){if(0==a&&(b=function(){if(!document)return null;var a=document.createElement("style");a.type="text/css";var b=f||c.nc;return b&&a.setAttribute("nonce",b),a}())){var e,g;(e=b).styleSheet?e.styleSheet.cssText=d:e.appendChild(document.createTextNode(d)),g=b,(document.head||document.getElementsByTagName("head")[0]).appendChild(g)}a++},remove:function(){--a||!b||(b.parentNode&&b.parentNode.removeChild(b),b=null)}}},b_=function(){var a=b$();return function(b,c){h.useEffect(function(){return a.add(b),function(){a.remove()}},[b&&c])}},b0=function(){var a=b_();return function(b){return a(b.styles,b.dynamic),null}},b1={left:0,top:0,right:0,gap:0},b2=function(a){return parseInt(a||"",10)||0},b3=function(a){var b=window.getComputedStyle(document.body),c=b["padding"===a?"paddingLeft":"marginLeft"],d=b["padding"===a?"paddingTop":"marginTop"],e=b["padding"===a?"paddingRight":"marginRight"];return[b2(c),b2(d),b2(e)]},b4=function(a){if(void 0===a&&(a="margin"),"undefined"==typeof window)return b1;var b=b3(a),c=document.documentElement.clientWidth,d=window.innerWidth;return{left:b[0],top:b[1],right:b[2],gap:Math.max(0,d-c+b[2]-b[0])}},b5=b0(),b6="data-scroll-locked",b7=function(a,b,c,d){var e=a.left,f=a.top,g=a.right,h=a.gap;return void 0===c&&(c="margin"),"\n  .".concat("with-scroll-bars-hidden"," {\n   overflow: hidden ").concat(d,";\n   padding-right: ").concat(h,"px ").concat(d,";\n  }\n  body[").concat(b6,"] {\n    overflow: hidden ").concat(d,";\n    overscroll-behavior: contain;\n    ").concat([b&&"position: relative ".concat(d,";"),"margin"===c&&"\n    padding-left: ".concat(e,"px;\n    padding-top: ").concat(f,"px;\n    padding-right: ").concat(g,"px;\n    margin-left:0;\n    margin-top:0;\n    margin-right: ").concat(h,"px ").concat(d,";\n    "),"padding"===c&&"padding-right: ".concat(h,"px ").concat(d,";")].filter(Boolean).join(""),"\n  }\n  \n  .").concat(bQ," {\n    right: ").concat(h,"px ").concat(d,";\n  }\n  \n  .").concat(bR," {\n    margin-right: ").concat(h,"px ").concat(d,";\n  }\n  \n  .").concat(bQ," .").concat(bQ," {\n    right: 0 ").concat(d,";\n  }\n  \n  .").concat(bR," .").concat(bR," {\n    margin-right: 0 ").concat(d,";\n  }\n  \n  body[").concat(b6,"] {\n    ").concat("--removed-body-scroll-bar-size",": ").concat(h,"px;\n  }\n")},b8=function(){var a=parseInt(document.body.getAttribute(b6)||"0",10);return isFinite(a)?a:0},b9=function(){h.useEffect(function(){return document.body.setAttribute(b6,(b8()+1).toString()),function(){var a=b8()-1;a<=0?document.body.removeAttribute(b6):document.body.setAttribute(b6,a.toString())}},[])},ca=function(a){var b=a.noRelative,c=a.noImportant,d=a.gapMode,e=void 0===d?"margin":d;b9();var f=h.useMemo(function(){return b4(e)},[e]);return h.createElement(b5,{styles:b7(f,!b,e,c?"":"!important")})},cb=!1;if("undefined"!=typeof window)try{var cc=Object.defineProperty({},"passive",{get:function(){return cb=!0,!0}});window.addEventListener("test",cc,cc),window.removeEventListener("test",cc,cc)}catch(a){cb=!1}var cd=!!cb&&{passive:!1},ce=function(a,b){if(!(a instanceof Element))return!1;var c=window.getComputedStyle(a);return"hidden"!==c[b]&&(c.overflowY!==c.overflowX||"TEXTAREA"===a.tagName||"visible"!==c[b])},cf=function(a,b){var c=b.ownerDocument,d=b;do{if("undefined"!=typeof ShadowRoot&&d instanceof ShadowRoot&&(d=d.host),cg(a,d)){var e=ch(a,d);if(e[1]>e[2])return!0}d=d.parentNode}while(d&&d!==c.body);return!1},cg=function(a,b){return"v"===a?ce(b,"overflowY"):ce(b,"overflowX")},ch=function(a,b){return"v"===a?[b.scrollTop,b.scrollHeight,b.clientHeight]:[b.scrollLeft,b.scrollWidth,b.clientWidth]},ci=function(a,b,c,d,e){var f,g=(f=window.getComputedStyle(b).direction,"h"===a&&"rtl"===f?-1:1),h=g*d,i=c.target,j=b.contains(i),k=!1,l=h>0,m=0,n=0;do{if(!i)break;var o=ch(a,i),p=o[0],q=o[1]-o[2]-g*p;(p||q)&&cg(a,i)&&(m+=q,n+=p);var r=i.parentNode;i=r&&r.nodeType===Node.DOCUMENT_FRAGMENT_NODE?r.host:r}while(!j&&i!==document.body||j&&(b.contains(i)||b===i));return l&&(e&&1>Math.abs(m)||!e&&h>m)?k=!0:!l&&(e&&1>Math.abs(n)||!e&&-h>n)&&(k=!0),k},cj=function(a){return"changedTouches"in a?[a.changedTouches[0].clientX,a.changedTouches[0].clientY]:[0,0]},ck=function(a){return[a.deltaX,a.deltaY]},cl=function(a){return a&&"current"in a?a.current:a},cm=0,cn=[];let co=(d=function(a){var b=h.useRef([]),c=h.useRef([0,0]),d=h.useRef(),e=h.useState(cm++)[0],f=h.useState(b0)[0],g=h.useRef(a);h.useEffect(function(){g.current=a},[a]),h.useEffect(function(){if(a.inert){document.body.classList.add("block-interactivity-".concat(e));var b=(function(a,b,c){if(c||2==arguments.length)for(var d,e=0,f=b.length;e<f;e++)!d&&e in b||(d||(d=Array.prototype.slice.call(b,0,e)),d[e]=b[e]);return a.concat(d||Array.prototype.slice.call(b))})([a.lockRef.current],(a.shards||[]).map(cl),!0).filter(Boolean);return b.forEach(function(a){return a.classList.add("allow-interactivity-".concat(e))}),function(){document.body.classList.remove("block-interactivity-".concat(e)),b.forEach(function(a){return a.classList.remove("allow-interactivity-".concat(e))})}}},[a.inert,a.lockRef.current,a.shards]);var i=h.useCallback(function(a,b){if("touches"in a&&2===a.touches.length||"wheel"===a.type&&a.ctrlKey)return!g.current.allowPinchZoom;var e,f=cj(a),h=c.current,i="deltaX"in a?a.deltaX:h[0]-f[0],j="deltaY"in a?a.deltaY:h[1]-f[1],k=a.target,l=Math.abs(i)>Math.abs(j)?"h":"v";if("touches"in a&&"h"===l&&"range"===k.type)return!1;var m=cf(l,k);if(!m)return!0;if(m?e=l:(e="v"===l?"h":"v",m=cf(l,k)),!m)return!1;if(!d.current&&"changedTouches"in a&&(i||j)&&(d.current=e),!e)return!0;var n=d.current||e;return ci(n,b,a,"h"===n?i:j,!0)},[]),j=h.useCallback(function(a){if(cn.length&&cn[cn.length-1]===f){var c="deltaY"in a?ck(a):cj(a),d=b.current.filter(function(b){var d;return b.name===a.type&&(b.target===a.target||a.target===b.shadowParent)&&(d=b.delta,d[0]===c[0]&&d[1]===c[1])})[0];if(d&&d.should){a.cancelable&&a.preventDefault();return}if(!d){var e=(g.current.shards||[]).map(cl).filter(Boolean).filter(function(b){return b.contains(a.target)});(e.length>0?i(a,e[0]):!g.current.noIsolation)&&a.cancelable&&a.preventDefault()}}},[]),k=h.useCallback(function(a,c,d,e){var f={name:a,delta:c,target:d,should:e,shadowParent:function(a){for(var b=null;null!==a;)a instanceof ShadowRoot&&(b=a.host,a=a.host),a=a.parentNode;return b}(d)};b.current.push(f),setTimeout(function(){b.current=b.current.filter(function(a){return a!==f})},1)},[]),l=h.useCallback(function(a){c.current=cj(a),d.current=void 0},[]),m=h.useCallback(function(b){k(b.type,ck(b),b.target,i(b,a.lockRef.current))},[]),n=h.useCallback(function(b){k(b.type,cj(b),b.target,i(b,a.lockRef.current))},[]);h.useEffect(function(){return cn.push(f),a.setCallbacks({onScrollCapture:m,onWheelCapture:m,onTouchMoveCapture:n}),document.addEventListener("wheel",j,cd),document.addEventListener("touchmove",j,cd),document.addEventListener("touchstart",l,cd),function(){cn=cn.filter(function(a){return a!==f}),document.removeEventListener("wheel",j,cd),document.removeEventListener("touchmove",j,cd),document.removeEventListener("touchstart",l,cd)}},[]);var o=a.removeScrollBar,p=a.inert;return h.createElement(h.Fragment,null,p?h.createElement(f,{styles:"\n  .block-interactivity-".concat(e," {pointer-events: none;}\n  .allow-interactivity-").concat(e," {pointer-events: all;}\n")}):null,o?h.createElement(ca,{noRelative:a.noRelative,gapMode:a.gapMode}):null)},bW.useMedium(d),bZ);var cp=h.forwardRef(function(a,b){return h.createElement(bY,bO({},a,{ref:b,sideCar:co}))});cp.classNames=bY.classNames;var cq=[" ","Enter","ArrowUp","ArrowDown"],cr=[" ","Enter"],cs="Select",[ct,cu,cv]=function(a){let b=a+"CollectionProvider",[c,d]=q(b),[e,f]=c(b,{collectionRef:{current:null},itemMap:new Map}),i=a=>{let{scope:b,children:c}=a,d=h.useRef(null),f=h.useRef(new Map).current;return(0,g.jsx)(e,{scope:b,itemMap:f,collectionRef:d,children:c})};i.displayName=b;let j=a+"CollectionSlot",k=u(j),l=h.forwardRef((a,b)=>{let{scope:c,children:d}=a,e=t(b,f(j,c).collectionRef);return(0,g.jsx)(k,{ref:e,children:d})});l.displayName=j;let m=a+"CollectionItemSlot",n="data-radix-collection-item",o=u(m),p=h.forwardRef((a,b)=>{let{scope:c,children:d,...e}=a,i=h.useRef(null),j=t(b,i),k=f(m,c);return h.useEffect(()=>(k.itemMap.set(i,{ref:i,...e}),()=>void k.itemMap.delete(i))),(0,g.jsx)(o,{...{[n]:""},ref:j,children:d})});return p.displayName=m,[{Provider:i,Slot:l,ItemSlot:p},function(b){let c=f(a+"CollectionConsumer",b);return h.useCallback(()=>{let a=c.collectionRef.current;if(!a)return[];let b=Array.from(a.querySelectorAll(`[${n}]`));return Array.from(c.itemMap.values()).sort((a,c)=>b.indexOf(a.ref.current)-b.indexOf(c.ref.current))},[c.collectionRef,c.itemMap])},d]}(cs),[cw,cx]=q(cs,[cv,bn]),cy=bn(),[cz,cA]=cw(cs),[cB,cC]=cw(cs),cD=a=>{let{__scopeSelect:b,children:c,open:d,defaultOpen:e,onOpenChange:f,value:i,defaultValue:j,onValueChange:k,dir:l,name:m,autoComplete:n,disabled:o,required:p,form:q}=a,r=cy(b),[s,t]=h.useState(null),[u,v]=h.useState(null),[w,x]=h.useState(!1),y=function(a){let b=h.useContext(B);return a||b||"ltr"}(l),[z,A]=bF({prop:d,defaultProp:e??!1,onChange:f,caller:cs}),[C,D]=bF({prop:i,defaultProp:j,onChange:k,caller:cs}),E=h.useRef(null),F=!s||q||!!s.closest("form"),[G,H]=h.useState(new Set),I=Array.from(G).map(a=>a.props.value).join(";");return(0,g.jsx)(bq,{...r,children:(0,g.jsxs)(cz,{required:p,scope:b,trigger:s,onTriggerChange:t,valueNode:u,onValueNodeChange:v,valueNodeHasChildren:w,onValueNodeHasChildrenChange:x,contentId:Y(),value:C,onValueChange:D,open:z,onOpenChange:A,dir:y,triggerPointerDownPosRef:E,disabled:o,children:[(0,g.jsx)(ct.Provider,{scope:b,children:(0,g.jsx)(cB,{scope:a.__scopeSelect,onNativeOptionAdd:h.useCallback(a=>{H(b=>new Set(b).add(a))},[]),onNativeOptionRemove:h.useCallback(a=>{H(b=>{let c=new Set(b);return c.delete(a),c})},[]),children:c})}),F?(0,g.jsxs)(dc,{"aria-hidden":!0,required:p,tabIndex:-1,name:m,autoComplete:n,value:C,onChange:a=>D(a.target.value),disabled:o,form:q,children:[void 0===C?(0,g.jsx)("option",{value:""}):null,Array.from(G)]},I):null]})})};cD.displayName=cs;var cE="SelectTrigger",cF=h.forwardRef((a,b)=>{let{__scopeSelect:c,disabled:d=!1,...e}=a,f=cy(c),i=cA(cE,c),j=i.disabled||d,k=t(b,i.onTriggerChange),l=cu(c),m=h.useRef("touch"),[n,o,q]=de(a=>{let b=l().filter(a=>!a.disabled),c=b.find(a=>a.value===i.value),d=df(b,a,c);void 0!==d&&i.onValueChange(d.value)}),r=a=>{j||(i.onOpenChange(!0),q()),a&&(i.triggerPointerDownPosRef.current={x:Math.round(a.pageX),y:Math.round(a.pageY)})};return(0,g.jsx)(bs,{asChild:!0,...f,children:(0,g.jsx)(C.button,{type:"button",role:"combobox","aria-controls":i.contentId,"aria-expanded":i.open,"aria-required":i.required,"aria-autocomplete":"none",dir:i.dir,"data-state":i.open?"open":"closed",disabled:j,"data-disabled":j?"":void 0,"data-placeholder":dd(i.value)?"":void 0,...e,ref:k,onClick:p(e.onClick,a=>{a.currentTarget.focus(),"mouse"!==m.current&&r(a)}),onPointerDown:p(e.onPointerDown,a=>{m.current=a.pointerType;let b=a.target;b.hasPointerCapture(a.pointerId)&&b.releasePointerCapture(a.pointerId),0===a.button&&!1===a.ctrlKey&&"mouse"===a.pointerType&&(r(a),a.preventDefault())}),onKeyDown:p(e.onKeyDown,a=>{let b=""!==n.current;a.ctrlKey||a.altKey||a.metaKey||1!==a.key.length||o(a.key),(!b||" "!==a.key)&&cq.includes(a.key)&&(r(),a.preventDefault())})})})});cF.displayName=cE;var cG="SelectValue",cH=h.forwardRef((a,b)=>{let{__scopeSelect:c,className:d,style:e,children:f,placeholder:h="",...i}=a,j=cA(cG,c),{onValueNodeHasChildrenChange:k}=j,l=void 0!==f,m=t(b,j.onValueNodeChange);return V(()=>{k(l)},[k,l]),(0,g.jsx)(C.span,{...i,ref:m,style:{pointerEvents:"none"},children:dd(j.value)?(0,g.jsx)(g.Fragment,{children:h}):f})});cH.displayName=cG;var cI=h.forwardRef((a,b)=>{let{__scopeSelect:c,children:d,...e}=a;return(0,g.jsx)(C.span,{"aria-hidden":!0,...e,ref:b,children:d||""})});cI.displayName="SelectIcon";var cJ=a=>(0,g.jsx)(bD,{asChild:!0,...a});cJ.displayName="SelectPortal";var cK="SelectContent",cL=h.forwardRef((a,b)=>{let c=cA(cK,a.__scopeSelect),[d,e]=h.useState();return(V(()=>{e(new DocumentFragment)},[]),c.open)?(0,g.jsx)(cP,{...a,ref:b}):d?n.createPortal((0,g.jsx)(cM,{scope:a.__scopeSelect,children:(0,g.jsx)(ct.Slot,{scope:a.__scopeSelect,children:(0,g.jsx)("div",{children:a.children})})}),d):null});cL.displayName=cK;var[cM,cN]=cw(cK),cO=u("SelectContent.RemoveScroll"),cP=h.forwardRef((a,b)=>{let{__scopeSelect:c,position:d="item-aligned",onCloseAutoFocus:e,onEscapeKeyDown:f,onPointerDownOutside:i,side:j,sideOffset:k,align:l,alignOffset:m,arrowPadding:n,collisionBoundary:o,collisionPadding:q,sticky:r,hideWhenDetached:s,avoidCollisions:u,...v}=a,w=cA(cK,c),[x,y]=h.useState(null),[z,A]=h.useState(null),B=t(b,a=>y(a)),[C,D]=h.useState(null),[E,F]=h.useState(null),H=cu(c),[I,J]=h.useState(!1),L=h.useRef(!1);h.useEffect(()=>{if(x)return bN(x)},[x]),K();let M=h.useCallback(a=>{let[b,...c]=H().map(a=>a.ref.current),[d]=c.slice(-1),e=document.activeElement;for(let c of a)if(c===e||(c?.scrollIntoView({block:"nearest"}),c===b&&z&&(z.scrollTop=0),c===d&&z&&(z.scrollTop=z.scrollHeight),c?.focus(),document.activeElement!==e))return},[H,z]),N=h.useCallback(()=>M([C,x]),[M,C,x]);h.useEffect(()=>{I&&N()},[I,N]);let{onOpenChange:O,triggerPointerDownPosRef:Q}=w;h.useEffect(()=>{if(x){let a={x:0,y:0},b=b=>{a={x:Math.abs(Math.round(b.pageX)-(Q.current?.x??0)),y:Math.abs(Math.round(b.pageY)-(Q.current?.y??0))}},c=c=>{a.x<=10&&a.y<=10?c.preventDefault():x.contains(c.target)||O(!1),document.removeEventListener("pointermove",b),Q.current=null};return null!==Q.current&&(document.addEventListener("pointermove",b),document.addEventListener("pointerup",c,{capture:!0,once:!0})),()=>{document.removeEventListener("pointermove",b),document.removeEventListener("pointerup",c,{capture:!0})}}},[x,O,Q]),h.useEffect(()=>{let a=()=>O(!1);return window.addEventListener("blur",a),window.addEventListener("resize",a),()=>{window.removeEventListener("blur",a),window.removeEventListener("resize",a)}},[O]);let[R,S]=de(a=>{let b=H().filter(a=>!a.disabled),c=b.find(a=>a.ref.current===document.activeElement),d=df(b,a,c);d&&setTimeout(()=>d.ref.current.focus())}),T=h.useCallback((a,b,c)=>{let d=!L.current&&!c;(void 0!==w.value&&w.value===b||d)&&(D(a),d&&(L.current=!0))},[w.value]),U=h.useCallback(()=>x?.focus(),[x]),V=h.useCallback((a,b,c)=>{let d=!L.current&&!c;(void 0!==w.value&&w.value===b||d)&&F(a)},[w.value]),W="popper"===d?cR:cQ,X=W===cR?{side:j,sideOffset:k,align:l,alignOffset:m,arrowPadding:n,collisionBoundary:o,collisionPadding:q,sticky:r,hideWhenDetached:s,avoidCollisions:u}:{};return(0,g.jsx)(cM,{scope:c,content:x,viewport:z,onViewportChange:A,itemRefCallback:T,selectedItem:C,onItemLeave:U,itemTextRefCallback:V,focusSelectedItem:N,selectedItemText:E,position:d,isPositioned:I,searchRef:R,children:(0,g.jsx)(cp,{as:cO,allowPinchZoom:!0,children:(0,g.jsx)(P,{asChild:!0,trapped:w.open,onMountAutoFocus:a=>{a.preventDefault()},onUnmountAutoFocus:p(e,a=>{w.trigger?.focus({preventScroll:!0}),a.preventDefault()}),children:(0,g.jsx)(G,{asChild:!0,disableOutsidePointerEvents:!0,onEscapeKeyDown:f,onPointerDownOutside:i,onFocusOutside:a=>a.preventDefault(),onDismiss:()=>w.onOpenChange(!1),children:(0,g.jsx)(W,{role:"listbox",id:w.contentId,"data-state":w.open?"open":"closed",dir:w.dir,onContextMenu:a=>a.preventDefault(),...v,...X,onPlaced:()=>J(!0),ref:B,style:{display:"flex",flexDirection:"column",outline:"none",...v.style},onKeyDown:p(v.onKeyDown,a=>{let b=a.ctrlKey||a.altKey||a.metaKey;if("Tab"===a.key&&a.preventDefault(),b||1!==a.key.length||S(a.key),["ArrowUp","ArrowDown","Home","End"].includes(a.key)){let b=H().filter(a=>!a.disabled).map(a=>a.ref.current);if(["ArrowUp","End"].includes(a.key)&&(b=b.slice().reverse()),["ArrowUp","ArrowDown"].includes(a.key)){let c=a.target,d=b.indexOf(c);b=b.slice(d+1)}setTimeout(()=>M(b)),a.preventDefault()}})})})})})})});cP.displayName="SelectContentImpl";var cQ=h.forwardRef((a,b)=>{let{__scopeSelect:c,onPlaced:d,...e}=a,f=cA(cK,c),i=cN(cK,c),[j,k]=h.useState(null),[l,m]=h.useState(null),n=t(b,a=>m(a)),p=cu(c),q=h.useRef(!1),r=h.useRef(!0),{viewport:s,selectedItem:u,selectedItemText:v,focusSelectedItem:w}=i,x=h.useCallback(()=>{if(f.trigger&&f.valueNode&&j&&l&&s&&u&&v){let a=f.trigger.getBoundingClientRect(),b=l.getBoundingClientRect(),c=f.valueNode.getBoundingClientRect(),e=v.getBoundingClientRect();if("rtl"!==f.dir){let d=e.left-b.left,f=c.left-d,g=a.left-f,h=a.width+g,i=Math.max(h,b.width),k=o(f,[10,Math.max(10,window.innerWidth-10-i)]);j.style.minWidth=h+"px",j.style.left=k+"px"}else{let d=b.right-e.right,f=window.innerWidth-c.right-d,g=window.innerWidth-a.right-f,h=a.width+g,i=Math.max(h,b.width),k=o(f,[10,Math.max(10,window.innerWidth-10-i)]);j.style.minWidth=h+"px",j.style.right=k+"px"}let g=p(),h=window.innerHeight-20,i=s.scrollHeight,k=window.getComputedStyle(l),m=parseInt(k.borderTopWidth,10),n=parseInt(k.paddingTop,10),r=parseInt(k.borderBottomWidth,10),t=m+n+i+parseInt(k.paddingBottom,10)+r,w=Math.min(5*u.offsetHeight,t),x=window.getComputedStyle(s),y=parseInt(x.paddingTop,10),z=parseInt(x.paddingBottom,10),A=a.top+a.height/2-10,B=u.offsetHeight/2,C=m+n+(u.offsetTop+B);if(C<=A){let a=g.length>0&&u===g[g.length-1].ref.current;j.style.bottom="0px";let b=Math.max(h-A,B+(a?z:0)+(l.clientHeight-s.offsetTop-s.offsetHeight)+r);j.style.height=C+b+"px"}else{let a=g.length>0&&u===g[0].ref.current;j.style.top="0px";let b=Math.max(A,m+s.offsetTop+(a?y:0)+B);j.style.height=b+(t-C)+"px",s.scrollTop=C-A+s.offsetTop}j.style.margin="10px 0",j.style.minHeight=w+"px",j.style.maxHeight=h+"px",d?.(),requestAnimationFrame(()=>q.current=!0)}},[p,f.trigger,f.valueNode,j,l,s,u,v,f.dir,d]);V(()=>x(),[x]);let[y,z]=h.useState();V(()=>{l&&z(window.getComputedStyle(l).zIndex)},[l]);let A=h.useCallback(a=>{a&&!0===r.current&&(x(),w?.(),r.current=!1)},[x,w]);return(0,g.jsx)(cS,{scope:c,contentWrapper:j,shouldExpandOnScrollRef:q,onScrollButtonChange:A,children:(0,g.jsx)("div",{ref:k,style:{display:"flex",flexDirection:"column",position:"fixed",zIndex:y},children:(0,g.jsx)(C.div,{...e,ref:n,style:{boxSizing:"border-box",maxHeight:"100%",...e.style}})})})});cQ.displayName="SelectItemAlignedPosition";var cR=h.forwardRef((a,b)=>{let{__scopeSelect:c,align:d="start",collisionPadding:e=10,...f}=a,h=cy(c);return(0,g.jsx)(bw,{...h,...f,ref:b,align:d,collisionPadding:e,style:{boxSizing:"border-box",...f.style,"--radix-select-content-transform-origin":"var(--radix-popper-transform-origin)","--radix-select-content-available-width":"var(--radix-popper-available-width)","--radix-select-content-available-height":"var(--radix-popper-available-height)","--radix-select-trigger-width":"var(--radix-popper-anchor-width)","--radix-select-trigger-height":"var(--radix-popper-anchor-height)"}})});cR.displayName="SelectPopperPosition";var[cS,cT]=cw(cK,{}),cU="SelectViewport",cV=h.forwardRef((a,b)=>{let{__scopeSelect:c,nonce:d,...e}=a,f=cN(cU,c),i=cT(cU,c),j=t(b,f.onViewportChange),k=h.useRef(0);return(0,g.jsxs)(g.Fragment,{children:[(0,g.jsx)("style",{dangerouslySetInnerHTML:{__html:"[data-radix-select-viewport]{scrollbar-width:none;-ms-overflow-style:none;-webkit-overflow-scrolling:touch;}[data-radix-select-viewport]::-webkit-scrollbar{display:none}"},nonce:d}),(0,g.jsx)(ct.Slot,{scope:c,children:(0,g.jsx)(C.div,{"data-radix-select-viewport":"",role:"presentation",...e,ref:j,style:{position:"relative",flex:1,overflow:"hidden auto",...e.style},onScroll:p(e.onScroll,a=>{let b=a.currentTarget,{contentWrapper:c,shouldExpandOnScrollRef:d}=i;if(d?.current&&c){let a=Math.abs(k.current-b.scrollTop);if(a>0){let d=window.innerHeight-20,e=Math.max(parseFloat(c.style.minHeight),parseFloat(c.style.height));if(e<d){let f=e+a,g=Math.min(d,f),h=f-g;c.style.height=g+"px","0px"===c.style.bottom&&(b.scrollTop=h>0?h:0,c.style.justifyContent="flex-end")}}}k.current=b.scrollTop})})})]})});cV.displayName=cU;var cW="SelectGroup",[cX,cY]=cw(cW);h.forwardRef((a,b)=>{let{__scopeSelect:c,...d}=a,e=Y();return(0,g.jsx)(cX,{scope:c,id:e,children:(0,g.jsx)(C.div,{role:"group","aria-labelledby":e,...d,ref:b})})}).displayName=cW;var cZ="SelectLabel";h.forwardRef((a,b)=>{let{__scopeSelect:c,...d}=a,e=cY(cZ,c);return(0,g.jsx)(C.div,{id:e.id,...d,ref:b})}).displayName=cZ;var c$="SelectItem",[c_,c0]=cw(c$),c1=h.forwardRef((a,b)=>{let{__scopeSelect:c,value:d,disabled:e=!1,textValue:f,...i}=a,j=cA(c$,c),k=cN(c$,c),l=j.value===d,[m,n]=h.useState(f??""),[o,q]=h.useState(!1),r=t(b,a=>k.itemRefCallback?.(a,d,e)),s=Y(),u=h.useRef("touch"),v=()=>{e||(j.onValueChange(d),j.onOpenChange(!1))};if(""===d)throw Error("A <Select.Item /> must have a value prop that is not an empty string. This is because the Select value can be set to an empty string to clear the selection and show the placeholder.");return(0,g.jsx)(c_,{scope:c,value:d,disabled:e,textId:s,isSelected:l,onItemTextChange:h.useCallback(a=>{n(b=>b||(a?.textContent??"").trim())},[]),children:(0,g.jsx)(ct.ItemSlot,{scope:c,value:d,disabled:e,textValue:m,children:(0,g.jsx)(C.div,{role:"option","aria-labelledby":s,"data-highlighted":o?"":void 0,"aria-selected":l&&o,"data-state":l?"checked":"unchecked","aria-disabled":e||void 0,"data-disabled":e?"":void 0,tabIndex:e?void 0:-1,...i,ref:r,onFocus:p(i.onFocus,()=>q(!0)),onBlur:p(i.onBlur,()=>q(!1)),onClick:p(i.onClick,()=>{"mouse"!==u.current&&v()}),onPointerUp:p(i.onPointerUp,()=>{"mouse"===u.current&&v()}),onPointerDown:p(i.onPointerDown,a=>{u.current=a.pointerType}),onPointerMove:p(i.onPointerMove,a=>{u.current=a.pointerType,e?k.onItemLeave?.():"mouse"===u.current&&a.currentTarget.focus({preventScroll:!0})}),onPointerLeave:p(i.onPointerLeave,a=>{a.currentTarget===document.activeElement&&k.onItemLeave?.()}),onKeyDown:p(i.onKeyDown,a=>{(k.searchRef?.current===""||" "!==a.key)&&(cr.includes(a.key)&&v()," "===a.key&&a.preventDefault())})})})})});c1.displayName=c$;var c2="SelectItemText",c3=h.forwardRef((a,b)=>{let{__scopeSelect:c,className:d,style:e,...f}=a,i=cA(c2,c),j=cN(c2,c),k=c0(c2,c),l=cC(c2,c),[m,o]=h.useState(null),p=t(b,a=>o(a),k.onItemTextChange,a=>j.itemTextRefCallback?.(a,k.value,k.disabled)),q=m?.textContent,r=h.useMemo(()=>(0,g.jsx)("option",{value:k.value,disabled:k.disabled,children:q},k.value),[k.disabled,k.value,q]),{onNativeOptionAdd:s,onNativeOptionRemove:u}=l;return V(()=>(s(r),()=>u(r)),[s,u,r]),(0,g.jsxs)(g.Fragment,{children:[(0,g.jsx)(C.span,{id:k.textId,...f,ref:p}),k.isSelected&&i.valueNode&&!i.valueNodeHasChildren?n.createPortal(f.children,i.valueNode):null]})});c3.displayName=c2;var c4="SelectItemIndicator",c5=h.forwardRef((a,b)=>{let{__scopeSelect:c,...d}=a;return c0(c4,c).isSelected?(0,g.jsx)(C.span,{"aria-hidden":!0,...d,ref:b}):null});c5.displayName=c4;var c6="SelectScrollUpButton",c7=h.forwardRef((a,b)=>{let c=cN(c6,a.__scopeSelect),d=cT(c6,a.__scopeSelect),[e,f]=h.useState(!1),i=t(b,d.onScrollButtonChange);return V(()=>{if(c.viewport&&c.isPositioned){let a=function(){f(b.scrollTop>0)},b=c.viewport;return a(),b.addEventListener("scroll",a),()=>b.removeEventListener("scroll",a)}},[c.viewport,c.isPositioned]),e?(0,g.jsx)(da,{...a,ref:i,onAutoScroll:()=>{let{viewport:a,selectedItem:b}=c;a&&b&&(a.scrollTop=a.scrollTop-b.offsetHeight)}}):null});c7.displayName=c6;var c8="SelectScrollDownButton",c9=h.forwardRef((a,b)=>{let c=cN(c8,a.__scopeSelect),d=cT(c8,a.__scopeSelect),[e,f]=h.useState(!1),i=t(b,d.onScrollButtonChange);return V(()=>{if(c.viewport&&c.isPositioned){let a=function(){let a=b.scrollHeight-b.clientHeight;f(Math.ceil(b.scrollTop)<a)},b=c.viewport;return a(),b.addEventListener("scroll",a),()=>b.removeEventListener("scroll",a)}},[c.viewport,c.isPositioned]),e?(0,g.jsx)(da,{...a,ref:i,onAutoScroll:()=>{let{viewport:a,selectedItem:b}=c;a&&b&&(a.scrollTop=a.scrollTop+b.offsetHeight)}}):null});c9.displayName=c8;var da=h.forwardRef((a,b)=>{let{__scopeSelect:c,onAutoScroll:d,...e}=a,f=cN("SelectScrollButton",c),i=h.useRef(null),j=cu(c),k=h.useCallback(()=>{null!==i.current&&(window.clearInterval(i.current),i.current=null)},[]);return h.useEffect(()=>()=>k(),[k]),V(()=>{let a=j().find(a=>a.ref.current===document.activeElement);a?.ref.current?.scrollIntoView({block:"nearest"})},[j]),(0,g.jsx)(C.div,{"aria-hidden":!0,...e,ref:b,style:{flexShrink:0,...e.style},onPointerDown:p(e.onPointerDown,()=>{null===i.current&&(i.current=window.setInterval(d,50))}),onPointerMove:p(e.onPointerMove,()=>{f.onItemLeave?.(),null===i.current&&(i.current=window.setInterval(d,50))}),onPointerLeave:p(e.onPointerLeave,()=>{k()})})});h.forwardRef((a,b)=>{let{__scopeSelect:c,...d}=a;return(0,g.jsx)(C.div,{"aria-hidden":!0,...d,ref:b})}).displayName="SelectSeparator";var db="SelectArrow";h.forwardRef((a,b)=>{let{__scopeSelect:c,...d}=a,e=cy(c),f=cA(db,c),h=cN(db,c);return f.open&&"popper"===h.position?(0,g.jsx)(bz,{...e,...d,ref:b}):null}).displayName=db;var dc=h.forwardRef(({__scopeSelect:a,value:b,...c},d)=>{let e=h.useRef(null),f=t(d,e),i=function(a){let b=h.useRef({value:a,previous:a});return h.useMemo(()=>(b.current.value!==a&&(b.current.previous=b.current.value,b.current.value=a),b.current.previous),[a])}(b);return h.useEffect(()=>{let a=e.current;if(!a)return;let c=Object.getOwnPropertyDescriptor(window.HTMLSelectElement.prototype,"value").set;if(i!==b&&c){let d=new Event("change",{bubbles:!0});c.call(a,b),a.dispatchEvent(d)}},[i,b]),(0,g.jsx)(C.select,{...c,style:{...bG,...c.style},ref:f,defaultValue:b})});function dd(a){return""===a||void 0===a}function de(a){let b=D(a),c=h.useRef(""),d=h.useRef(0),e=h.useCallback(a=>{let e=c.current+a;b(e),function a(b){c.current=b,window.clearTimeout(d.current),""!==b&&(d.current=window.setTimeout(()=>a(""),1e3))}(e)},[b]),f=h.useCallback(()=>{c.current="",window.clearTimeout(d.current)},[]);return h.useEffect(()=>()=>window.clearTimeout(d.current),[]),[c,e,f]}function df(a,b,c){var d,e;let f=b.length>1&&Array.from(b).every(a=>a===b[0])?b[0]:b,g=c?a.indexOf(c):-1,h=(d=a,e=Math.max(g,0),d.map((a,b)=>d[(e+b)%d.length]));1===f.length&&(h=h.filter(a=>a!==c));let i=h.find(a=>a.textValue.toLowerCase().startsWith(f.toLowerCase()));return i!==c?i:void 0}dc.displayName="SelectBubbleInput";let dg=a=>{let b=a.replace(/^([A-Z])|[\s-_]+(\w)/g,(a,b,c)=>c?c.toUpperCase():b.toLowerCase());return b.charAt(0).toUpperCase()+b.slice(1)},dh=(...a)=>a.filter((a,b,c)=>!!a&&""!==a.trim()&&c.indexOf(a)===b).join(" ").trim();var di={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};let dj=(0,h.forwardRef)(({color:a="currentColor",size:b=24,strokeWidth:c=2,absoluteStrokeWidth:d,className:e="",children:f,iconNode:g,...i},j)=>(0,h.createElement)("svg",{ref:j,...di,width:b,height:b,stroke:a,strokeWidth:d?24*Number(c)/Number(b):c,className:dh("lucide",e),...!f&&!(a=>{for(let b in a)if(b.startsWith("aria-")||"role"===b||"title"===b)return!0})(i)&&{"aria-hidden":"true"},...i},[...g.map(([a,b])=>(0,h.createElement)(a,b)),...Array.isArray(f)?f:[f]])),dk=(a,b)=>{let c=(0,h.forwardRef)(({className:c,...d},e)=>(0,h.createElement)(dj,{ref:e,iconNode:b,className:dh(`lucide-${dg(a).replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,`lucide-${a}`,c),...d}));return c.displayName=dg(a),c},dl=dk("chevron-down",[["path",{d:"m6 9 6 6 6-6",key:"qrunsl"}]]),dm=dk("check",[["path",{d:"M20 6 9 17l-5-5",key:"1gmf2c"}]]),dn=dk("chevron-up",[["path",{d:"m18 15-6-6-6 6",key:"153udz"}]]);function dp(){for(var a,b,c=0,d="",e=arguments.length;c<e;c++)(a=arguments[c])&&(b=function a(b){var c,d,e="";if("string"==typeof b||"number"==typeof b)e+=b;else if("object"==typeof b)if(Array.isArray(b)){var f=b.length;for(c=0;c<f;c++)b[c]&&(d=a(b[c]))&&(e&&(e+=" "),e+=d)}else for(d in b)b[d]&&(e&&(e+=" "),e+=d);return e}(a))&&(d&&(d+=" "),d+=b);return d}let dq=(a,b)=>{if(0===a.length)return b.classGroupId;let c=a[0],d=b.nextPart.get(c),e=d?dq(a.slice(1),d):void 0;if(e)return e;if(0===b.validators.length)return;let f=a.join("-");return b.validators.find(({validator:a})=>a(f))?.classGroupId},dr=/^\[(.+)\]$/,ds=(a,b,c,d)=>{a.forEach(a=>{if("string"==typeof a){(""===a?b:dt(b,a)).classGroupId=c;return}if("function"==typeof a)return du(a)?void ds(a(d),b,c,d):void b.validators.push({validator:a,classGroupId:c});Object.entries(a).forEach(([a,e])=>{ds(e,dt(b,a),c,d)})})},dt=(a,b)=>{let c=a;return b.split("-").forEach(a=>{c.nextPart.has(a)||c.nextPart.set(a,{nextPart:new Map,validators:[]}),c=c.nextPart.get(a)}),c},du=a=>a.isThemeGetter,dv=/\s+/;function dw(){let a,b,c=0,d="";for(;c<arguments.length;)(a=arguments[c++])&&(b=dx(a))&&(d&&(d+=" "),d+=b);return d}let dx=a=>{let b;if("string"==typeof a)return a;let c="";for(let d=0;d<a.length;d++)a[d]&&(b=dx(a[d]))&&(c&&(c+=" "),c+=b);return c},dy=a=>{let b=b=>b[a]||[];return b.isThemeGetter=!0,b},dz=/^\[(?:(\w[\w-]*):)?(.+)\]$/i,dA=/^\((?:(\w[\w-]*):)?(.+)\)$/i,dB=/^\d+\/\d+$/,dC=/^(\d+(\.\d+)?)?(xs|sm|md|lg|xl)$/,dD=/\d+(%|px|r?em|[sdl]?v([hwib]|min|max)|pt|pc|in|cm|mm|cap|ch|ex|r?lh|cq(w|h|i|b|min|max))|\b(calc|min|max|clamp)\(.+\)|^0$/,dE=/^(rgba?|hsla?|hwb|(ok)?(lab|lch)|color-mix)\(.+\)$/,dF=/^(inset_)?-?((\d+)?\.?(\d+)[a-z]+|0)_-?((\d+)?\.?(\d+)[a-z]+|0)/,dG=/^(url|image|image-set|cross-fade|element|(repeating-)?(linear|radial|conic)-gradient)\(.+\)$/,dH=a=>dB.test(a),dI=a=>!!a&&!Number.isNaN(Number(a)),dJ=a=>!!a&&Number.isInteger(Number(a)),dK=a=>a.endsWith("%")&&dI(a.slice(0,-1)),dL=a=>dC.test(a),dM=()=>!0,dN=a=>dD.test(a)&&!dE.test(a),dO=()=>!1,dP=a=>dF.test(a),dQ=a=>dG.test(a),dR=a=>!dT(a)&&!dZ(a),dS=a=>d4(a,d8,dO),dT=a=>dz.test(a),dU=a=>d4(a,d9,dN),dV=a=>d4(a,ea,dI),dW=a=>d4(a,d6,dO),dX=a=>d4(a,d7,dQ),dY=a=>d4(a,ec,dP),dZ=a=>dA.test(a),d$=a=>d5(a,d9),d_=a=>d5(a,eb),d0=a=>d5(a,d6),d1=a=>d5(a,d8),d2=a=>d5(a,d7),d3=a=>d5(a,ec,!0),d4=(a,b,c)=>{let d=dz.exec(a);return!!d&&(d[1]?b(d[1]):c(d[2]))},d5=(a,b,c=!1)=>{let d=dA.exec(a);return!!d&&(d[1]?b(d[1]):c)},d6=a=>"position"===a||"percentage"===a,d7=a=>"image"===a||"url"===a,d8=a=>"length"===a||"size"===a||"bg-size"===a,d9=a=>"length"===a,ea=a=>"number"===a,eb=a=>"family-name"===a,ec=a=>"shadow"===a;Symbol.toStringTag;let ed=function(a,...b){let c,d,e,f=function(h){let i;return d=(c={cache:(a=>{if(a<1)return{get:()=>void 0,set:()=>{}};let b=0,c=new Map,d=new Map,e=(e,f)=>{c.set(e,f),++b>a&&(b=0,d=c,c=new Map)};return{get(a){let b=c.get(a);return void 0!==b?b:void 0!==(b=d.get(a))?(e(a,b),b):void 0},set(a,b){c.has(a)?c.set(a,b):e(a,b)}}})((i=b.reduce((a,b)=>b(a),a())).cacheSize),parseClassName:(a=>{let{prefix:b,experimentalParseClassName:c}=a,d=a=>{let b,c,d=[],e=0,f=0,g=0;for(let c=0;c<a.length;c++){let h=a[c];if(0===e&&0===f){if(":"===h){d.push(a.slice(g,c)),g=c+1;continue}if("/"===h){b=c;continue}}"["===h?e++:"]"===h?e--:"("===h?f++:")"===h&&f--}let h=0===d.length?a:a.substring(g),i=(c=h).endsWith("!")?c.substring(0,c.length-1):c.startsWith("!")?c.substring(1):c;return{modifiers:d,hasImportantModifier:i!==h,baseClassName:i,maybePostfixModifierPosition:b&&b>g?b-g:void 0}};if(b){let a=b+":",c=d;d=b=>b.startsWith(a)?c(b.substring(a.length)):{isExternal:!0,modifiers:[],hasImportantModifier:!1,baseClassName:b,maybePostfixModifierPosition:void 0}}if(c){let a=d;d=b=>c({className:b,parseClassName:a})}return d})(i),sortModifiers:(a=>{let b=Object.fromEntries(a.orderSensitiveModifiers.map(a=>[a,!0]));return a=>{if(a.length<=1)return a;let c=[],d=[];return a.forEach(a=>{"["===a[0]||b[a]?(c.push(...d.sort(),a),d=[]):d.push(a)}),c.push(...d.sort()),c}})(i),...(a=>{let b=(a=>{let{theme:b,classGroups:c}=a,d={nextPart:new Map,validators:[]};for(let a in c)ds(c[a],d,a,b);return d})(a),{conflictingClassGroups:c,conflictingClassGroupModifiers:d}=a;return{getClassGroupId:a=>{let c=a.split("-");return""===c[0]&&1!==c.length&&c.shift(),dq(c,b)||(a=>{if(dr.test(a)){let b=dr.exec(a)[1],c=b?.substring(0,b.indexOf(":"));if(c)return"arbitrary.."+c}})(a)},getConflictingClassGroupIds:(a,b)=>{let e=c[a]||[];return b&&d[a]?[...e,...d[a]]:e}}})(i)}).cache.get,e=c.cache.set,f=g,g(h)};function g(a){let b=d(a);if(b)return b;let f=((a,b)=>{let{parseClassName:c,getClassGroupId:d,getConflictingClassGroupIds:e,sortModifiers:f}=b,g=[],h=a.trim().split(dv),i="";for(let a=h.length-1;a>=0;a-=1){let b=h[a],{isExternal:j,modifiers:k,hasImportantModifier:l,baseClassName:m,maybePostfixModifierPosition:n}=c(b);if(j){i=b+(i.length>0?" "+i:i);continue}let o=!!n,p=d(o?m.substring(0,n):m);if(!p){if(!o||!(p=d(m))){i=b+(i.length>0?" "+i:i);continue}o=!1}let q=f(k).join(":"),r=l?q+"!":q,s=r+p;if(g.includes(s))continue;g.push(s);let t=e(p,o);for(let a=0;a<t.length;++a){let b=t[a];g.push(r+b)}i=b+(i.length>0?" "+i:i)}return i})(a,c);return e(a,f),f}return function(){return f(dw.apply(null,arguments))}}(()=>{let a=dy("color"),b=dy("font"),c=dy("text"),d=dy("font-weight"),e=dy("tracking"),f=dy("leading"),g=dy("breakpoint"),h=dy("container"),i=dy("spacing"),j=dy("radius"),k=dy("shadow"),l=dy("inset-shadow"),m=dy("text-shadow"),n=dy("drop-shadow"),o=dy("blur"),p=dy("perspective"),q=dy("aspect"),r=dy("ease"),s=dy("animate"),t=()=>["auto","avoid","all","avoid-page","page","left","right","column"],u=()=>["center","top","bottom","left","right","top-left","left-top","top-right","right-top","bottom-right","right-bottom","bottom-left","left-bottom"],v=()=>[...u(),dZ,dT],w=()=>["auto","hidden","clip","visible","scroll"],x=()=>["auto","contain","none"],y=()=>[dZ,dT,i],z=()=>[dH,"full","auto",...y()],A=()=>[dJ,"none","subgrid",dZ,dT],B=()=>["auto",{span:["full",dJ,dZ,dT]},dJ,dZ,dT],C=()=>[dJ,"auto",dZ,dT],D=()=>["auto","min","max","fr",dZ,dT],E=()=>["start","end","center","between","around","evenly","stretch","baseline","center-safe","end-safe"],F=()=>["start","end","center","stretch","center-safe","end-safe"],G=()=>["auto",...y()],H=()=>[dH,"auto","full","dvw","dvh","lvw","lvh","svw","svh","min","max","fit",...y()],I=()=>[a,dZ,dT],J=()=>[...u(),d0,dW,{position:[dZ,dT]}],K=()=>["no-repeat",{repeat:["","x","y","space","round"]}],L=()=>["auto","cover","contain",d1,dS,{size:[dZ,dT]}],M=()=>[dK,d$,dU],N=()=>["","none","full",j,dZ,dT],O=()=>["",dI,d$,dU],P=()=>["solid","dashed","dotted","double"],Q=()=>["normal","multiply","screen","overlay","darken","lighten","color-dodge","color-burn","hard-light","soft-light","difference","exclusion","hue","saturation","color","luminosity"],R=()=>[dI,dK,d0,dW],S=()=>["","none",o,dZ,dT],T=()=>["none",dI,dZ,dT],U=()=>["none",dI,dZ,dT],V=()=>[dI,dZ,dT],W=()=>[dH,"full",...y()];return{cacheSize:500,theme:{animate:["spin","ping","pulse","bounce"],aspect:["video"],blur:[dL],breakpoint:[dL],color:[dM],container:[dL],"drop-shadow":[dL],ease:["in","out","in-out"],font:[dR],"font-weight":["thin","extralight","light","normal","medium","semibold","bold","extrabold","black"],"inset-shadow":[dL],leading:["none","tight","snug","normal","relaxed","loose"],perspective:["dramatic","near","normal","midrange","distant","none"],radius:[dL],shadow:[dL],spacing:["px",dI],text:[dL],"text-shadow":[dL],tracking:["tighter","tight","normal","wide","wider","widest"]},classGroups:{aspect:[{aspect:["auto","square",dH,dT,dZ,q]}],container:["container"],columns:[{columns:[dI,dT,dZ,h]}],"break-after":[{"break-after":t()}],"break-before":[{"break-before":t()}],"break-inside":[{"break-inside":["auto","avoid","avoid-page","avoid-column"]}],"box-decoration":[{"box-decoration":["slice","clone"]}],box:[{box:["border","content"]}],display:["block","inline-block","inline","flex","inline-flex","table","inline-table","table-caption","table-cell","table-column","table-column-group","table-footer-group","table-header-group","table-row-group","table-row","flow-root","grid","inline-grid","contents","list-item","hidden"],sr:["sr-only","not-sr-only"],float:[{float:["right","left","none","start","end"]}],clear:[{clear:["left","right","both","none","start","end"]}],isolation:["isolate","isolation-auto"],"object-fit":[{object:["contain","cover","fill","none","scale-down"]}],"object-position":[{object:v()}],overflow:[{overflow:w()}],"overflow-x":[{"overflow-x":w()}],"overflow-y":[{"overflow-y":w()}],overscroll:[{overscroll:x()}],"overscroll-x":[{"overscroll-x":x()}],"overscroll-y":[{"overscroll-y":x()}],position:["static","fixed","absolute","relative","sticky"],inset:[{inset:z()}],"inset-x":[{"inset-x":z()}],"inset-y":[{"inset-y":z()}],start:[{start:z()}],end:[{end:z()}],top:[{top:z()}],right:[{right:z()}],bottom:[{bottom:z()}],left:[{left:z()}],visibility:["visible","invisible","collapse"],z:[{z:[dJ,"auto",dZ,dT]}],basis:[{basis:[dH,"full","auto",h,...y()]}],"flex-direction":[{flex:["row","row-reverse","col","col-reverse"]}],"flex-wrap":[{flex:["nowrap","wrap","wrap-reverse"]}],flex:[{flex:[dI,dH,"auto","initial","none",dT]}],grow:[{grow:["",dI,dZ,dT]}],shrink:[{shrink:["",dI,dZ,dT]}],order:[{order:[dJ,"first","last","none",dZ,dT]}],"grid-cols":[{"grid-cols":A()}],"col-start-end":[{col:B()}],"col-start":[{"col-start":C()}],"col-end":[{"col-end":C()}],"grid-rows":[{"grid-rows":A()}],"row-start-end":[{row:B()}],"row-start":[{"row-start":C()}],"row-end":[{"row-end":C()}],"grid-flow":[{"grid-flow":["row","col","dense","row-dense","col-dense"]}],"auto-cols":[{"auto-cols":D()}],"auto-rows":[{"auto-rows":D()}],gap:[{gap:y()}],"gap-x":[{"gap-x":y()}],"gap-y":[{"gap-y":y()}],"justify-content":[{justify:[...E(),"normal"]}],"justify-items":[{"justify-items":[...F(),"normal"]}],"justify-self":[{"justify-self":["auto",...F()]}],"align-content":[{content:["normal",...E()]}],"align-items":[{items:[...F(),{baseline:["","last"]}]}],"align-self":[{self:["auto",...F(),{baseline:["","last"]}]}],"place-content":[{"place-content":E()}],"place-items":[{"place-items":[...F(),"baseline"]}],"place-self":[{"place-self":["auto",...F()]}],p:[{p:y()}],px:[{px:y()}],py:[{py:y()}],ps:[{ps:y()}],pe:[{pe:y()}],pt:[{pt:y()}],pr:[{pr:y()}],pb:[{pb:y()}],pl:[{pl:y()}],m:[{m:G()}],mx:[{mx:G()}],my:[{my:G()}],ms:[{ms:G()}],me:[{me:G()}],mt:[{mt:G()}],mr:[{mr:G()}],mb:[{mb:G()}],ml:[{ml:G()}],"space-x":[{"space-x":y()}],"space-x-reverse":["space-x-reverse"],"space-y":[{"space-y":y()}],"space-y-reverse":["space-y-reverse"],size:[{size:H()}],w:[{w:[h,"screen",...H()]}],"min-w":[{"min-w":[h,"screen","none",...H()]}],"max-w":[{"max-w":[h,"screen","none","prose",{screen:[g]},...H()]}],h:[{h:["screen","lh",...H()]}],"min-h":[{"min-h":["screen","lh","none",...H()]}],"max-h":[{"max-h":["screen","lh",...H()]}],"font-size":[{text:["base",c,d$,dU]}],"font-smoothing":["antialiased","subpixel-antialiased"],"font-style":["italic","not-italic"],"font-weight":[{font:[d,dZ,dV]}],"font-stretch":[{"font-stretch":["ultra-condensed","extra-condensed","condensed","semi-condensed","normal","semi-expanded","expanded","extra-expanded","ultra-expanded",dK,dT]}],"font-family":[{font:[d_,dT,b]}],"fvn-normal":["normal-nums"],"fvn-ordinal":["ordinal"],"fvn-slashed-zero":["slashed-zero"],"fvn-figure":["lining-nums","oldstyle-nums"],"fvn-spacing":["proportional-nums","tabular-nums"],"fvn-fraction":["diagonal-fractions","stacked-fractions"],tracking:[{tracking:[e,dZ,dT]}],"line-clamp":[{"line-clamp":[dI,"none",dZ,dV]}],leading:[{leading:[f,...y()]}],"list-image":[{"list-image":["none",dZ,dT]}],"list-style-position":[{list:["inside","outside"]}],"list-style-type":[{list:["disc","decimal","none",dZ,dT]}],"text-alignment":[{text:["left","center","right","justify","start","end"]}],"placeholder-color":[{placeholder:I()}],"text-color":[{text:I()}],"text-decoration":["underline","overline","line-through","no-underline"],"text-decoration-style":[{decoration:[...P(),"wavy"]}],"text-decoration-thickness":[{decoration:[dI,"from-font","auto",dZ,dU]}],"text-decoration-color":[{decoration:I()}],"underline-offset":[{"underline-offset":[dI,"auto",dZ,dT]}],"text-transform":["uppercase","lowercase","capitalize","normal-case"],"text-overflow":["truncate","text-ellipsis","text-clip"],"text-wrap":[{text:["wrap","nowrap","balance","pretty"]}],indent:[{indent:y()}],"vertical-align":[{align:["baseline","top","middle","bottom","text-top","text-bottom","sub","super",dZ,dT]}],whitespace:[{whitespace:["normal","nowrap","pre","pre-line","pre-wrap","break-spaces"]}],break:[{break:["normal","words","all","keep"]}],wrap:[{wrap:["break-word","anywhere","normal"]}],hyphens:[{hyphens:["none","manual","auto"]}],content:[{content:["none",dZ,dT]}],"bg-attachment":[{bg:["fixed","local","scroll"]}],"bg-clip":[{"bg-clip":["border","padding","content","text"]}],"bg-origin":[{"bg-origin":["border","padding","content"]}],"bg-position":[{bg:J()}],"bg-repeat":[{bg:K()}],"bg-size":[{bg:L()}],"bg-image":[{bg:["none",{linear:[{to:["t","tr","r","br","b","bl","l","tl"]},dJ,dZ,dT],radial:["",dZ,dT],conic:[dJ,dZ,dT]},d2,dX]}],"bg-color":[{bg:I()}],"gradient-from-pos":[{from:M()}],"gradient-via-pos":[{via:M()}],"gradient-to-pos":[{to:M()}],"gradient-from":[{from:I()}],"gradient-via":[{via:I()}],"gradient-to":[{to:I()}],rounded:[{rounded:N()}],"rounded-s":[{"rounded-s":N()}],"rounded-e":[{"rounded-e":N()}],"rounded-t":[{"rounded-t":N()}],"rounded-r":[{"rounded-r":N()}],"rounded-b":[{"rounded-b":N()}],"rounded-l":[{"rounded-l":N()}],"rounded-ss":[{"rounded-ss":N()}],"rounded-se":[{"rounded-se":N()}],"rounded-ee":[{"rounded-ee":N()}],"rounded-es":[{"rounded-es":N()}],"rounded-tl":[{"rounded-tl":N()}],"rounded-tr":[{"rounded-tr":N()}],"rounded-br":[{"rounded-br":N()}],"rounded-bl":[{"rounded-bl":N()}],"border-w":[{border:O()}],"border-w-x":[{"border-x":O()}],"border-w-y":[{"border-y":O()}],"border-w-s":[{"border-s":O()}],"border-w-e":[{"border-e":O()}],"border-w-t":[{"border-t":O()}],"border-w-r":[{"border-r":O()}],"border-w-b":[{"border-b":O()}],"border-w-l":[{"border-l":O()}],"divide-x":[{"divide-x":O()}],"divide-x-reverse":["divide-x-reverse"],"divide-y":[{"divide-y":O()}],"divide-y-reverse":["divide-y-reverse"],"border-style":[{border:[...P(),"hidden","none"]}],"divide-style":[{divide:[...P(),"hidden","none"]}],"border-color":[{border:I()}],"border-color-x":[{"border-x":I()}],"border-color-y":[{"border-y":I()}],"border-color-s":[{"border-s":I()}],"border-color-e":[{"border-e":I()}],"border-color-t":[{"border-t":I()}],"border-color-r":[{"border-r":I()}],"border-color-b":[{"border-b":I()}],"border-color-l":[{"border-l":I()}],"divide-color":[{divide:I()}],"outline-style":[{outline:[...P(),"none","hidden"]}],"outline-offset":[{"outline-offset":[dI,dZ,dT]}],"outline-w":[{outline:["",dI,d$,dU]}],"outline-color":[{outline:I()}],shadow:[{shadow:["","none",k,d3,dY]}],"shadow-color":[{shadow:I()}],"inset-shadow":[{"inset-shadow":["none",l,d3,dY]}],"inset-shadow-color":[{"inset-shadow":I()}],"ring-w":[{ring:O()}],"ring-w-inset":["ring-inset"],"ring-color":[{ring:I()}],"ring-offset-w":[{"ring-offset":[dI,dU]}],"ring-offset-color":[{"ring-offset":I()}],"inset-ring-w":[{"inset-ring":O()}],"inset-ring-color":[{"inset-ring":I()}],"text-shadow":[{"text-shadow":["none",m,d3,dY]}],"text-shadow-color":[{"text-shadow":I()}],opacity:[{opacity:[dI,dZ,dT]}],"mix-blend":[{"mix-blend":[...Q(),"plus-darker","plus-lighter"]}],"bg-blend":[{"bg-blend":Q()}],"mask-clip":[{"mask-clip":["border","padding","content","fill","stroke","view"]},"mask-no-clip"],"mask-composite":[{mask:["add","subtract","intersect","exclude"]}],"mask-image-linear-pos":[{"mask-linear":[dI]}],"mask-image-linear-from-pos":[{"mask-linear-from":R()}],"mask-image-linear-to-pos":[{"mask-linear-to":R()}],"mask-image-linear-from-color":[{"mask-linear-from":I()}],"mask-image-linear-to-color":[{"mask-linear-to":I()}],"mask-image-t-from-pos":[{"mask-t-from":R()}],"mask-image-t-to-pos":[{"mask-t-to":R()}],"mask-image-t-from-color":[{"mask-t-from":I()}],"mask-image-t-to-color":[{"mask-t-to":I()}],"mask-image-r-from-pos":[{"mask-r-from":R()}],"mask-image-r-to-pos":[{"mask-r-to":R()}],"mask-image-r-from-color":[{"mask-r-from":I()}],"mask-image-r-to-color":[{"mask-r-to":I()}],"mask-image-b-from-pos":[{"mask-b-from":R()}],"mask-image-b-to-pos":[{"mask-b-to":R()}],"mask-image-b-from-color":[{"mask-b-from":I()}],"mask-image-b-to-color":[{"mask-b-to":I()}],"mask-image-l-from-pos":[{"mask-l-from":R()}],"mask-image-l-to-pos":[{"mask-l-to":R()}],"mask-image-l-from-color":[{"mask-l-from":I()}],"mask-image-l-to-color":[{"mask-l-to":I()}],"mask-image-x-from-pos":[{"mask-x-from":R()}],"mask-image-x-to-pos":[{"mask-x-to":R()}],"mask-image-x-from-color":[{"mask-x-from":I()}],"mask-image-x-to-color":[{"mask-x-to":I()}],"mask-image-y-from-pos":[{"mask-y-from":R()}],"mask-image-y-to-pos":[{"mask-y-to":R()}],"mask-image-y-from-color":[{"mask-y-from":I()}],"mask-image-y-to-color":[{"mask-y-to":I()}],"mask-image-radial":[{"mask-radial":[dZ,dT]}],"mask-image-radial-from-pos":[{"mask-radial-from":R()}],"mask-image-radial-to-pos":[{"mask-radial-to":R()}],"mask-image-radial-from-color":[{"mask-radial-from":I()}],"mask-image-radial-to-color":[{"mask-radial-to":I()}],"mask-image-radial-shape":[{"mask-radial":["circle","ellipse"]}],"mask-image-radial-size":[{"mask-radial":[{closest:["side","corner"],farthest:["side","corner"]}]}],"mask-image-radial-pos":[{"mask-radial-at":u()}],"mask-image-conic-pos":[{"mask-conic":[dI]}],"mask-image-conic-from-pos":[{"mask-conic-from":R()}],"mask-image-conic-to-pos":[{"mask-conic-to":R()}],"mask-image-conic-from-color":[{"mask-conic-from":I()}],"mask-image-conic-to-color":[{"mask-conic-to":I()}],"mask-mode":[{mask:["alpha","luminance","match"]}],"mask-origin":[{"mask-origin":["border","padding","content","fill","stroke","view"]}],"mask-position":[{mask:J()}],"mask-repeat":[{mask:K()}],"mask-size":[{mask:L()}],"mask-type":[{"mask-type":["alpha","luminance"]}],"mask-image":[{mask:["none",dZ,dT]}],filter:[{filter:["","none",dZ,dT]}],blur:[{blur:S()}],brightness:[{brightness:[dI,dZ,dT]}],contrast:[{contrast:[dI,dZ,dT]}],"drop-shadow":[{"drop-shadow":["","none",n,d3,dY]}],"drop-shadow-color":[{"drop-shadow":I()}],grayscale:[{grayscale:["",dI,dZ,dT]}],"hue-rotate":[{"hue-rotate":[dI,dZ,dT]}],invert:[{invert:["",dI,dZ,dT]}],saturate:[{saturate:[dI,dZ,dT]}],sepia:[{sepia:["",dI,dZ,dT]}],"backdrop-filter":[{"backdrop-filter":["","none",dZ,dT]}],"backdrop-blur":[{"backdrop-blur":S()}],"backdrop-brightness":[{"backdrop-brightness":[dI,dZ,dT]}],"backdrop-contrast":[{"backdrop-contrast":[dI,dZ,dT]}],"backdrop-grayscale":[{"backdrop-grayscale":["",dI,dZ,dT]}],"backdrop-hue-rotate":[{"backdrop-hue-rotate":[dI,dZ,dT]}],"backdrop-invert":[{"backdrop-invert":["",dI,dZ,dT]}],"backdrop-opacity":[{"backdrop-opacity":[dI,dZ,dT]}],"backdrop-saturate":[{"backdrop-saturate":[dI,dZ,dT]}],"backdrop-sepia":[{"backdrop-sepia":["",dI,dZ,dT]}],"border-collapse":[{border:["collapse","separate"]}],"border-spacing":[{"border-spacing":y()}],"border-spacing-x":[{"border-spacing-x":y()}],"border-spacing-y":[{"border-spacing-y":y()}],"table-layout":[{table:["auto","fixed"]}],caption:[{caption:["top","bottom"]}],transition:[{transition:["","all","colors","opacity","shadow","transform","none",dZ,dT]}],"transition-behavior":[{transition:["normal","discrete"]}],duration:[{duration:[dI,"initial",dZ,dT]}],ease:[{ease:["linear","initial",r,dZ,dT]}],delay:[{delay:[dI,dZ,dT]}],animate:[{animate:["none",s,dZ,dT]}],backface:[{backface:["hidden","visible"]}],perspective:[{perspective:[p,dZ,dT]}],"perspective-origin":[{"perspective-origin":v()}],rotate:[{rotate:T()}],"rotate-x":[{"rotate-x":T()}],"rotate-y":[{"rotate-y":T()}],"rotate-z":[{"rotate-z":T()}],scale:[{scale:U()}],"scale-x":[{"scale-x":U()}],"scale-y":[{"scale-y":U()}],"scale-z":[{"scale-z":U()}],"scale-3d":["scale-3d"],skew:[{skew:V()}],"skew-x":[{"skew-x":V()}],"skew-y":[{"skew-y":V()}],transform:[{transform:[dZ,dT,"","none","gpu","cpu"]}],"transform-origin":[{origin:v()}],"transform-style":[{transform:["3d","flat"]}],translate:[{translate:W()}],"translate-x":[{"translate-x":W()}],"translate-y":[{"translate-y":W()}],"translate-z":[{"translate-z":W()}],"translate-none":["translate-none"],accent:[{accent:I()}],appearance:[{appearance:["none","auto"]}],"caret-color":[{caret:I()}],"color-scheme":[{scheme:["normal","dark","light","light-dark","only-dark","only-light"]}],cursor:[{cursor:["auto","default","pointer","wait","text","move","help","not-allowed","none","context-menu","progress","cell","crosshair","vertical-text","alias","copy","no-drop","grab","grabbing","all-scroll","col-resize","row-resize","n-resize","e-resize","s-resize","w-resize","ne-resize","nw-resize","se-resize","sw-resize","ew-resize","ns-resize","nesw-resize","nwse-resize","zoom-in","zoom-out",dZ,dT]}],"field-sizing":[{"field-sizing":["fixed","content"]}],"pointer-events":[{"pointer-events":["auto","none"]}],resize:[{resize:["none","","y","x"]}],"scroll-behavior":[{scroll:["auto","smooth"]}],"scroll-m":[{"scroll-m":y()}],"scroll-mx":[{"scroll-mx":y()}],"scroll-my":[{"scroll-my":y()}],"scroll-ms":[{"scroll-ms":y()}],"scroll-me":[{"scroll-me":y()}],"scroll-mt":[{"scroll-mt":y()}],"scroll-mr":[{"scroll-mr":y()}],"scroll-mb":[{"scroll-mb":y()}],"scroll-ml":[{"scroll-ml":y()}],"scroll-p":[{"scroll-p":y()}],"scroll-px":[{"scroll-px":y()}],"scroll-py":[{"scroll-py":y()}],"scroll-ps":[{"scroll-ps":y()}],"scroll-pe":[{"scroll-pe":y()}],"scroll-pt":[{"scroll-pt":y()}],"scroll-pr":[{"scroll-pr":y()}],"scroll-pb":[{"scroll-pb":y()}],"scroll-pl":[{"scroll-pl":y()}],"snap-align":[{snap:["start","end","center","align-none"]}],"snap-stop":[{snap:["normal","always"]}],"snap-type":[{snap:["none","x","y","both"]}],"snap-strictness":[{snap:["mandatory","proximity"]}],touch:[{touch:["auto","none","manipulation"]}],"touch-x":[{"touch-pan":["x","left","right"]}],"touch-y":[{"touch-pan":["y","up","down"]}],"touch-pz":["touch-pinch-zoom"],select:[{select:["none","text","all","auto"]}],"will-change":[{"will-change":["auto","scroll","contents","transform",dZ,dT]}],fill:[{fill:["none",...I()]}],"stroke-w":[{stroke:[dI,d$,dU,dV]}],stroke:[{stroke:["none",...I()]}],"forced-color-adjust":[{"forced-color-adjust":["auto","none"]}]},conflictingClassGroups:{overflow:["overflow-x","overflow-y"],overscroll:["overscroll-x","overscroll-y"],inset:["inset-x","inset-y","start","end","top","right","bottom","left"],"inset-x":["right","left"],"inset-y":["top","bottom"],flex:["basis","grow","shrink"],gap:["gap-x","gap-y"],p:["px","py","ps","pe","pt","pr","pb","pl"],px:["pr","pl"],py:["pt","pb"],m:["mx","my","ms","me","mt","mr","mb","ml"],mx:["mr","ml"],my:["mt","mb"],size:["w","h"],"font-size":["leading"],"fvn-normal":["fvn-ordinal","fvn-slashed-zero","fvn-figure","fvn-spacing","fvn-fraction"],"fvn-ordinal":["fvn-normal"],"fvn-slashed-zero":["fvn-normal"],"fvn-figure":["fvn-normal"],"fvn-spacing":["fvn-normal"],"fvn-fraction":["fvn-normal"],"line-clamp":["display","overflow"],rounded:["rounded-s","rounded-e","rounded-t","rounded-r","rounded-b","rounded-l","rounded-ss","rounded-se","rounded-ee","rounded-es","rounded-tl","rounded-tr","rounded-br","rounded-bl"],"rounded-s":["rounded-ss","rounded-es"],"rounded-e":["rounded-se","rounded-ee"],"rounded-t":["rounded-tl","rounded-tr"],"rounded-r":["rounded-tr","rounded-br"],"rounded-b":["rounded-br","rounded-bl"],"rounded-l":["rounded-tl","rounded-bl"],"border-spacing":["border-spacing-x","border-spacing-y"],"border-w":["border-w-x","border-w-y","border-w-s","border-w-e","border-w-t","border-w-r","border-w-b","border-w-l"],"border-w-x":["border-w-r","border-w-l"],"border-w-y":["border-w-t","border-w-b"],"border-color":["border-color-x","border-color-y","border-color-s","border-color-e","border-color-t","border-color-r","border-color-b","border-color-l"],"border-color-x":["border-color-r","border-color-l"],"border-color-y":["border-color-t","border-color-b"],translate:["translate-x","translate-y","translate-none"],"translate-none":["translate","translate-x","translate-y","translate-z"],"scroll-m":["scroll-mx","scroll-my","scroll-ms","scroll-me","scroll-mt","scroll-mr","scroll-mb","scroll-ml"],"scroll-mx":["scroll-mr","scroll-ml"],"scroll-my":["scroll-mt","scroll-mb"],"scroll-p":["scroll-px","scroll-py","scroll-ps","scroll-pe","scroll-pt","scroll-pr","scroll-pb","scroll-pl"],"scroll-px":["scroll-pr","scroll-pl"],"scroll-py":["scroll-pt","scroll-pb"],touch:["touch-x","touch-y","touch-pz"],"touch-x":["touch"],"touch-y":["touch"],"touch-pz":["touch"]},conflictingClassGroupModifiers:{"font-size":["leading"]},orderSensitiveModifiers:["*","**","after","backdrop","before","details-content","file","first-letter","first-line","marker","placeholder","selection"]}});function ee(...a){return ed(dp(a))}function ef({...a}){return(0,g.jsx)(cD,{"data-slot":"select",...a})}function eg({...a}){return(0,g.jsx)(cH,{"data-slot":"select-value",...a})}function eh({className:a,size:b="default",children:c,...d}){return(0,g.jsxs)(cF,{"data-slot":"select-trigger","data-size":b,className:ee("border-input data-[placeholder]:text-muted-foreground [&_svg:not([class*='text-'])]:text-muted-foreground focus-visible:border-ring focus-visible:ring-ring/50 aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive dark:bg-input/30 dark:hover:bg-input/50 flex w-fit items-center justify-between gap-2 rounded-md border bg-transparent px-3 py-2 text-sm whitespace-nowrap shadow-xs transition-[color,box-shadow] outline-none focus-visible:ring-[3px] disabled:cursor-not-allowed disabled:opacity-50 data-[size=default]:h-9 data-[size=sm]:h-8 *:data-[slot=select-value]:line-clamp-1 *:data-[slot=select-value]:flex *:data-[slot=select-value]:items-center *:data-[slot=select-value]:gap-2 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",a),...d,children:[c,(0,g.jsx)(cI,{asChild:!0,children:(0,g.jsx)(dl,{className:"size-4 opacity-50"})})]})}function ei({className:a,children:b,position:c="popper",...d}){return(0,g.jsx)(cJ,{children:(0,g.jsxs)(cL,{"data-slot":"select-content",className:ee("bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 relative z-50 max-h-(--radix-select-content-available-height) min-w-[8rem] origin-(--radix-select-content-transform-origin) overflow-x-hidden overflow-y-auto rounded-md border shadow-md","popper"===c&&"data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",a),position:c,...d,children:[(0,g.jsx)(ek,{}),(0,g.jsx)(cV,{className:ee("p-1","popper"===c&&"h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)] scroll-my-1"),children:b}),(0,g.jsx)(el,{})]})})}function ej({className:a,children:b,...c}){return(0,g.jsxs)(c1,{"data-slot":"select-item",className:ee("focus:bg-accent focus:text-accent-foreground [&_svg:not([class*='text-'])]:text-muted-foreground relative flex w-full cursor-default items-center gap-2 rounded-sm py-1.5 pr-8 pl-2 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4 *:[span]:last:flex *:[span]:last:items-center *:[span]:last:gap-2",a),...c,children:[(0,g.jsx)("span",{className:"absolute right-2 flex size-3.5 items-center justify-center",children:(0,g.jsx)(c5,{children:(0,g.jsx)(dm,{className:"size-4"})})}),(0,g.jsx)(c3,{children:b})]})}function ek({className:a,...b}){return(0,g.jsx)(c7,{"data-slot":"select-scroll-up-button",className:ee("flex cursor-default items-center justify-center py-1",a),...b,children:(0,g.jsx)(dn,{className:"size-4"})})}function el({className:a,...b}){return(0,g.jsx)(c9,{"data-slot":"select-scroll-down-button",className:ee("flex cursor-default items-center justify-center py-1",a),...b,children:(0,g.jsx)(dl,{className:"size-4"})})}let em=({title:a,allTags:b,selectedTag:c,onTagChange:d})=>(0,g.jsxs)("div",{className:"flex items-center gap-4",children:[(0,g.jsx)("h3",{className:"text-lg font-semibold",children:a}),(0,g.jsxs)(ef,{value:c,onValueChange:d,children:[(0,g.jsx)(eh,{className:"w-[200px]",children:(0,g.jsx)(eg,{placeholder:"Select a tag"})}),(0,g.jsx)(ei,{children:b.map(a=>(0,g.jsx)(ej,{value:a,children:a},a))})]})]});function en({className:a,...b}){return(0,g.jsx)("div",{"data-slot":"table-container",className:"relative w-full overflow-x-auto",children:(0,g.jsx)("table",{"data-slot":"table",className:ee("w-full caption-bottom text-sm",a),...b})})}function eo({className:a,...b}){return(0,g.jsx)("thead",{"data-slot":"table-header",className:ee("[&_tr]:border-b",a),...b})}function ep({className:a,...b}){return(0,g.jsx)("tbody",{"data-slot":"table-body",className:ee("[&_tr:last-child]:border-0",a),...b})}function eq({className:a,...b}){return(0,g.jsx)("tr",{"data-slot":"table-row",className:ee("hover:bg-muted/50 data-[state=selected]:bg-muted border-b transition-colors",a),...b})}function er({className:a,...b}){return(0,g.jsx)("th",{"data-slot":"table-head",className:ee("text-foreground h-10 px-2 text-left align-middle font-medium whitespace-nowrap [&:has([role=checkbox])]:pr-0 [&>[role=checkbox]]:translate-y-[2px]",a),...b})}function es({className:a,...b}){return(0,g.jsx)("td",{"data-slot":"table-cell",className:ee("p-2 align-middle whitespace-nowrap [&:has([role=checkbox])]:pr-0 [&>[role=checkbox]]:translate-y-[2px]",a),...b})}var et=a=>{let{present:b,children:c}=a,d=function(a){var b,c;let[d,e]=h.useState(),f=h.useRef(null),g=h.useRef(a),i=h.useRef("none"),[j,k]=(b=a?"mounted":"unmounted",c={mounted:{UNMOUNT:"unmounted",ANIMATION_OUT:"unmountSuspended"},unmountSuspended:{MOUNT:"mounted",ANIMATION_END:"unmounted"},unmounted:{MOUNT:"mounted"}},h.useReducer((a,b)=>c[a][b]??a,b));return h.useEffect(()=>{let a=eu(f.current);i.current="mounted"===j?a:"none"},[j]),V(()=>{let b=f.current,c=g.current;if(c!==a){let d=i.current,e=eu(b);a?k("MOUNT"):"none"===e||b?.display==="none"?k("UNMOUNT"):c&&d!==e?k("ANIMATION_OUT"):k("UNMOUNT"),g.current=a}},[a,k]),V(()=>{if(d){let a,b=d.ownerDocument.defaultView??window,c=c=>{let e=eu(f.current).includes(CSS.escape(c.animationName));if(c.target===d&&e&&(k("ANIMATION_END"),!g.current)){let c=d.style.animationFillMode;d.style.animationFillMode="forwards",a=b.setTimeout(()=>{"forwards"===d.style.animationFillMode&&(d.style.animationFillMode=c)})}},e=a=>{a.target===d&&(i.current=eu(f.current))};return d.addEventListener("animationstart",e),d.addEventListener("animationcancel",c),d.addEventListener("animationend",c),()=>{b.clearTimeout(a),d.removeEventListener("animationstart",e),d.removeEventListener("animationcancel",c),d.removeEventListener("animationend",c)}}k("ANIMATION_END")},[d,k]),{isPresent:["mounted","unmountSuspended"].includes(j),ref:h.useCallback(a=>{f.current=a?getComputedStyle(a):null,e(a)},[])}}(b),e="function"==typeof c?c({present:d.isPresent}):h.Children.only(c),f=t(d.ref,function(a){let b=Object.getOwnPropertyDescriptor(a.props,"ref")?.get,c=b&&"isReactWarning"in b&&b.isReactWarning;return c?a.ref:(c=(b=Object.getOwnPropertyDescriptor(a,"ref")?.get)&&"isReactWarning"in b&&b.isReactWarning)?a.props.ref:a.props.ref||a.ref}(e));return"function"==typeof c||d.isPresent?h.cloneElement(e,{ref:f}):null};function eu(a){return a?.animationName||"none"}et.displayName="Presence";var ev="Dialog",[ew,ex]=q(ev),[ey,ez]=ew(ev),eA=a=>{let{__scopeDialog:b,children:c,open:d,defaultOpen:e,onOpenChange:f,modal:i=!0}=a,j=h.useRef(null),k=h.useRef(null),[l,m]=bF({prop:d,defaultProp:e??!1,onChange:f,caller:ev});return(0,g.jsx)(ey,{scope:b,triggerRef:j,contentRef:k,contentId:Y(),titleId:Y(),descriptionId:Y(),open:l,onOpenChange:m,onOpenToggle:h.useCallback(()=>m(a=>!a),[m]),modal:i,children:c})};eA.displayName=ev;var eB="DialogTrigger",eC=h.forwardRef((a,b)=>{let{__scopeDialog:c,...d}=a,e=ez(eB,c),f=t(b,e.triggerRef);return(0,g.jsx)(C.button,{type:"button","aria-haspopup":"dialog","aria-expanded":e.open,"aria-controls":e.contentId,"data-state":eW(e.open),...d,ref:f,onClick:p(a.onClick,e.onOpenToggle)})});eC.displayName=eB;var eD="DialogPortal",[eE,eF]=ew(eD,{forceMount:void 0}),eG=a=>{let{__scopeDialog:b,forceMount:c,children:d,container:e}=a,f=ez(eD,b);return(0,g.jsx)(eE,{scope:b,forceMount:c,children:h.Children.map(d,a=>(0,g.jsx)(et,{present:c||f.open,children:(0,g.jsx)(bD,{asChild:!0,container:e,children:a})}))})};eG.displayName=eD;var eH="DialogOverlay",eI=h.forwardRef((a,b)=>{let c=eF(eH,a.__scopeDialog),{forceMount:d=c.forceMount,...e}=a,f=ez(eH,a.__scopeDialog);return f.modal?(0,g.jsx)(et,{present:d||f.open,children:(0,g.jsx)(eK,{...e,ref:b})}):null});eI.displayName=eH;var eJ=u("DialogOverlay.RemoveScroll"),eK=h.forwardRef((a,b)=>{let{__scopeDialog:c,...d}=a,e=ez(eH,c);return(0,g.jsx)(cp,{as:eJ,allowPinchZoom:!0,shards:[e.contentRef],children:(0,g.jsx)(C.div,{"data-state":eW(e.open),...d,ref:b,style:{pointerEvents:"auto",...d.style}})})}),eL="DialogContent",eM=h.forwardRef((a,b)=>{let c=eF(eL,a.__scopeDialog),{forceMount:d=c.forceMount,...e}=a,f=ez(eL,a.__scopeDialog);return(0,g.jsx)(et,{present:d||f.open,children:f.modal?(0,g.jsx)(eN,{...e,ref:b}):(0,g.jsx)(eO,{...e,ref:b})})});eM.displayName=eL;var eN=h.forwardRef((a,b)=>{let c=ez(eL,a.__scopeDialog),d=h.useRef(null),e=t(b,c.contentRef,d);return h.useEffect(()=>{let a=d.current;if(a)return bN(a)},[]),(0,g.jsx)(eP,{...a,ref:e,trapFocus:c.open,disableOutsidePointerEvents:!0,onCloseAutoFocus:p(a.onCloseAutoFocus,a=>{a.preventDefault(),c.triggerRef.current?.focus()}),onPointerDownOutside:p(a.onPointerDownOutside,a=>{let b=a.detail.originalEvent,c=0===b.button&&!0===b.ctrlKey;(2===b.button||c)&&a.preventDefault()}),onFocusOutside:p(a.onFocusOutside,a=>a.preventDefault())})}),eO=h.forwardRef((a,b)=>{let c=ez(eL,a.__scopeDialog),d=h.useRef(!1),e=h.useRef(!1);return(0,g.jsx)(eP,{...a,ref:b,trapFocus:!1,disableOutsidePointerEvents:!1,onCloseAutoFocus:b=>{a.onCloseAutoFocus?.(b),b.defaultPrevented||(d.current||c.triggerRef.current?.focus(),b.preventDefault()),d.current=!1,e.current=!1},onInteractOutside:b=>{a.onInteractOutside?.(b),b.defaultPrevented||(d.current=!0,"pointerdown"===b.detail.originalEvent.type&&(e.current=!0));let f=b.target;c.triggerRef.current?.contains(f)&&b.preventDefault(),"focusin"===b.detail.originalEvent.type&&e.current&&b.preventDefault()}})}),eP=h.forwardRef((a,b)=>{let{__scopeDialog:c,trapFocus:d,onOpenAutoFocus:e,onCloseAutoFocus:f,...i}=a,j=ez(eL,c),k=h.useRef(null),l=t(b,k);return K(),(0,g.jsxs)(g.Fragment,{children:[(0,g.jsx)(P,{asChild:!0,loop:!0,trapped:d,onMountAutoFocus:e,onUnmountAutoFocus:f,children:(0,g.jsx)(G,{role:"dialog",id:j.contentId,"aria-describedby":j.descriptionId,"aria-labelledby":j.titleId,"data-state":eW(j.open),...i,ref:l,onDismiss:()=>j.onOpenChange(!1)})}),(0,g.jsxs)(g.Fragment,{children:[(0,g.jsx)(e$,{titleId:j.titleId}),(0,g.jsx)(e_,{contentRef:k,descriptionId:j.descriptionId})]})]})}),eQ="DialogTitle",eR=h.forwardRef((a,b)=>{let{__scopeDialog:c,...d}=a,e=ez(eQ,c);return(0,g.jsx)(C.h2,{id:e.titleId,...d,ref:b})});eR.displayName=eQ;var eS="DialogDescription",eT=h.forwardRef((a,b)=>{let{__scopeDialog:c,...d}=a,e=ez(eS,c);return(0,g.jsx)(C.p,{id:e.descriptionId,...d,ref:b})});eT.displayName=eS;var eU="DialogClose",eV=h.forwardRef((a,b)=>{let{__scopeDialog:c,...d}=a,e=ez(eU,c);return(0,g.jsx)(C.button,{type:"button",...d,ref:b,onClick:p(a.onClick,()=>e.onOpenChange(!1))})});function eW(a){return a?"open":"closed"}eV.displayName=eU;var eX="DialogTitleWarning",[eY,eZ]=function(a,b){let c=h.createContext(b),d=a=>{let{children:b,...d}=a,e=h.useMemo(()=>d,Object.values(d));return(0,g.jsx)(c.Provider,{value:e,children:b})};return d.displayName=a+"Provider",[d,function(d){let e=h.useContext(c);if(e)return e;if(void 0!==b)return b;throw Error(`\`${d}\` must be used within \`${a}\``)}]}(eX,{contentName:eL,titleName:eQ,docsSlug:"dialog"}),e$=({titleId:a})=>{let b=eZ(eX),c=`\`${b.contentName}\` requires a \`${b.titleName}\` for the component to be accessible for screen reader users.

If you want to hide the \`${b.titleName}\`, you can wrap it with our VisuallyHidden component.

For more information, see https://radix-ui.com/primitives/docs/components/${b.docsSlug}`;return h.useEffect(()=>{a&&(document.getElementById(a)||console.error(c))},[c,a]),null},e_=({contentRef:a,descriptionId:b})=>{let c=eZ("DialogDescriptionWarning"),d=`Warning: Missing \`Description\` or \`aria-describedby={undefined}\` for {${c.contentName}}.`;return h.useEffect(()=>{let c=a.current?.getAttribute("aria-describedby");b&&c&&(document.getElementById(b)||console.warn(d))},[d,a,b]),null};let e0=dk("x",[["path",{d:"M18 6 6 18",key:"1bl5f8"}],["path",{d:"m6 6 12 12",key:"d8bk6v"}]]);function e1({...a}){return(0,g.jsx)(eA,{"data-slot":"dialog",...a})}function e2({...a}){return(0,g.jsx)(eC,{"data-slot":"dialog-trigger",...a})}function e3({...a}){return(0,g.jsx)(eG,{"data-slot":"dialog-portal",...a})}function e4({className:a,...b}){return(0,g.jsx)(eI,{"data-slot":"dialog-overlay",className:ee("data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 fixed inset-0 z-50 bg-black/50",a),...b})}function e5({className:a,children:b,showCloseButton:c=!0,...d}){return(0,g.jsxs)(e3,{"data-slot":"dialog-portal",children:[(0,g.jsx)(e4,{}),(0,g.jsxs)(eM,{"data-slot":"dialog-content",className:ee("bg-background data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 fixed top-[50%] left-[50%] z-50 grid w-full max-w-[calc(100%-2rem)] translate-x-[-50%] translate-y-[-50%] gap-4 rounded-lg border p-6 shadow-lg duration-200 sm:max-w-lg",a),...d,children:[b,c&&(0,g.jsxs)(eV,{"data-slot":"dialog-close",className:"ring-offset-background focus:ring-ring data-[state=open]:bg-accent data-[state=open]:text-muted-foreground absolute top-4 right-4 rounded-xs opacity-70 transition-opacity hover:opacity-100 focus:ring-2 focus:ring-offset-2 focus:outline-hidden disabled:pointer-events-none [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*='size-'])]:size-4",children:[(0,g.jsx)(e0,{}),(0,g.jsx)("span",{className:"sr-only",children:"Close"})]})]})]})}function e6({className:a,...b}){return(0,g.jsx)("div",{"data-slot":"dialog-header",className:ee("flex flex-col gap-2 text-center sm:text-left",a),...b})}function e7({className:a,...b}){return(0,g.jsx)(eR,{"data-slot":"dialog-title",className:ee("text-lg leading-none font-semibold",a),...b})}function e8({className:a,...b}){return(0,g.jsx)(eT,{"data-slot":"dialog-description",className:ee("text-muted-foreground text-sm",a),...b})}let e9=a=>"boolean"==typeof a?`${a}`:0===a?"0":a,fa=(a,b)=>c=>{var d;if((null==b?void 0:b.variants)==null)return dp(a,null==c?void 0:c.class,null==c?void 0:c.className);let{variants:e,defaultVariants:f}=b,g=Object.keys(e).map(a=>{let b=null==c?void 0:c[a],d=null==f?void 0:f[a];if(null===b)return null;let g=e9(b)||e9(d);return e[a][g]}),h=c&&Object.entries(c).reduce((a,b)=>{let[c,d]=b;return void 0===d||(a[c]=d),a},{});return dp(a,g,null==b||null==(d=b.compoundVariants)?void 0:d.reduce((a,b)=>{let{class:c,className:d,...e}=b;return Object.entries(e).every(a=>{let[b,c]=a;return Array.isArray(c)?c.includes({...f,...h}[b]):({...f,...h})[b]===c})?[...a,c,d]:a},[]),null==c?void 0:c.class,null==c?void 0:c.className)},fb=fa("inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",{variants:{variant:{default:"bg-primary text-primary-foreground shadow-xs hover:bg-primary/90",destructive:"bg-destructive text-white shadow-xs hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",outline:"border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50",secondary:"bg-secondary text-secondary-foreground shadow-xs hover:bg-secondary/80",ghost:"hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50",link:"text-primary underline-offset-4 hover:underline"},size:{default:"h-9 px-4 py-2 has-[>svg]:px-3",sm:"h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5",lg:"h-10 rounded-md px-6 has-[>svg]:px-4",icon:"size-9"}},defaultVariants:{variant:"default",size:"default"}});function fc({className:a,variant:b,size:c,asChild:d=!1,...e}){return(0,g.jsx)(d?v:"button",{"data-slot":"button",className:ee(fb({variant:b,size:c,className:a})),...e})}let fd=dk("external-link",[["path",{d:"M15 3h6v6",key:"1q9fwt"}],["path",{d:"M10 14 21 3",key:"gplh6r"}],["path",{d:"M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6",key:"a6xqqp"}]]),fe=fa("inline-flex items-center justify-center rounded-md border px-2 py-0.5 text-xs font-medium w-fit whitespace-nowrap shrink-0 [&>svg]:size-3 gap-1 [&>svg]:pointer-events-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive transition-[color,box-shadow] overflow-hidden",{variants:{variant:{default:"border-transparent bg-primary text-primary-foreground [a&]:hover:bg-primary/90",secondary:"border-transparent bg-secondary text-secondary-foreground [a&]:hover:bg-secondary/90",destructive:"border-transparent bg-destructive text-white [a&]:hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",outline:"text-foreground [a&]:hover:bg-accent [a&]:hover:text-accent-foreground"}},defaultVariants:{variant:"default"}});function ff({className:a,variant:b,asChild:c=!1,...d}){return(0,g.jsx)(c?v:"span",{"data-slot":"badge",className:ee(fe({variant:b}),a),...d})}var fg=c(1261),fh=c.n(fg);let fi=({itemViewList:a})=>(0,g.jsx)("div",{className:"rounded-md border overflow-x-auto",children:(0,g.jsxs)(en,{children:[(0,g.jsx)(eo,{children:(0,g.jsxs)(eq,{children:[(0,g.jsx)(er,{className:"w-[100px] text-center",children:"Publication"}),(0,g.jsx)(er,{className:"w-[120px]",children:"Abbr."}),(0,g.jsx)(er,{children:"Title"}),(0,g.jsx)(er,{className:"w-[100px]",children:"Conference"}),(0,g.jsx)(er,{className:"w-[50px]",children:"Link"}),(0,g.jsx)(er,{className:"w-[50px]",children:"Page"}),(0,g.jsx)(er,{className:"w-[100px]",children:"Code"}),(0,g.jsx)(er,{className:"w-[180px]",children:"Backbone"}),(0,g.jsx)(er,{className:"w-[180px]",children:"Approach"})]})}),(0,g.jsx)(ep,{children:a.map(a=>(0,g.jsxs)(eq,{children:[(0,g.jsx)(es,{className:"text-center",children:a.publication}),(0,g.jsx)(es,{children:(0,g.jsxs)(e1,{children:[(0,g.jsx)(e2,{asChild:!0,children:(0,g.jsx)(fc,{variant:"link",className:"p-0 h-auto text-blue-600 hover:text-blue-800",children:a.abbr})}),(0,g.jsxs)(e5,{className:"sm:max-w-[800px]",children:[(0,g.jsxs)(e6,{children:[(0,g.jsx)(e7,{children:a.title}),(0,g.jsx)(e8,{children:a.authors.join(", ")})]}),(0,g.jsx)("p",{className:"text-sm text-gray-600",children:a.abstract})]})]})}),(0,g.jsx)(es,{className:"max-w-xs",children:(0,g.jsx)("div",{className:"truncate",title:a.title,children:a.title})}),(0,g.jsx)(es,{children:a.conference}),(0,g.jsx)(es,{children:a.link&&(0,g.jsx)("a",{href:a.link,target:"_blank",rel:"noopener noreferrer",className:"flex justify-center items-center",children:(0,g.jsx)(fd,{className:"w-4 h-4"})})}),(0,g.jsx)(es,{children:a.page&&(0,g.jsx)("a",{href:a.page,target:"_blank",rel:"noopener noreferrer",className:"flex justify-center items-center",children:(0,g.jsx)(fd,{className:"w-4 h-4"})})}),(0,g.jsx)(es,{children:a.code&&(0,g.jsx)("a",{href:a.code,target:"_blank",rel:"noopener noreferrer",className:"flex justify-center items-center",children:(0,g.jsx)(fh(),{src:(a=>{if(!a||!a.includes("github.com"))return"";let b=a.split("/"),c=b[3],d=b[4];return c&&d?`https://img.shields.io/github/stars/${c}/${d}`:""})(a.code),alt:"GitHub stars",className:"h-5",width:90,height:20,unoptimized:!0})})}),(0,g.jsx)(es,{children:(0,g.jsx)("div",{className:"flex flex-wrap gap-1",children:a.backbone_tags.map(a=>(0,g.jsx)(ff,{variant:"secondary",className:"text-xs whitespace-nowrap",children:a},a))})}),(0,g.jsx)(es,{children:(0,g.jsx)("div",{className:"flex flex-wrap gap-1",children:a.approach_tags.map(a=>(0,g.jsx)(ff,{variant:"secondary",className:"text-xs whitespace-nowrap",children:a},a))})})]},a.arxiv_id))})]})});c(4263);let fj=function(){let a=(0,h.useMemo)(()=>["All",...new Set(j.flatMap(a=>l(a.backbone_tags)))],[]),b=(0,h.useMemo)(()=>["All",...new Set(j.flatMap(a=>l(a.approach_tags)))],[]),[c,d]=(0,h.useState)("All"),[e,f]=(0,h.useState)("All"),[i,n]=(0,h.useState)(!1),[o,p]=(0,h.useState)(!1),[q,r]=(0,h.useState)(!1),s=(0,h.useMemo)(()=>{let a=j.filter(a=>!0===a.survey);return a.sort((a,b)=>{let c=100*a.year+a.month;return 100*b.year+b.month-c}),a.map(a=>m(a,k.Survey))},[]),t=(0,h.useMemo)(()=>{let a=j.filter(a=>!0===a.dataset);return a.sort((a,b)=>{let c=100*a.year+a.month;return 100*b.year+b.month-c}),a.map(a=>m(a,k.Dataset))},[]),u=(0,h.useMemo)(()=>{let a=j.filter(a=>{if(!0!==a.model)return!1;let b=a.submission&&""!==a.submission.trim();if(i&&!b||o&&(!a.page||""===a.page.trim())||q&&(!a.repo||""===a.repo.trim()))return!1;let d=l(a.backbone_tags),f=l(a.approach_tags),g="All"===c||d.includes(c),h="All"===e||f.includes(e);return g&&h});return a.sort((a,b)=>{let c=100*a.year+a.month;return 100*b.year+b.month-c}),a.map(a=>m(a,k.Model))},[c,e,i,o,q]);return(0,g.jsxs)("div",{className:"container mx-auto p-0 pb-10 space-y-10 max-w-screen-3xl",children:[(0,g.jsx)("h1",{className:"text-6xl font-bold text-center mb-16 mt-16",children:"awesome-text-to-motion"}),(0,g.jsx)("h2",{className:"text-4xl font-bold text-left",children:"Surveys"}),(0,g.jsx)(fi,{itemViewList:s}),(0,g.jsxs)("div",{className:"text-center text-muted-foreground",children:["Showing ",s.length," / ",j.filter(a=>!0===a.survey).length," surveys"]}),(0,g.jsx)("h2",{className:"text-4xl font-bold text-left",children:"Datasets"}),(0,g.jsx)(fi,{itemViewList:t}),(0,g.jsxs)("div",{className:"text-center text-muted-foreground",children:["Showing ",t.length," / ",j.filter(a=>!0===a.dataset).length," datasets"]}),(0,g.jsx)("h2",{className:"text-4xl font-bold text-left",children:"Models"}),(0,g.jsxs)("div",{className:"space-y-8",children:[(0,g.jsx)(em,{title:"Backbone Tag",allTags:a,selectedTag:c,onTagChange:d}),(0,g.jsx)(em,{title:"Approach Tag",allTags:b,selectedTag:e,onTagChange:f}),(0,g.jsxs)("div",{children:[(0,g.jsx)("h3",{className:"text-lg font-semibold mb-2",children:"Filter Options"}),(0,g.jsxs)("div",{className:"flex flex-wrap gap-4",children:[(0,g.jsxs)("div",{className:"flex items-center space-x-2",children:[(0,g.jsx)("input",{type:"checkbox",checked:i,onChange:a=>n(a.target.checked)}),(0,g.jsx)("label",{children:"Show Published Only"})]}),(0,g.jsxs)("div",{className:"flex items-center space-x-2",children:[(0,g.jsx)("input",{type:"checkbox",checked:o,onChange:a=>p(a.target.checked)}),(0,g.jsx)("label",{children:"Show with Page Only"})]}),(0,g.jsxs)("div",{className:"flex items-center space-x-2",children:[(0,g.jsx)("input",{type:"checkbox",checked:q,onChange:a=>r(a.target.checked)}),(0,g.jsx)("label",{children:"Show with Code Repo Only"})]})]})]})]}),(0,g.jsx)(fi,{itemViewList:u}),(0,g.jsxs)("div",{className:"text-center text-muted-foreground",children:["Showing ",u.length," / ",j.filter(a=>!0===a.model).length," models"]})]})}},4953:(a,b,c)=>{"use strict";Object.defineProperty(b,"__esModule",{value:!0}),Object.defineProperty(b,"getImgProps",{enumerable:!0,get:function(){return i}}),c(148);let d=c(1480),e=c(2756),f=["-moz-initial","fill","none","scale-down",void 0];function g(a){return void 0!==a.default}function h(a){return void 0===a?a:"number"==typeof a?Number.isFinite(a)?a:NaN:"string"==typeof a&&/^[0-9]+$/.test(a)?parseInt(a,10):NaN}function i(a,b){var c,i;let j,k,l,{src:m,sizes:n,unoptimized:o=!1,priority:p=!1,loading:q,className:r,quality:s,width:t,height:u,fill:v=!1,style:w,overrideSrc:x,onLoad:y,onLoadingComplete:z,placeholder:A="empty",blurDataURL:B,fetchPriority:C,decoding:D="async",layout:E,objectFit:F,objectPosition:G,lazyBoundary:H,lazyRoot:I,...J}=a,{imgConf:K,showAltText:L,blurComplete:M,defaultLoader:N}=b,O=K||e.imageConfigDefault;if("allSizes"in O)j=O;else{let a=[...O.deviceSizes,...O.imageSizes].sort((a,b)=>a-b),b=O.deviceSizes.sort((a,b)=>a-b),d=null==(c=O.qualities)?void 0:c.sort((a,b)=>a-b);j={...O,allSizes:a,deviceSizes:b,qualities:d}}if(void 0===N)throw Object.defineProperty(Error("images.loaderFile detected but the file is missing default export.\nRead more: https://nextjs.org/docs/messages/invalid-images-config"),"__NEXT_ERROR_CODE",{value:"E163",enumerable:!1,configurable:!0});let P=J.loader||N;delete J.loader,delete J.srcSet;let Q="__next_img_default"in P;if(Q){if("custom"===j.loader)throw Object.defineProperty(Error('Image with src "'+m+'" is missing "loader" prop.\nRead more: https://nextjs.org/docs/messages/next-image-missing-loader'),"__NEXT_ERROR_CODE",{value:"E252",enumerable:!1,configurable:!0})}else{let a=P;P=b=>{let{config:c,...d}=b;return a(d)}}if(E){"fill"===E&&(v=!0);let a={intrinsic:{maxWidth:"100%",height:"auto"},responsive:{width:"100%",height:"auto"}}[E];a&&(w={...w,...a});let b={responsive:"100vw",fill:"100vw"}[E];b&&!n&&(n=b)}let R="",S=h(t),T=h(u);if((i=m)&&"object"==typeof i&&(g(i)||void 0!==i.src)){let a=g(m)?m.default:m;if(!a.src)throw Object.defineProperty(Error("An object should only be passed to the image component src parameter if it comes from a static image import. It must include src. Received "+JSON.stringify(a)),"__NEXT_ERROR_CODE",{value:"E460",enumerable:!1,configurable:!0});if(!a.height||!a.width)throw Object.defineProperty(Error("An object should only be passed to the image component src parameter if it comes from a static image import. It must include height and width. Received "+JSON.stringify(a)),"__NEXT_ERROR_CODE",{value:"E48",enumerable:!1,configurable:!0});if(k=a.blurWidth,l=a.blurHeight,B=B||a.blurDataURL,R=a.src,!v)if(S||T){if(S&&!T){let b=S/a.width;T=Math.round(a.height*b)}else if(!S&&T){let b=T/a.height;S=Math.round(a.width*b)}}else S=a.width,T=a.height}let U=!p&&("lazy"===q||void 0===q);(!(m="string"==typeof m?m:R)||m.startsWith("data:")||m.startsWith("blob:"))&&(o=!0,U=!1),j.unoptimized&&(o=!0),Q&&!j.dangerouslyAllowSVG&&m.split("?",1)[0].endsWith(".svg")&&(o=!0);let V=h(s),W=Object.assign(v?{position:"absolute",height:"100%",width:"100%",left:0,top:0,right:0,bottom:0,objectFit:F,objectPosition:G}:{},L?{}:{color:"transparent"},w),X=M||"empty"===A?null:"blur"===A?'url("data:image/svg+xml;charset=utf-8,'+(0,d.getImageBlurSvg)({widthInt:S,heightInt:T,blurWidth:k,blurHeight:l,blurDataURL:B||"",objectFit:W.objectFit})+'")':'url("'+A+'")',Y=f.includes(W.objectFit)?"fill"===W.objectFit?"100% 100%":"cover":W.objectFit,Z=X?{backgroundSize:Y,backgroundPosition:W.objectPosition||"50% 50%",backgroundRepeat:"no-repeat",backgroundImage:X}:{},$=function(a){let{config:b,src:c,unoptimized:d,width:e,quality:f,sizes:g,loader:h}=a;if(d)return{src:c,srcSet:void 0,sizes:void 0};let{widths:i,kind:j}=function(a,b,c){let{deviceSizes:d,allSizes:e}=a;if(c){let a=/(^|\s)(1?\d?\d)vw/g,b=[];for(let d;d=a.exec(c);)b.push(parseInt(d[2]));if(b.length){let a=.01*Math.min(...b);return{widths:e.filter(b=>b>=d[0]*a),kind:"w"}}return{widths:e,kind:"w"}}return"number"!=typeof b?{widths:d,kind:"w"}:{widths:[...new Set([b,2*b].map(a=>e.find(b=>b>=a)||e[e.length-1]))],kind:"x"}}(b,e,g),k=i.length-1;return{sizes:g||"w"!==j?g:"100vw",srcSet:i.map((a,d)=>h({config:b,src:c,quality:f,width:a})+" "+("w"===j?a:d+1)+j).join(", "),src:h({config:b,src:c,quality:f,width:i[k]})}}({config:j,src:m,unoptimized:o,width:S,quality:V,sizes:n,loader:P});return{props:{...J,loading:U?"lazy":q,fetchPriority:C,width:S,height:T,decoding:D,className:r,style:{...W,...Z},sizes:$.sizes,srcSet:$.srcSet,src:x||$.src},meta:{unoptimized:o,priority:p,placeholder:A,fill:v}}}},4959:(a,b,c)=>{"use strict";a.exports=c(4041).vendored.contexts.AmpContext},5091:()=>{},6439:a=>{"use strict";a.exports=require("next/dist/shared/lib/no-fallback-error.external")},6533:(a,b,c)=>{"use strict";Object.defineProperty(b,"__esModule",{value:!0}),Object.defineProperty(b,"Image",{enumerable:!0,get:function(){return u}});let d=c(4985),e=c(740),f=c(687),g=e._(c(3210)),h=d._(c(1215)),i=d._(c(512)),j=c(4953),k=c(2756),l=c(7903);c(148);let m=c(9148),n=d._(c(1933)),o=c(3038),p={deviceSizes:[640,750,828,1080,1200,1920,2048,3840],imageSizes:[16,32,48,64,96,128,256,384],path:"/_next/image",loader:"default",dangerouslyAllowSVG:!1,unoptimized:!1};function q(a,b,c,d,e,f,g){let h=null==a?void 0:a.src;a&&a["data-loaded-src"]!==h&&(a["data-loaded-src"]=h,("decode"in a?a.decode():Promise.resolve()).catch(()=>{}).then(()=>{if(a.parentElement&&a.isConnected){if("empty"!==b&&e(!0),null==c?void 0:c.current){let b=new Event("load");Object.defineProperty(b,"target",{writable:!1,value:a});let d=!1,e=!1;c.current({...b,nativeEvent:b,currentTarget:a,target:a,isDefaultPrevented:()=>d,isPropagationStopped:()=>e,persist:()=>{},preventDefault:()=>{d=!0,b.preventDefault()},stopPropagation:()=>{e=!0,b.stopPropagation()}})}(null==d?void 0:d.current)&&d.current(a)}}))}function r(a){return g.use?{fetchPriority:a}:{fetchpriority:a}}globalThis.__NEXT_IMAGE_IMPORTED=!0;let s=(0,g.forwardRef)((a,b)=>{let{src:c,srcSet:d,sizes:e,height:h,width:i,decoding:j,className:k,style:l,fetchPriority:m,placeholder:n,loading:p,unoptimized:s,fill:t,onLoadRef:u,onLoadingCompleteRef:v,setBlurComplete:w,setShowAltText:x,sizesInput:y,onLoad:z,onError:A,...B}=a,C=(0,g.useCallback)(a=>{a&&(A&&(a.src=a.src),a.complete&&q(a,n,u,v,w,s,y))},[c,n,u,v,w,A,s,y]),D=(0,o.useMergedRef)(b,C);return(0,f.jsx)("img",{...B,...r(m),loading:p,width:i,height:h,decoding:j,"data-nimg":t?"fill":"1",className:k,style:l,sizes:e,srcSet:d,src:c,ref:D,onLoad:a=>{q(a.currentTarget,n,u,v,w,s,y)},onError:a=>{x(!0),"empty"!==n&&w(!0),A&&A(a)}})});function t(a){let{isAppRouter:b,imgAttributes:c}=a,d={as:"image",imageSrcSet:c.srcSet,imageSizes:c.sizes,crossOrigin:c.crossOrigin,referrerPolicy:c.referrerPolicy,...r(c.fetchPriority)};return b&&h.default.preload?(h.default.preload(c.src,d),null):(0,f.jsx)(i.default,{children:(0,f.jsx)("link",{rel:"preload",href:c.srcSet?void 0:c.src,...d},"__nimg-"+c.src+c.srcSet+c.sizes)})}let u=(0,g.forwardRef)((a,b)=>{let c=(0,g.useContext)(m.RouterContext),d=(0,g.useContext)(l.ImageConfigContext),e=(0,g.useMemo)(()=>{var a;let b=p||d||k.imageConfigDefault,c=[...b.deviceSizes,...b.imageSizes].sort((a,b)=>a-b),e=b.deviceSizes.sort((a,b)=>a-b),f=null==(a=b.qualities)?void 0:a.sort((a,b)=>a-b);return{...b,allSizes:c,deviceSizes:e,qualities:f}},[d]),{onLoad:h,onLoadingComplete:i}=a,o=(0,g.useRef)(h);(0,g.useEffect)(()=>{o.current=h},[h]);let q=(0,g.useRef)(i);(0,g.useEffect)(()=>{q.current=i},[i]);let[r,u]=(0,g.useState)(!1),[v,w]=(0,g.useState)(!1),{props:x,meta:y}=(0,j.getImgProps)(a,{defaultLoader:n.default,imgConf:e,blurComplete:r,showAltText:v});return(0,f.jsxs)(f.Fragment,{children:[(0,f.jsx)(s,{...x,unoptimized:y.unoptimized,placeholder:y.placeholder,fill:y.fill,onLoadRef:o,onLoadingCompleteRef:q,setBlurComplete:u,setShowAltText:w,sizesInput:a.sizes,ref:b}),y.priority?(0,f.jsx)(t,{isAppRouter:!c,imgAttributes:x}):null]})});("function"==typeof b.default||"object"==typeof b.default&&null!==b.default)&&void 0===b.default.__esModule&&(Object.defineProperty(b.default,"__esModule",{value:!0}),Object.assign(b.default,b),a.exports=b.default)},6713:a=>{"use strict";a.exports=require("next/dist/shared/lib/router/utils/is-bot")},7755:(a,b,c)=>{"use strict";Object.defineProperty(b,"__esModule",{value:!0}),Object.defineProperty(b,"default",{enumerable:!0,get:function(){return f}});let d=c(3210),e=()=>{};function f(a){var b;let{headManager:c,reduceComponentsToState:f}=a;function g(){if(c&&c.mountedInstances){let b=d.Children.toArray(Array.from(c.mountedInstances).filter(Boolean));c.updateHead(f(b,a))}}return null==c||null==(b=c.mountedInstances)||b.add(a.children),g(),e(()=>{var b;return null==c||null==(b=c.mountedInstances)||b.add(a.children),()=>{var b;null==c||null==(b=c.mountedInstances)||b.delete(a.children)}}),e(()=>(c&&(c._pendingUpdate=g),()=>{c&&(c._pendingUpdate=g)})),null}},7903:(a,b,c)=>{"use strict";a.exports=c(4041).vendored.contexts.ImageConfigContext},8297:(a,b,c)=>{Promise.resolve().then(c.t.bind(c,6133,23)),Promise.resolve().then(c.t.bind(c,6444,23)),Promise.resolve().then(c.t.bind(c,6042,23)),Promise.resolve().then(c.t.bind(c,9477,23)),Promise.resolve().then(c.t.bind(c,9345,23)),Promise.resolve().then(c.t.bind(c,2089,23)),Promise.resolve().then(c.t.bind(c,6577,23)),Promise.resolve().then(c.t.bind(c,1307,23)),Promise.resolve().then(c.t.bind(c,4817,23))},8354:a=>{"use strict";a.exports=require("util")},8400:(a,b,c)=>{"use strict";c.r(b),c.d(b,{GlobalError:()=>C.a,__next_app__:()=>I,handler:()=>K,pages:()=>H,routeModule:()=>J,tree:()=>G});var d=c(5239),e=c(8088),f=c(7220),g=c(1289),h=c(6191),i=c(4823),j=c(1998),k=c(2603),l=c(4649),m=c(2781),n=c(2602),o=c(1268),p=c(4853),q=c(261),r=c(5052),s=c(9977),t=c(6713),u=c(3365),v=c(1454),w=c(7778),x=c(6143),y=c(9105),z=c(8171),A=c(6439),B=c(6133),C=c.n(B),D=c(893),E=c(2836),F={};for(let a in D)0>["default","tree","pages","GlobalError","__next_app__","routeModule","handler"].indexOf(a)&&(F[a]=()=>D[a]);c.d(b,F);let G={children:["",{children:["__PAGE__",{},{page:[()=>Promise.resolve().then(c.bind(c,1204)),"/home/user/Desktop/awesome-text-to-motion/page/src/app/page.tsx"],metadata:{icon:[async a=>(await Promise.resolve().then(c.bind(c,440))).default(a)],apple:[],openGraph:[],twitter:[],manifest:void 0}}]},{layout:[()=>Promise.resolve().then(c.bind(c,4431)),"/home/user/Desktop/awesome-text-to-motion/page/src/app/layout.tsx"],"global-error":[()=>Promise.resolve().then(c.t.bind(c,6133,23)),"next/dist/client/components/builtin/global-error.js"],"not-found":[()=>Promise.resolve().then(c.t.bind(c,849,23)),"next/dist/client/components/builtin/not-found.js"],forbidden:[()=>Promise.resolve().then(c.t.bind(c,9868,23)),"next/dist/client/components/builtin/forbidden.js"],unauthorized:[()=>Promise.resolve().then(c.t.bind(c,9615,23)),"next/dist/client/components/builtin/unauthorized.js"],metadata:{icon:[async a=>(await Promise.resolve().then(c.bind(c,440))).default(a)],apple:[],openGraph:[],twitter:[],manifest:void 0}}]}.children,H=["/home/user/Desktop/awesome-text-to-motion/page/src/app/page.tsx"],I={require:c,loadChunk:()=>Promise.resolve()},J=new d.AppPageRouteModule({definition:{kind:e.RouteKind.APP_PAGE,page:"/page",pathname:"/",bundlePath:"",filename:"",appPaths:[]},userland:{loaderTree:G},distDir:".next",projectDir:""});async function K(a,b,c){var d;let B="/page";"/index"===B&&(B="/");let F="false",L=(0,h.getRequestMeta)(a,"postponed"),M=(0,h.getRequestMeta)(a,"minimalMode"),N=await J.prepare(a,b,{srcPage:B,multiZoneDraftMode:F});if(!N)return b.statusCode=400,b.end("Bad Request"),null==c.waitUntil||c.waitUntil.call(c,Promise.resolve()),null;let{buildId:O,query:P,params:Q,parsedUrl:R,pageIsDynamic:S,buildManifest:T,nextFontManifest:U,reactLoadableManifest:V,serverActionsManifest:W,clientReferenceManifest:X,subresourceIntegrityManifest:Y,prerenderManifest:Z,isDraftMode:$,resolvedPathname:_,revalidateOnlyGenerated:aa,routerServerContext:ab,nextConfig:ac}=N,ad=R.pathname||"/",ae=(0,q.normalizeAppPath)(B),{isOnDemandRevalidate:af}=N,ag=Z.dynamicRoutes[ae],ah=Z.routes[_],ai=!!(ag||ah||Z.routes[ae]),aj=a.headers["user-agent"]||"",ak=(0,t.getBotType)(aj),al=(0,o.isHtmlBotRequest)(a),am=(0,h.getRequestMeta)(a,"isPrefetchRSCRequest")??!!a.headers[s.NEXT_ROUTER_PREFETCH_HEADER],an=(0,h.getRequestMeta)(a,"isRSCRequest")??!!a.headers[s.RSC_HEADER],ao=(0,r.getIsPossibleServerAction)(a),ap=(0,l.checkIsAppPPREnabled)(ac.experimental.ppr)&&(null==(d=Z.routes[ae]??Z.dynamicRoutes[ae])?void 0:d.renderingMode)==="PARTIALLY_STATIC",aq=!1,ar=!1,as=ap?L:void 0,at=ap&&an&&!am,au=(0,h.getRequestMeta)(a,"segmentPrefetchRSCRequest"),av=!aj||(0,o.shouldServeStreamingMetadata)(aj,ac.htmlLimitedBots);al&&ap&&(ai=!1,av=!1);let aw=!0===J.isDev||!ai||"string"==typeof L||at,ax=al&&ap,ay=null;$||!ai||aw||ao||as||at||(ay=_);let az=ay;!az&&J.isDev&&(az=_);let aA={...D,tree:G,pages:H,GlobalError:C(),handler:K,routeModule:J,__next_app__:I};W&&X&&(0,n.setReferenceManifestsSingleton)({page:B,clientReferenceManifest:X,serverActionsManifest:W,serverModuleMap:(0,p.createServerModuleMap)({serverActionsManifest:W})});let aB=a.method||"GET",aC=(0,g.getTracer)(),aD=aC.getActiveScopeSpan();try{let d=async(c,d)=>{let e=new k.NodeNextRequest(a),f=new k.NodeNextResponse(b);return J.render(e,f,d).finally(()=>{if(!c)return;c.setAttributes({"http.status_code":b.statusCode,"next.rsc":!1});let d=aC.getRootSpanAttributes();if(!d)return;if(d.get("next.span_type")!==i.BaseServerSpan.handleRequest)return void console.warn(`Unexpected root span type '${d.get("next.span_type")}'. Please report this Next.js issue https://github.com/vercel/next.js`);let e=d.get("next.route");if(e){let a=`${aB} ${e}`;c.setAttributes({"next.route":e,"http.route":e,"next.span_name":a}),c.updateName(a)}else c.updateName(`${aB} ${a.url}`)})},f=async({span:e,postponed:f,fallbackRouteParams:g})=>{let i={query:P,params:Q,page:ae,sharedContext:{buildId:O},serverComponentsHmrCache:(0,h.getRequestMeta)(a,"serverComponentsHmrCache"),fallbackRouteParams:g,renderOpts:{App:()=>null,Document:()=>null,pageConfig:{},ComponentMod:aA,Component:(0,j.T)(aA),params:Q,routeModule:J,page:B,postponed:f,shouldWaitOnAllReady:ax,serveStreamingMetadata:av,supportsDynamicResponse:"string"==typeof f||aw,buildManifest:T,nextFontManifest:U,reactLoadableManifest:V,subresourceIntegrityManifest:Y,serverActionsManifest:W,clientReferenceManifest:X,setIsrStatus:null==ab?void 0:ab.setIsrStatus,dir:J.projectDir,isDraftMode:$,isRevalidate:ai&&!f&&!at,botType:ak,isOnDemandRevalidate:af,isPossibleServerAction:ao,assetPrefix:ac.assetPrefix,nextConfigOutput:ac.output,crossOrigin:ac.crossOrigin,trailingSlash:ac.trailingSlash,previewProps:Z.preview,deploymentId:ac.deploymentId,enableTainting:ac.experimental.taint,htmlLimitedBots:ac.htmlLimitedBots,devtoolSegmentExplorer:ac.experimental.devtoolSegmentExplorer,reactMaxHeadersLength:ac.reactMaxHeadersLength,multiZoneDraftMode:F,incrementalCache:(0,h.getRequestMeta)(a,"incrementalCache"),cacheLifeProfiles:ac.experimental.cacheLife,basePath:ac.basePath,serverActions:ac.experimental.serverActions,...aq?{nextExport:!0,supportsDynamicResponse:!1,isStaticGeneration:!0,isRevalidate:!0,isDebugDynamicAccesses:aq}:{},experimental:{isRoutePPREnabled:ap,expireTime:ac.expireTime,staleTimes:ac.experimental.staleTimes,dynamicIO:!!ac.experimental.dynamicIO,clientSegmentCache:!!ac.experimental.clientSegmentCache,dynamicOnHover:!!ac.experimental.dynamicOnHover,inlineCss:!!ac.experimental.inlineCss,authInterrupts:!!ac.experimental.authInterrupts,clientTraceMetadata:ac.experimental.clientTraceMetadata||[]},waitUntil:c.waitUntil,onClose:a=>{b.on("close",a)},onAfterTaskError:()=>{},onInstrumentationRequestError:(b,c,d)=>J.onRequestError(a,b,d,ab),err:(0,h.getRequestMeta)(a,"invokeError"),dev:J.isDev}},k=await d(e,i),{metadata:l}=k,{cacheControl:m,headers:n={},fetchTags:o}=l;if(o&&(n[x.NEXT_CACHE_TAGS_HEADER]=o),a.fetchMetrics=l.fetchMetrics,ai&&(null==m?void 0:m.revalidate)===0&&!J.isDev&&!ap){let a=l.staticBailoutInfo,b=Object.defineProperty(Error(`Page changed from static to dynamic at runtime ${_}${(null==a?void 0:a.description)?`, reason: ${a.description}`:""}
see more here https://nextjs.org/docs/messages/app-static-to-dynamic-error`),"__NEXT_ERROR_CODE",{value:"E132",enumerable:!1,configurable:!0});if(null==a?void 0:a.stack){let c=a.stack;b.stack=b.message+c.substring(c.indexOf("\n"))}throw b}return{value:{kind:u.CachedRouteKind.APP_PAGE,html:k,headers:n,rscData:l.flightData,postponed:l.postponed,status:l.statusCode,segmentData:l.segmentData},cacheControl:m}},l=async({hasResolved:d,previousCacheEntry:g,isRevalidating:i,span:j})=>{let k,l=!1===J.isDev,n=d||b.writableEnded;if(af&&aa&&!g&&!M)return(null==ab?void 0:ab.render404)?await ab.render404(a,b):(b.statusCode=404,b.end("This page could not be found")),null;if(ag&&(k=(0,v.parseFallbackField)(ag.fallback)),k===v.FallbackMode.PRERENDER&&(0,t.isBot)(aj)&&(k=v.FallbackMode.BLOCKING_STATIC_RENDER),(null==g?void 0:g.isStale)===-1&&(af=!0),af&&(k!==v.FallbackMode.NOT_FOUND||g)&&(k=v.FallbackMode.BLOCKING_STATIC_RENDER),!M&&k!==v.FallbackMode.BLOCKING_STATIC_RENDER&&az&&!n&&!$&&S&&(l||!ah)){let b;if((l||ag)&&k===v.FallbackMode.NOT_FOUND)throw new A.NoFallbackError;if(ap&&!an){if(b=await J.handleResponse({cacheKey:l?ae:null,req:a,nextConfig:ac,routeKind:e.RouteKind.APP_PAGE,isFallback:!0,prerenderManifest:Z,isRoutePPREnabled:ap,responseGenerator:async()=>f({span:j,postponed:void 0,fallbackRouteParams:l||ar?(0,m.u)(ae):null}),waitUntil:c.waitUntil}),null===b)return null;if(b)return delete b.cacheControl,b}}let o=af||i||!as?void 0:as;if(aq&&void 0!==o)return{cacheControl:{revalidate:1,expire:void 0},value:{kind:u.CachedRouteKind.PAGES,html:w.default.fromStatic(""),pageData:{},headers:void 0,status:void 0}};let p=S&&ap&&((0,h.getRequestMeta)(a,"renderFallbackShell")||ar)?(0,m.u)(ad):null;return f({span:j,postponed:o,fallbackRouteParams:p})},n=async d=>{var g,i,j,k,m;let n,o=await J.handleResponse({cacheKey:ay,responseGenerator:a=>l({span:d,...a}),routeKind:e.RouteKind.APP_PAGE,isOnDemandRevalidate:af,isRoutePPREnabled:ap,req:a,nextConfig:ac,prerenderManifest:Z,waitUntil:c.waitUntil});if($&&b.setHeader("Cache-Control","private, no-cache, no-store, max-age=0, must-revalidate"),J.isDev&&b.setHeader("Cache-Control","no-store, must-revalidate"),!o){if(ay)throw Object.defineProperty(Error("invariant: cache entry required but not generated"),"__NEXT_ERROR_CODE",{value:"E62",enumerable:!1,configurable:!0});return null}if((null==(g=o.value)?void 0:g.kind)!==u.CachedRouteKind.APP_PAGE)throw Object.defineProperty(Error(`Invariant app-page handler received invalid cache entry ${null==(j=o.value)?void 0:j.kind}`),"__NEXT_ERROR_CODE",{value:"E707",enumerable:!1,configurable:!0});let p="string"==typeof o.value.postponed;ai&&!at&&(!p||am)&&(M||b.setHeader("x-nextjs-cache",af?"REVALIDATED":o.isMiss?"MISS":o.isStale?"STALE":"HIT"),b.setHeader(s.NEXT_IS_PRERENDER_HEADER,"1"));let{value:q}=o;if(as)n={revalidate:0,expire:void 0};else if(M&&an&&!am&&ap)n={revalidate:0,expire:void 0};else if(!J.isDev)if($)n={revalidate:0,expire:void 0};else if(ai){if(o.cacheControl)if("number"==typeof o.cacheControl.revalidate){if(o.cacheControl.revalidate<1)throw Object.defineProperty(Error(`Invalid revalidate configuration provided: ${o.cacheControl.revalidate} < 1`),"__NEXT_ERROR_CODE",{value:"E22",enumerable:!1,configurable:!0});n={revalidate:o.cacheControl.revalidate,expire:(null==(k=o.cacheControl)?void 0:k.expire)??ac.expireTime}}else n={revalidate:x.CACHE_ONE_YEAR,expire:void 0}}else b.getHeader("Cache-Control")||(n={revalidate:0,expire:void 0});if(o.cacheControl=n,"string"==typeof au&&(null==q?void 0:q.kind)===u.CachedRouteKind.APP_PAGE&&q.segmentData){b.setHeader(s.NEXT_DID_POSTPONE_HEADER,"2");let c=null==(m=q.headers)?void 0:m[x.NEXT_CACHE_TAGS_HEADER];M&&ai&&c&&"string"==typeof c&&b.setHeader(x.NEXT_CACHE_TAGS_HEADER,c);let d=q.segmentData.get(au);return void 0!==d?(0,z.sendRenderResult)({req:a,res:b,type:"rsc",generateEtags:ac.generateEtags,poweredByHeader:ac.poweredByHeader,result:w.default.fromStatic(d),cacheControl:o.cacheControl}):(b.statusCode=204,(0,z.sendRenderResult)({req:a,res:b,type:"rsc",generateEtags:ac.generateEtags,poweredByHeader:ac.poweredByHeader,result:w.default.fromStatic(""),cacheControl:o.cacheControl}))}let r=(0,h.getRequestMeta)(a,"onCacheEntry");if(r&&await r({...o,value:{...o.value,kind:"PAGE"}},{url:(0,h.getRequestMeta)(a,"initURL")}))return null;if(p&&as)throw Object.defineProperty(Error("Invariant: postponed state should not be present on a resume request"),"__NEXT_ERROR_CODE",{value:"E396",enumerable:!1,configurable:!0});if(q.headers){let a={...q.headers};for(let[c,d]of(M&&ai||delete a[x.NEXT_CACHE_TAGS_HEADER],Object.entries(a)))if(void 0!==d)if(Array.isArray(d))for(let a of d)b.appendHeader(c,a);else"number"==typeof d&&(d=d.toString()),b.appendHeader(c,d)}let t=null==(i=q.headers)?void 0:i[x.NEXT_CACHE_TAGS_HEADER];if(M&&ai&&t&&"string"==typeof t&&b.setHeader(x.NEXT_CACHE_TAGS_HEADER,t),!q.status||an&&ap||(b.statusCode=q.status),!M&&q.status&&E.RedirectStatusCode[q.status]&&an&&(b.statusCode=200),p&&b.setHeader(s.NEXT_DID_POSTPONE_HEADER,"1"),an&&!$){if(void 0===q.rscData){if(q.postponed)throw Object.defineProperty(Error("Invariant: Expected postponed to be undefined"),"__NEXT_ERROR_CODE",{value:"E372",enumerable:!1,configurable:!0});return(0,z.sendRenderResult)({req:a,res:b,type:"rsc",generateEtags:ac.generateEtags,poweredByHeader:ac.poweredByHeader,result:q.html,cacheControl:at?{revalidate:0,expire:void 0}:o.cacheControl})}return(0,z.sendRenderResult)({req:a,res:b,type:"rsc",generateEtags:ac.generateEtags,poweredByHeader:ac.poweredByHeader,result:w.default.fromStatic(q.rscData),cacheControl:o.cacheControl})}let v=q.html;if(!p||M)return(0,z.sendRenderResult)({req:a,res:b,type:"html",generateEtags:ac.generateEtags,poweredByHeader:ac.poweredByHeader,result:v,cacheControl:o.cacheControl});if(aq)return v.chain(new ReadableStream({start(a){a.enqueue(y.ENCODED_TAGS.CLOSED.BODY_AND_HTML),a.close()}})),(0,z.sendRenderResult)({req:a,res:b,type:"html",generateEtags:ac.generateEtags,poweredByHeader:ac.poweredByHeader,result:v,cacheControl:{revalidate:0,expire:void 0}});let A=new TransformStream;return v.chain(A.readable),f({span:d,postponed:q.postponed,fallbackRouteParams:null}).then(async a=>{var b,c;if(!a)throw Object.defineProperty(Error("Invariant: expected a result to be returned"),"__NEXT_ERROR_CODE",{value:"E463",enumerable:!1,configurable:!0});if((null==(b=a.value)?void 0:b.kind)!==u.CachedRouteKind.APP_PAGE)throw Object.defineProperty(Error(`Invariant: expected a page response, got ${null==(c=a.value)?void 0:c.kind}`),"__NEXT_ERROR_CODE",{value:"E305",enumerable:!1,configurable:!0});await a.value.html.pipeTo(A.writable)}).catch(a=>{A.writable.abort(a).catch(a=>{console.error("couldn't abort transformer",a)})}),(0,z.sendRenderResult)({req:a,res:b,type:"html",generateEtags:ac.generateEtags,poweredByHeader:ac.poweredByHeader,result:v,cacheControl:{revalidate:0,expire:void 0}})};if(!aD)return await aC.withPropagatedContext(a.headers,()=>aC.trace(i.BaseServerSpan.handleRequest,{spanName:`${aB} ${a.url}`,kind:g.SpanKind.SERVER,attributes:{"http.method":aB,"http.target":a.url}},n));await n(aD)}catch(b){throw aD||b instanceof A.NoFallbackError||await J.onRequestError(a,b,{routerKind:"App Router",routePath:B,routeType:"render",revalidateReason:(0,f.c)({isRevalidate:ai,isOnDemandRevalidate:af})},ab),b}}},8424:(a,b,c)=>{Promise.resolve().then(c.bind(c,4946))},9121:a=>{"use strict";a.exports=require("next/dist/server/app-render/action-async-storage.external.js")},9148:(a,b,c)=>{"use strict";a.exports=c(4041).vendored.contexts.RouterContext},9294:a=>{"use strict";a.exports=require("next/dist/server/app-render/work-async-storage.external.js")},9513:(a,b,c)=>{"use strict";a.exports=c(4041).vendored.contexts.HeadManagerContext}};var b=require("../webpack-runtime.js");b.C(a);var c=b.X(0,[985,400],()=>b(b.s=8400));module.exports=c})();